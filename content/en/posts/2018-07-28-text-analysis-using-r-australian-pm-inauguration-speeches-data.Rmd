---
title: 'Text Analysis with visNetwork'
author: Davide Lorino
date: '2018-07-28'
slug: text-analysis-with-visnetwork
categories:
  - R
  - Data Science
  - Text Analysis
tags:
  - R
  - Text Analysis
  - Data Science
---

Text Analysis is growing in popularity just about everywhere - it's an abundant source of data that until recently most analysts have ignored because of it's unwieldy structure. Recent developments and packages in the R programming language have made it easy to derive significant meaning from a text corpus. 

## Practical Applications of Text Analysis

  - Analysing email data (e.g. from a customer service inbox to analyze       queries and feedback), 
  - Web scraping (e.g. Cambridge Analytica, though always consider the        ethical implications of such methods even if the data is publicly         available or rightfully yours to share), 
  - Literary analysis (e.g. poems (Shelley vs Byron) and speeches (Obama      vs Trump))
  
In this post we'll look at the Australian Election Speeches 1901 - 2016 dataset provided by https://electionspeeches.moadoph.gov.au/speeches - we'll connect R to their API to read the data, then we'll clean, analyze and visualize the data with some helpful R packages.

```{r}
library(jsonlite)
library(readr)
library(dplyr)
```
  
## Connecting to the API

Running the line below will pull the speeches data directly from the API of the Museum of Australian Democracy 

```{r}
speeches <- fromJSON("https://electionspeeches.moadoph.gov.au/api/speeches.json")
```

Call str() on the resulting 'speeches' data.frame - i'm not going to print the output here because it's really large.

## Cleaning the Data

Load the stringr package to join the 'candidate_name' variable into a 'Name_Surname' format instead of 'Name Surname' (R doesn't like spaces in variable names - it can handle them but you have to start using 'single quotes' around the variables and it's a hassle).

```{r speeches}
library(stringr)
speeches$candidate_name <- str_replace(speeches$candidate_name, " ", "_")
```

Convert 'candidate_name' from a character variable to a factor variable

```{r}
speeches$candidate_name <- as.factor(speeches$candidate_name)
```

And now let's remove that twelfth column in our dataset, it's just a big messy list of metadata that we aren't even using

```{r}
speeches <- speeches[,-12]
```

## More Cleaning - Creating Helper Functions

Now that our dataset is looking a little tidier, we'll want to start cleaning that 'body' variable because it contains all of the speeches we'll be analyzing. 

For our purposes we'll want to:

  - strip the html embedded into the speeches data
  - make everything lowercase (otherwise we double up on words)
  - remove numbers (not always necessary, it will help us now though)
  - remove punctuation (otherwise "word" and "word," are treated differently)
  - remove anything annoying or unhelpful that's left over
  
```{r}

cleanFun <- function(htmlString) {
  return(gsub("<.*?>", "", htmlString))
}
```

Our handy little cleanFun() function will strip 99% of the html from the body. We'll handle the rest of the html and cleaning with this next function:

```{r}
library(tm)

clean_this <- function(x){
       cleanFun(x) %>%
       tolower() %>%
       tm::removeNumbers() %>% 
       tm::removePunctuation() -> yy 
       (gsub("\\\n", " ", yy))
}
```

Ok now let's use it on the speeches data.

```{r}
speeches2 <- clean_this(speeches$body)
```

Our text data should look a lot cleaner now... let's take a look.

```{r}
str(speeches2)
```

## Preparing the Data for visNetwork 

Cool, now that we've cleaned our speeches we can start to analyze visualize them. 

Let's look at word pairs first. 

Load the tidytext and tidyr libraries.

```{r}
library(tidytext)
library(tidyr)

```

Create a single_tokens variable that holds every individual word in the data ranked by count.

```{r}
single_tokens <- speeches %>%
  unnest_tokens(output = word, input = body) %>%
  anti_join(stop_words) %>%
  count(word) %>%
  rename(value = n)
```

Create body_bigrams2 - a variable that holds every individual pair of words, separated into "from" and "to" columns ranked by count. We'll trim this down to word pairs that appear greater than or equal to 17 times and we'll use dplyr and tidytext to filter out stopwords like "to", "in", "the" etc.

```{r}
body_bigrams2 <- speeches %>%
     unnest_tokens(bigram, body, token = "ngrams", n = 2) %>%
     separate(bigram, c("from", "to"), sep = " ") %>%
     filter(!from %in% stop_words$word) %>%
     filter(!to %in% stop_words$word) %>%
     count(from, to, sort = TRUE) %>%
     rename(size = n) %>%
     filter(size >= 17)
```

## Visualize the Word Pairs

Let's use our single_tokens and body_bigrams2 data to make an interactive network graph of the most common word pairs in the history of Australian election speeches. 

Load ggraph, igraph and DT.

```{r}
library(ggraph)
library(igraph)
library(DT)
```

Create the nodes of the network.

```{r}
body_nodes <- tibble(id = unique(c(body_bigrams2$from, body_bigrams2$to)),
                        label = unique(c(body_bigrams2$from, body_bigrams2$to))) %>%
                left_join(single_tokens, by = c(id = "word"))
```

Last step! Let's visualize the network...

```{r}
library(visNetwork)

visNetwork(body_nodes, body_bigrams2)
```

At first the map appears a little zoomed out. Zoom in with your mouse or trackpad scroll feature and explore the network, click anywhere to drag the map around and click on nodes to highlight their connections.

`------------------------------------------------





