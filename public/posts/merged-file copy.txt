In practice risk management is a rational, means-end based process: risks are identified, analysed and then  solved  (or mitigated).  Although these steps seem to be objective, each of them involves human perceptions, biases and interests. Where Jill sees an opportunity, Jack may see only risks.  Indeed, the problem of differences in stakeholder perceptions is broader than risk analysis. The recognition that such differences in world-views may be irreconcilable is what led Horst Rittel to coin the now well-known term, wicked problem.  These problems tend to be made up of complex interconnected and interdependent issues which makes them difficult to tackle using standard rational- analytical methods of problem solving.  Most high-stakes risks that organisations face have elements of wickedness – indeed any significant organisational change is fraught with risk. Murphy rules; things can go wrong, and they often do. The current paradigm of risk management, which focuses on analyzing and quantifying risks using rational methods, is not broad enough to account for the wicked aspects of risk.  I had been thinking about this for a while when I stumbled on a fascinating paper by Robin Holt entitled, Risk Management: The Talking Cure, which outlines a possible approach to analysing interconnected risks. In brief, Holt draws a parallel between psychoanalysis (as a means to tackle individual anxiety) and risk management (as a means to tackle organizational anxiety).  In this post, I present an extensive discussion and interpretation of Holt s paper. Although more about the philosophy of risk management than its practice, I found the paper interesting, relevant and thought provoking. My hope is that some readers might find it so too.  Background Holt begins by noting that modern life is characterized by uncertainty. Paradoxically, technological progress which should have increased our sense of control over our surroundings and lives has actually heightened our personal feelings of uncertainty. Moreover, this sense of uncertainty is not allayed by rational analysis. On the contrary, it may have even increased it by, for example, drawing our attention to risks that we may otherwise have remained unaware of. Risk thus becomes a lens through which we perceive the world. The danger is that this can paralyze.  As Holt puts it,  …risk becomes the only backdrop to perceiving the world and perception collapses into self-inhibition, thereby compounding uncertainty through inertia.  Most individuals know this through experience: most of us have at one time or another been frozen into inaction because of perceived risks.  We also  know  at a deep personal level that the standard responses to risk are inadequate because many of our worries tend to be inchoate and therefore can neither be coherently articulated nor analysed. In Holt s words:  ..People do not recognize [risk] from the perspective of a breakdown in their rational calculations alone, but because of threats to their forms of life – to the non-calculative way they see themselves and the world. [Mainstream risk analysis] remains caught in the thrall of its own  expert  presumptions, denigrating the very lay knowledge and perceptions on the grounds that they cannot be codified and institutionally expressed.  Holt suggests that risk management should account for the  codified, uncodified and uncodifiable aspects of uncertainty from an organizational perspective.  This entails a mode of analysis that takes into account different, even conflicting, perspectives in a non-judgemental way. In essence, he suggests  talking it over  as a means to increase awareness of the contingent nature of risks rather than a means of definitively resolving them.  Shortcomings of risk analysis The basic aim of risk analysis (as it is practiced) is to contain uncertainty within set bounds that are determined by an organisation s risk appetite.  As mentioned earlier, this process begins by identifying and classifying risks. Once this is done, one determines the probability and impact of each risk. Then, based on priorities and resources available (again determined by the organisation s risk appetite) one develops strategies to mitigate the risks that are significant from the organisation s perspective.  However, the messiness of organizational life makes it difficult to see risk in such a clear-cut way. We may  pretend to be rational about it, but in reality we perceive it through the lens of our background, interests , experiences.  Based on these perceptions we rationalize our action (or inaction!) and simply get on with life. As Holt writes:  The concept [of risk] refers to…the mélange of experience, where managers accept contingencies without being overwhelmed to a point of complete passivity or confusion, Managers learn to recognize the differences between things, to acknowledge their and our limits. Only in this way can managers be said to make judgements, to be seen as being involved in something called the future.  Then, in a memorable line, he goes on to say:  The future, however, lasts a long time, so much so as to make its containment and prediction an often futile exercise.  Although one may well argue that this is not the case for many organizational risks, it is undeniable that certain mitigation strategies (for example, accepting risks that turn out to be significant later) may have significant consequences in the not-so-near future.  Advice from a politician-scholar So how can one address the slippery aspects of risk – the things people sense intuitively, but find difficult to articulate?  Taking inspiration from Machiavelli, Holt suggests reframing risk management as a means to determine wise actions in the face of the contradictory forces of fortune and necessity.  As Holt puts it:  Necessity describes forces that are unbreachable but manageable by acceptance and containment—acts of God, tendencies of the species, and so on. In recognizing inevitability, [one can retain one s] position, enhancing it only to the extent that others fail to recognize necessity. Far more influential, and often confused with necessity, is fortune. Fortune is elusive but approachable. Fortune is never to be relied upon:  The greatest good fortune is always least to be trusted ; the good is often kept underfoot and the ridiculous elevated, but it provides [one] with opportunity.  Wise actions involve resolve and cunning (which I interpret as political nous). This entails understanding that we do not have complete (or even partial) control over events that may occur in the future. The future is largely unknowable as are people s true drives and motivations. Yet, despite this, managers must act.  This requires personal determination together with a deep understanding of the social and political aspects of one s environment.  And a little later,  …risk management is not the clear conception of a problem coupled to modes of rankable resolutions, or a limited process, but a judgemental  analysis limited by the vicissitudes of budgets, programmes, personalities and contested priorities.  In short: risk management in practice tends to be a far way off from how it is portrayed in textbooks and the professional literature.  The wickedness of risk management Most managers and those who work under their supervision have been schooled in the rational-scientific approach of problem solving. It is no surprise, therefore, that they use it to manage risks: they gather and analyse information about potential risks, formulate potential solutions (or mitigation strategies) and then implement the best one (according to predetermined criteria). However, this method works only for problems that are straightforward or tame, rather than wicked.  Many of the issues that risk managers are confronted with are wicked, messy or both.  Often though, such problems are treated as being tame.  Reducing a wicked or messy problem to one amenable to rational analysis invariably entails overlooking  the views of certain stakeholder groups or, worse, ignoring key  aspects of the problem.  This may work in the short term, but will only exacerbate the problem in the longer run. Holt illustrates this point as follows:  A primary danger in mistaking a mess for a tame problem is that it becomes even more difficult to deal with the mess. Blaming  operator error  for a mishap on the production line and introducing added surveillance is an illustration of a mess being mistaken for a tame problem. An operator is easily isolated and identifiable, whereas a technological system or process is embedded, unwieldy and, initially, far more costly to alter. Blaming operators is politically expedient. It might also be because managers and administrators do not know how to think in terms of messes; they have not learned how to sort through complex socio-technical systems.  It is important to note that although many risk management practitioners recognize the essential wickedness of the issues they deal with, the practice of risk management is not quite up to the task of dealing with such matters.  One step towards doing this is to develop a shared (enterprise-wide) understanding of risks by soliciting input from diverse stakeholders groups, some of who may hold opposing views.  The skills required to do this are very different from the analytical techniques that are the focus of problem solving and decision making techniques that are taught in colleges and business schools.  Analysis is replaced by sensemaking – a collaborative process that harnesses the wisdom of a group to arrive at a collective understanding of a problem and thence a common  commitment to a course of action. This necessarily involves skills that do not appear in the lexicon of rational problem solving: negotiation, facilitation, rhetoric and those of the same ilk that are dismissed as being of no relevance by the scientifically oriented analyst.  In the end though, even this may not be enough: different stakeholders may perceive a given  risk  in have wildly different ways, so much so that no consensus can be reached.  The problem is that the current framework of risk management requires the analyst to perform an objective analysis of situation/problem, even in situations where this is not possible.  To get around this Holt suggests that it may be more useful to see risk management as a way to encounter problems rather than analyse or solve them.  What does this mean?  He sees this as a forum in which people can talk about the risks openly:  To enable organizational members to encounter problems, risk management s repertoire of activity needs to engage their all too human components: belief, perception, enthusiasm and fear.  This gets to the root of the problem: risk matters because it increases anxiety and generally affects peoples  sense of wellbeing. Given this, it is no surprise that Holt s proposed solution draws on psychoanalysis.  The analogy between psychoanalysis and risk management Any discussion of psychoanalysis –especially one that is intended for an audience that is largely schooled in rational/scientific methods of analysis – must begin with the acknowledgement that the claims of psychoanalysis cannot be tested. That is, since psychoanalysis speaks of unobservable  objects  such as the ego and the unconscious, any claims it makes about these concepts cannot be proven or falsified.  However  as Holt suggests, this is exactly what makes it a good fit for encountering (as opposed to  analyzing) risks. In his words:  It is precisely because psychoanalysis avoids an overarching claim to produce testable, watertight, universal theories that it is of relevance for risk management. By so avoiding universal theories and formulas, risk management can afford to deviate from pronouncements using mathematical formulas to cover the  immanent indeterminables  manifest in human perception and awareness and systems integration.  His point is that there is a clear parallel between psychoanalysis and the individual, and risk management and the organisation:  We understand ourselves not according to a template but according to our own peculiar, beguiling histories. Metaphorically, risk management can make explicit a similar realization within and between organizations. The revealing of an unconscious world and its being in a constant state of tension between excess and stricture, between knowledge and ignorance, is emblematic of how organizational members encountering messes, wicked problems and wicked messes can be forced to think.  In brief, Holt suggests that what psychoanalysis does for the individual, risk management ought to do for the organisation.  Talking it over – the importance of conversations A key element of psychoanalysis is the conversation between the analyst and patient. Through this process, the analyst attempts to get the patient to become aware of hidden fears and motivations. As Holt puts it,  Psychoanalysis occupies the point of rupture between conscious intention and unconscious desire — revealing repressed or overdetermined aspects of self-organization manifest in various expressions of anxiety, humour, and so on  And then, a little later,  he makes the connection to organisations:  The fact that organizations emerge from contingent, complex interdependencies between specific narrative histories suggests that risk management would be able to use similar conversations to psychoanalysis to investigate hidden motives, to examine…the possible reception of initiatives or strategies from the perspective of inherently divergent stakeholders, or to analyse the motives for and expectations of risk management itself. This fundamentally reorients the perspective of risk management from facing apparent uncertainties using technical assessment tools, to using conversations devoid of fixed formulas to encounter questioned identities, indeterminate destinies, multiple and conflicting aims and myriad anxieties.  Through conversations involving groups of stakeholders who have different risk perceptions,  one might be able to get a better understanding of a particular risk and hence, may be, design a more effective mitigation strategy.  More importantly, one may even realise that certain risks are not risks at all or others that seem straightforward have implications that would have remained hidden were it not for the conversation.  These collective conversations would take place in workshops…  …that tackle problems as wicked messes, avoid lowest-denominator consensus in favour of continued discovery of alternatives through conversation, and are instructed by metaphor rather than technical taxonomy, risk management is better able to appreciate the everyday ambivalence that fundamentally influences late-modern organizational activity. As such, risk management would be not merely a rationalization of uncertain experience but a structured and contested activity involving multiple stakeholders engaged in perpetual translation from within environments of operation and complexes of aims.  As a facilitator of such workshops, the risk analyst provokes stakeholders to think about their feelings and motivations that may be  out of bounds  in a standard risk analysis workshop.  Such a paradigm goes well beyond mainstream risk management because it addresses the risk-related anxieties and fears of individuals who are affected by it.  Conclusion This brings me to the end of my not-so-short summary of Holt s paper. Given the length of this post, I reckon I should keep my closing remarks short. So I ll leave it here paraphrasing the last line of the paper, which summarises its main message:  risk management ought to be about developing an organizational capacity for overcoming risks, freed from the presumption of absolute control.Enterprise risk management (ERM) refers to the process by which uncertainties are identified, analysed and managed from an organization-wide perspective. In principle such a perspective enables organisations to deal with risks in a holistic manner, avoiding the silo mentality that plagues much of risk management practice.  This is the claim made of ERM at any rate, and most practitioners accept it as such.  However, whether the claim really holds is another matter altogether. Unfortunately,  most of the available critiques of ERM  are written for academics or risk management experts. In this post I summarise a critique of ERM presented in a paper by Michael Power entitled, The Risk Management of Nothing.  I'll begin with a brief overview of ERM frameworks and then summarise the main points of the paper along with some of my comments and annotations.  ERM Frameworks and Definitions What is ERM?  The best way to answer this question is to look at a couple of well-known ERM frameworks, one from the Casualty Actuarial Society (CAS) and the other from the Committee of Sponsoring Organisations of the Treadway Commission (COSO).  CAS defines ERM as:  … the discipline by which an organization in any industry assesses, controls, exploits, finances, and monitors risks from all sources for the purpose of increasing the organization's short- and long-term value to its stakeholders.  See this article for an overview of ERM from actuarial perspective.  COSO defines ERM as:  …a process, effected by an entity's board of directors, management and other personnel, applied in strategy setting and across the enterprise, designed to identify potential events that may affect the entity, and manage risk to be within its risk appetite, to provide reasonable assurance regarding the achievement of entity objectives.  The term risk appetite in the above definition refers to the risk an organisation is willing to bear. See the first article in the  June 2003 issue of Internal Auditor for more on the COSO perspective on ERM.  In both frameworks, the focus is very much on quantifying risks through (primarily) financial measures and on establishing accountability for managing these risks in a systematic way.  All this sounds very sensible and uncontroversial. So, where's the problem?  The problems with ERM The author of the paper begins with the observation that the basic aim of ERM is to identify risks that can affect an organisation's objectives and then design controls and mitigation strategies that reduce these risks (collectively) to below a predetermined  value that  is specified by the organisation's risk appetite. Operationally, identified risks are monitored and corrective action is taken when they go beyond limits specified by the controls, much like the operation of a thermostat.  In this view, risk management is a mechanistic process.  Failures of risk management are seen more as being due to "not doing it right" (implementation failure) or politics getting in the way (organizational friction), rather than a problem with the framework itself. The basic design of the framework is rarely questioned.  Contrary to common wisdom, the author of the paper believes that the design of ERM is flawed in the following three ways:  The idea of a single, organisation-wide risk appetite is simplistic. The assumption that risk can be dealt with by detailed, process-based rules (suitable for audit and control) is questionable. The undue focus on developing financial metrics and controls blind it to "bigger picture", interconnected risks because these cannot be quantified or controlled by such mechanisms. We'll now take a look at each of the above in some detail  Appetite vs. appetisation As mentioned earlier, risk appetite is defined as the risk the organisation is willing to bear. Although ERM frameworks allow for qualitative measures of risk appetite, most organisations implementing ERM tend to prefer quantitative ones. This is a problem because the definition of risk appetite can vary significantly across an organization. For example, the sales and audit functions within an organisation could (will!) have different appetites for risk.  As another example, familiar to anyone who reads the news, is that there is usually a big significant gap between the risk appetites of financial institutions and regulatory authorities.  The difference in risk appetites of different stakeholder groups  is a manifestation of the fact that risk is a social construct – different stakeholder groups view a given risk in different ways, and some may not even see certain risks as risks (witness the behaviour of certain financial "masters of the universe")  Since a single, organisation-wide risk appetite is difficult to come up with, the author suggests a different approach – one which takes into account the multiplicity of viewpoints in an organisation; a process he calls "risk appetizing".  This involves getting diverse stakeholders to achieve a consensus / agreement on what constitutes risk appetite. Power argues that this process of reconciling different viewpoints of risk would lead to a more realistic view of the risk the organization is willing to bear. Quoting from the paper:  Conceptualising risk appetising as a process might better direct risk management attention to where it has likely been lacking, namely to the multiplicity of interactions which shape operational and ethical boundaries at the level of organizational practice. COSO-style ERM principles effectively limit the concept of risk appetite within a capital measurement discourse. Framing risk appetite as the process through which ethics and incentives are formed and reformed would not exclude this technical conception, but would bring it closer to the insights of several decades of organization theory.  Explicitly acknowledging the diversity of viewpoints on risk is likely to be closer to reality because:  …a conflictual and pluralistic model is more descriptive of how organizations actually work, and makes lower demands on organizational and political rationality to produce a single ‘appetite' by explicitly recognising and institutionalising processes by which different appetites and values can be mediated.  Such a process is difficult because it involves getting people who have different viewpoints to agree on what constitutes a sensible definition of risk appetite.  A process bias A bigger problem, in Power's view, is that the ERM frameworks overemphasise financial / accounting measures and processes as a means of quantifying and controlling risk. As he puts it ERM:  … is fundamentally an accounting-driven blueprint which emphasises a controls-based approach to risk management. This design emphasis means that efforts at implementation will have an inherent tendency to elaborate detailed controls with corresponding documents trails.  This is a problem because it leads to a "rule-based compliance" mentality wherein risks are managed in a mechanical manner, using bureaucratic processes as a substitute for real thought about risks and how they should be managed. Such a process may work in a make-believe world where all risks are known, but is unlikely to work in one in which there is a great deal of ambiguity.  Power makes the important point that rule-based compliance chews up organizational resources. The tangible effort expended on compliance serves to reassure organizations that they are doing something to manage risks.  This is dangerous because it lulls them into a false sense of security:  Rule-based compliance lays down regulations to be met, and requires extensive evidence, audit trails and box ‘checking'. All this demands considerable work and there is daily pressure on operational staff to process regulatory requirements. Yet, despite the workload volume pressure, this is also a cognitively comfortable world which focuses inwards on routine systems and controls. The auditability of this controls architecture can be theorized as a defence against anxiety and enables organizational agents to feel that their work conforms to legitimised principles.  In this comfortable, prescriptive world of process-based risk management, there is little time to imagine and explore what (else) could go wrong. Further, the latter is often avoided because it is a difficult and often uncomfortable process:  …the imagination of alternative futures is likely to involve the production of discomfort, as compared with formal ‘comfort' of auditing. The approach can take the form of scenario analysis in which participants from different disciplines in an organization can collectively track the trajectory of potential decisions and events. The process begins as an ‘encounter' with risk and leads to the confrontation of limitation and ambiguity.  Such a process necessarily involves debate and dialogue – it is essentially a deliberative process. And as Power puts it:  The challenge is to expand processes which support interaction and dialogue and de-emphasise due process – both within risk management practice and between regulator and regulated.  This is right of course, but that's not all:  a lot of other process-focused disciplines such as project management would also benefit by acknowledging and responding to this challenge.  A limited view of embeddedness One of the imperatives of ERM is to "embed" risk management within organisations. Among other things, this entails incorporating  risk management explicitly into job descriptions, and making senior managers responsible for managing risks.  Although this is a step in the right direction, Power argues that the concept of embeddeness as articulated in ERM remains limited because  it focuses on specific business entities, ignoring the wider environment and context in which they exist. The essential (but not always obvious) connections between entities are not necessarily accounted for. As Power puts it:  ERM systems cannot represent embeddedness in the sense of interconnectedness; its proponents seem only to demand an intensification of embedding at the individual entity level. Yet, this latter kind of embedding of a compliance driven risk management, epitomised by the Sarbanes-Oxley legislation, is arguably a disaster in itself, by tying up resources and, much worse, cognition and attention in ‘auditized' representations of business processes.  In short: the focus on following a process-oriented approach to risk management – as mandated by frameworks – has the potential to de-focus attention from risks that are less obvious, but are potentially more significant.  Addressing the limitations Power believes the flaws in ERM can be addressed by looking to the practice of business continuity management (BCM). BCM addresses the issue of disaster management – i.e. how to keep an organisation functioning in the event of a disaster. Consequently, there is a significant overlap between the aims of BCM and ERM. However, unlike ERM, BCM draws specialists from different fields and emphasizes collective action. Such an approach is therefore more likely to take a holistic view of risk, and that is the real point.  Regardless of the approach one takes, the point is to involve diverse stakeholders and work towards a shared (enterprise-wide) understanding of risks. Only then will it be possible to develop a risk management plan that incorporates the varying, even contradictory, perspectives that exist within an organisation. There are many techniques to work towards a shared understanding of risks, or any other issues for that matter. Some of these are discussed at length in my book.  Conclusion Power suggests that ERM, as articulated by bodies such as CAS and COSO, flawed because:  It attempts to quantify risk appetite at the organizational level – an essentially impossible task because different organizational stakeholders will have different views of risk. Risk is a social construct. It advocates a controls and rule-based approach to managing risks. Such a prescriptive "best" practice approach discourages debate and dialogue about risks. Consequently, many viewpoints are missed and quite possibly, so are many risks. Despite the rhetoric of ERM, implemented risk management controls and processes often overlook connections and dependencies between entities within organisations. So, although risk management appears to be embedded within the organisation, in reality it may not be so. Power suggests that ERM practice could learn a few lessons from Business Continuity Management (BCM), in particular about the interconnected nature of business risks and the collective action needed to tackle them. Indeed, any approach that attempts to reconcile diverse risk viewpoints will be a huge improvement on current practice. Until then ERM will continue to be an illusion, offering false comfort to those who are responsible for managing risk.The discussion of risk in presented in most textbooks and project management courses follows the well-trodden path of risk identification, analysis, response planning and monitoring (see the PMBOK guide, for example).  All good stuff, no doubt.  However, much of the guidance offered is at a very high level. Among other things, there is little practical advice on what not to do. In this post I address this issue by outlining some of the common pitfalls in project risk analysis.  1. Reliance on subjective judgement: People see things differently:  one person's risk may even be another person's opportunity. For example, using a new technology in a project can be seen as a risk (when focusing on the increased chance of failure) or opportunity (when focusing on the opportunities afforded by being an early adopter). This is a somewhat extreme example, but the fact remains that individual perceptions influence the way risks are evaluated.  Another problem with subjective judgement is that it is subject to cognitive biases – errors in perception. Many high profile project failures can be attributed to such biases:  see my post on cognitive bias and project failure for more on this. Given these points, potential risks should be discussed from different perspectives with the aim of reaching a common understanding of what they are and how they should be dealt with.  2. Using inappropriate historical data: Purveyors of risk analysis tools and methodologies exhort project managers to determine probabilities using relevant historical data. The word relevant is important: it emphasises that the data used to calculate probabilities (or distributions) should be from situations that are similar to the one at hand.  Consider, for example, the probability of a particular risk – say,  that a particular developer will not be able to deliver a module by a specified date.  One might have historical data for the developer, but the question remains as to which data points should be used. Clearly, only those data points that are from projects that are similar to the one at hand should be used.  But how is similarity defined? Although this is not an easy question to answer, it is critical as far as the relevance of the estimate is concerned. See my post on the reference class problem for more on this point.  3. Focusing on numerical measures exclusively: There is a widespread perception that quantitative measures of risk are better than qualitative ones. However,  even where reliable and relevant data is available,  the measures still need to  based on sound methodologies. Unfortunately, ad-hoc techniques abound in risk analysis:  see my posts on Cox's risk matrix theorem and limitations of risk scoring methods for more on these.  Risk metrics based on such techniques can be misleading.  As Glen Alleman points out in this comment, in many situations qualitative measures may be more appropriate and accurate than quantitative ones.  4. Ignoring known risks: It is surprising how often known risks are ignored.  The reasons for this have to do with politics and mismanagement. I won't dwell on this as I have dealt with it at length in an earlier post.  5. Overlooking the fact that risks are distributions, not point values: Risks are inherently uncertain, and any uncertain quantity is represented by a range of values, (each with an associated probability) rather than a single number (see this post for more on this point). Because of the scarcity or unreliability of historical data, distributions are often assumed a priori: that is, analysts will assume that the risk distribution has a particular form (say, normal or lognormal) and then evaluate distribution parameters using historical data.  Further, analysts often choose simple distributions that that are easy to work with mathematically.  These distributions often do not reflect reality. For example,  they may be vulnerable to "black swan" occurences because they do not account for outliers.  6. Failing to update risks in real time: Risks are rarely static – they evolve in time, influenced by circumstances and events both in and outside the project. For example, the acquisition of a key vendor by a mega-corporation is likely to affect the delivery of that module you are waiting on –and quite likely in an adverse way. Such a change in risk is obvious; there may be many that aren't. Consequently, project managers need to reevaluate and update risks periodically. To be fair, this is a point that most textbooks make – but it is advice that is not followed as often as it should be.  This brings me to the end of my (subjective) list of risk analysis pitfalls. Regular readers of this blog will have noticed that some of the points made in this post are similar to the ones I made in my post on estimation errors. This is no surprise: risk analysis and project estimation are activities that deal with an uncertain future, so it is to be expected that they have common problems and pitfalls. One could generalize this point:  any activity that involves gazing into a murky crystal ball will be plagued by similar problems.Project managers know from experience that projects can go wrong because of events that weren't foreseen. Some of these may be unforeseeable– that is, they could not have been anticipated given what was known prior to their occurrence. On the other hand  it is surprisingly common  that known risks are ignored.  The metaphor of the elephant in the room is appropriate here because these risks are quite obvious to outsiders, but apparently not to those involved in the project. This is a strange state of affairs because:  Those involved in the project are best placed to "see the elephant" They are directly affected  when the elephant goes on rampage  –  i.e. the risk eventuates. This post discusses reasons why these metaphorical pachyderms are ignored by those who need most to recognize their existence .  Let's get right into it then – seven reasons why risks are ignored on projects:  1. Let sleeping elephants lie: This is a situation in which stakeholders are aware of the risk, but don't do anything about it in the hope that it will not eventuate. Consequently, they have no idea how to handle it if it does. Unfortunately, as Murphy assures us, sleeping elephants will wake at the most inconvenient moment.  2. It's not my elephant: This is a situation where no one is willing to take responsibility for managing the risk. This game of "pass the elephant" is resolved by handing charge of the elephant to a reluctant mahout.  3. Deny the elephant's existence: This often manifests itself as a case of collective (and wilful) blindness to obvious risks.  No one acknowledges the risk, perhaps out of fear of that they will be handed responsibility for it (see point 2 above).  4. The elephant has powerful friends: This is a pathological situation where some stakeholders (often those with clout) actually increase the likelihood of a risk through bad decisions. A common example of this is the imposition of arbitrary deadlines, based on fantasy rather than fact.  5. The elephant might get up and walk away: This is wishful thinking, where the team assumes that the risk will magically disappear. This is the "hope and pray" method of risk management, quite common in some circles.  6. The elephant's not an elephant: This is a situation where a risk is mistaken for an opportunity. Yes, this does happen. An example is when a new technology is used on a project: some team members may see it as an opportunity, but in reality it may pose a risk.  7. The elephant's dead: This is exemplified by the response, "that is no longer a problem," when asked about the status of a risk.  The danger in these situations is that the elephant may only be fast asleep, not dead.  Risks that are ignored are the metaphorical pachyderms in the room.  Ignoring them is easy  because it involves no effort whatsoever. However, it is a strategy that is fraught with danger because once these risks eventuate, they can – like those apparently invisible elephants – run amok and wreak havoc on projects.I've written a number of articles on project failure, covering topics ranging from  definitions of success to the role of biases in project failure.  As interesting as these issues are, they are somewhat removed from the day-to-day concerns of a  project manager who is  more interested in avoiding failure than defining or analyzing it. In a paper entitled, Early warning signs of IT project failure: the dominant dozen,  Leon Kappelman et. al. outline the top twelve risks associated with IT project failures.  This post summarises the paper and lists the top twelve signs of impending trouble on projects.  Background and research methodology The authors  focus on early warning signs – i.e. those that occur within the initial 20% of the planned schedule. Further, to ensure comprehensive coverage of risks, their conclusions are  based on inputs from academic and industry journals as well as from experienced IT project managers.  The paper provides a detailed explanation of their research methodology, which I'll quote directly from the paper:  The research team first searched the literature extensively to develop a preliminary list of early warning signs (EWSs). The two authors experienced in IT project management then added several EWSs based on their personal experience. Next, 19 IT project management experts were asked to assess the list. On the basis of their feedback, we added new items and modified others to develop a list of 53 EWSs. Finally, the research team invited 138 experienced IT project managers (including the original 19 experts) to participate in rating the 53 EWSs using a scale from 1 (extremely unimportant) to 7 (extremely important). Fifty-five (55) of these managers completed the survey, yielding a response rate of nearly 40 percent. The respondents had an average of more than 15 years of IT project management experience. The budgets of the largest IT projects they managed ranged from 3 million to 7 billion dollars. About 30 percent held the title of program or project manager and nearly 20 percent had consultant titles. Director or program analyst titles accounted for about 15 percent each, 10 percent were vice presidents, and the rest held titles such as CEO, CIO, chief scientist, chief technologist, or partner.  Although the list and the rankings were based on the subjective opinions of experts, the large number of participants ensures a degree of  consensus regarding  the most important factors.  The troublesome twelve After ranking the fifty odd risks, the authors focused on those that had scores above 6 (out of  a maximum possible of  7 as discussed above).  There were 17 risks that satisfied this (somewhat arbitrary) criterion.  Some of these were similar, so they could be combined. For example, the four risks:  No documented milestone deliverables and due dates. No project status progress process Schedule deadline not reconciled to the project schedule Early project delays are ignored — no revision to the overall project schedule were combined into: ineffective schedule planning and/or management.  This process of combining the top 17 items resulted in twelve risks, half of which turned out to be people-related and the other half process-related.  I discuss each of the risks in detail below.  People-related early warning signs  1. Lack of top management support: This was the number one risk out of the fifty three that the authors listed. This isn't surprising – a project that lacks executive support is unlikely to get the financial,  material or human resources necessary to make it happen.  2. Ineffective project manager: Project managers who lack the communication and managerial skills needed to move the project ahead pose a serious risk to projects. The authors point out  that this is a common risk on IT projects because project managers  are often technical folks who have been promoted to managers. As such they may lack the interest, aptitude and/or skills to manage projects. Interestingly, the authors do not comment on the converse problem – whether the project manager's lack of technical/domain knowledge contributes to project failure.  3. No stakeholder involvement and/or participation: A large number of projects proceed with minimal involvement of key stakeholders. Such folks often lose interest in  projects when more immediate matters consume their attention.  In such situations a project manager may find it hard to get the resources he or she needs to get the project done. Stakeholder or sponsor apathy is an obvious warning sign that a project is headed for trouble.  4. Uncommitted project team: The commitment (preferably, full-time) of a team is essential for the success of a project.  Management needs to ensure that team members are given the time (and incentives) to work on the project. A point that is often left unconsidered is the intrinsic motivation of the team – see this post for a detailed discussion of motivation in project management.  5. Lack of technical knowledge/skills:  Project teams need to have the technical skills and knowledge that is relevant to the project. Managers sometimes wrongly assume that project staff can pick up the required skills  whilst working on a project.  Another common management misconception is that project personnel can master new technologies solely by attending training courses.  Getting contractors to do the work  is one solution to the problem.  However,  the best option is  to give the team enough time to get familiar with the technology prior to the project or, failing this,  to switch to a technology that the team is familiar with.  6. Subject matter experts are not available: It is often assumed that subject matter experts can provide adequate inputs into projects whilst doing their regular jobs. This seldom works – when there's a choice between the project and their jobs, the latter always wins.  Project sponsors need to  ensure that subject matter experts are freed up to work on the project.  Process-related early warning signs  1. Unclear scope: The authors label this one as "Lack of documented requirements and/or success criteria." However I think it is better described by the phrase I've used. All project management methodologies emphasise the importance of clear, well-documented requirements and success criteria –  and with good reason too. Lack of clarity regarding project scope means that no one knows where the project is headed – a sure sign of trouble ahead.  2. No change control process:  As the cliché reminds us, change is the only constant in business environments.  It is therefore inevitable that project scope will change.  Changes to scope –however minor they may seem- need to be assessed for their impact on the project. The effect of several small (unanalyzed) scope changes on the project schedule should not be underestimated! Many project managers have a hard time pushing back on scope changes foisted on them by senior executives. Hence it is important that the change control process applies across the board – to everyone regardless of their authority.  3. Ineffective scheduling and schedule management: Many schedules are built on little more than guesswork and an unhealthy dose of optimism, often because they are drawn up without input from the folks who'll actually do the work (see my article on estimation errors for more on this). Schedules need to be rooted in reality. For this to happen, they must be based on reliable estimates, preferably from those responsible for creating the deliverables. Once the schedule is created, it is the project manager's responsibility to update it continually, reflecting all the detours and road-bumps that have occurred along the way.  A common failing is that time overruns are not properly recorded,  leading to a false illusion of progress.  4. Communication breakdown: Project communication is the art of getting people on the same page when they are reading different books. In my post on obstacles to project communication, I have discussed some generic difficulties posed by differences in stakeholder backgrounds and world-views. One of the key responsibilities of a project manager is to ensure that everyone on the project has a shared understanding of the project goals and shared commitment to achieving them. This is as true in the middle or the end of a project as it is at the start.  5. Resources assigned to another project:  In my experience resources are rarely reassigned wholesale to other projects. What usually happens is that they are  reassigned on a part time basis,  as in "we'll take 20 % of Matt's time and 10% of Nick's time." The problem with this is that Matt and Nick will end up spending most  of their time on the other project, leaving the one on hand bereft.  6. No  business case: A not uncommon refrain in corporate hallways is, "Why are we doing this project?" No project should be given the go-ahead without a well-articulated business case. Further still, since an understanding reason(s) for doing the project are central to its success, these should be made available to every stakeholder:  a shared understanding of the goals of the project is a prerequisite to a shared understanding of the rationale behind it.  I'm sure there aren't any surprises in this list –  most project managers would agree that these are indeed common (and often ignored) early warning signs of failure.  However, I suspect that there will be substantial differences of opinion regarding their ranking. Wisely, the authors have refrained from attempting to rank the risks – the list is not in order of importance.  Conclusion Good projects managers  anticipate potential problems and take action to avoid them.  Although the risks listed above are indeed obvious , they are often ignored. Affected projects then  limp on to oblivion because those responsible failed to react to portents of trouble.  Granted, it can be hard to see problems from within the system,  particularly when the system is a  high-pressure project.  That's where such lists are useful:  they can warn the project manager of potential  trouble ahead.Most approaches to project risk management prescribe a structured approach to managing risks involving steps such as identification, analysis and response planning (see the PMBOK Guide, for example).  Implicit in this approach is the assumption that risks exist "out there" and  can be identified via systematic investigation.  Among other things, this leads to a view that it is possible – and indeed, desirable – to identify all possible risks on a project, and then manage these through the lifecycle of a project. Of course, new risks may emerge, but the underlying belief is that these too can be identified and managed using a structured approach.  In short, the fundamental assumption is that risks have an objective existence and can therefore be identified by an examination of the project and its environment.  In a paper entitled, The Limits of Risk Management – A Social Construction Approach, Bernd Stahl and co-authors argue that the traditional view of risk management is limited because:  The assumption of objectivity leads to a false sense of security. It (often) ignores (or understates) risks that may be specific to certain organisations or situations. Consequently, they suggest that it may be more fruitful to view risks as social constructs – perceptions of reality that are products of social interactions between stakeholders. In this post I explore this somewhat unusual view of risk. I'll first illustrate the general idea of risk as social construct and then describe how some common IS project risks can be seen as social constructs. I'll end with a brief look at the implications of this for project risk management.  Project risks as social constructs The first step in classical (or standard) risk analysis is to identify all possible events that may affect the project.  However this, in principle, is impossible because there is no general method to do so. Consequently, risk managers usually compile lists of potential risks (based on prior experience, research literature, textbooks, brainstorming etc.) and use these as starting points for analysis.  The point is, all these methods are ad-hoc and, despite claims to the contrary, cannot guarantee a comprehensive coverage of all risks.  This point is made very clearly in a  paper by Lyytinen et. al. which examines four techniques of software project risk analysis and concludes that there are substantial differences between them.  Quoting from the conclusion to the Lyytinen paper:  …practitioners should be cautious with regard to the expected miracles of any risk management approach. Risk management approaches are neither complete nor risk-proof (my italics). Their value in the context of managing complex socio-technical change is that they put high premium on shaping management attention through learning new organizing schemes that help make sense of the development situations in new ways…  According to Stahl, the conclusions of the Lyytinnen (and a few other researchers)  support the idea that  risk s is a social construct that comes about as a result of  interactions and agreements between those involved in the project (and individual perceptions of these):  We believe that [the] objective concept of risk is flawed and threatens the success of the entire enterprise. A concept of objective risk raises the expectation that risks can be completely controlled. Also, it suggests that once the comprehensive list of risks is compiled, the end of the theoretical work is reached and managerial practice can take over. These factors can combine to create a false sense of security that will dull managers' attention and can thereby create even bigger risks. We hold risk to be a social construction depending on social agreements and on individual perceptions. It must be ascribed to become real.  In my opinion there's a big  leap of logic in the above paragraph:  I do not see how "the expectation that risk can be completely controlled" and the "false sense of security this creates" implies that risk is a construct that depends on "social agreements  and individual perceptions." Nevertheless, I do believe that the social constructionist view of risk is useful because most project risks have a social dimension. I'll elaborate on this point via examples in the next section.  IS project risk as a social construct – some examples Most of the research literature on social construction tends to be hard to read and devoid of examples that professional project managers can relate to. This is a shame because much of this work casts a critical eye on the implicit assumptions that underlie project management practice. So, instead of quoting from and paraphrasing hard-to-read papers, I'll illustrate how some common project risks have a social dimension.  The risks I consider are drawn from a paper entitled, Early warning signs of IT project failure: the dominant dozen, by Leon Kappelman and his co workers.  The paper presents the top twelve early-stage project risks based on an analysis of data gathered from project professionals and a survey of research literature.  I'll discuss how  four risks from Kappelman's dominant dozen have social origins.  Lack of top management support  According to Kappelman et. al.  this is the number one risk in IS projects.  The point is that this risk most often arises because of (dysfunctional) politicking between managers. As  Kappelman et. al. state :  In many cases, IT projects get caught up in enterprise politics where there are fundamental disagreements about overall enterprise priorities. In these cases the resources and enterprise-wide commitment required for success are lacking. Middle managers do not see the project as being important to the enterprise or to their performance evaluations and therefore redirect resources and attention to activities that top management does support.  The point is: this risk is often a consequence of corporate politics, which is very much a social construct.  Lack of stakeholder involvement and/or participation  The following passage from the paper highlights the socially constructed nature of this risk:  If key project stakeholders do not participate in major review meetings, it signals they are not engaged in the project and therefore the project is not a high priority for them. Other stakeholders soon begin to disengage too. The project manager then finds it harder to get the participation and resources necessary for project success, especially from those who are not full-time members of the project team. Often such project team members get reassigned to other projects that are perceived to be more important.  The point is – key stakeholders, through their influence in the organisation, can cause a project to go awry.  Another common situation is one in which stakeholders lose interest in a project because they do not see the benefits of being involved. This point is well-recognised in traditional risk management which highlights the importance of getting stakeholder buy-in. The point here is that this risk is a consequence of individual perceptions, not of objective reality. In other words,  it is a social construct.  Lack of documented requirements and/or success criteria  This risk is an old classic, discussed and analysed by several well-known writers.  As Robert Glass  mentions in his book on the facts and fallacies of software engineering:  This problem is caused by the fact that the customers and users for the software solution are not really sure of what problem they need to have solved. They may think they know at the outset, only to discover as the project proceeds that the problem they wanted to solve is too simplistic or unrealistic or something else they weren't expecting. Or they may really not know anything at the outset and are simply exploring solutions to a vague problem they know needs to be solved….  Regardless of the causes or the outcome, the point is that Ill defined scope means that different stakeholders have different interpretations of what's to be delivered.  Consequently, project objectives become a matter of interpretation and opinion. This shows how even something as basic as project objectives are actually social constructs – they depend on individual stakeholder perceptions. Seen in this light it isn't a stretch to say that there can be  as many objectives as there are stakeholders!  Communication breakdown among stakeholders  This risk is perhaps the most obvious social construct – communication is not only the lifeblood of a project, it is also the basis on which professional and social relationships are built. Communication becomes all the more important on projects environments where diverse stakeholders and ongoing change are the norm. As Kappelman et. al. state:  Any significant project has multiple stakeholders and requires an ongoing choreography of various tasks and resources.  Change over the life of the project is inevitable — business environment, competitor strategic and tactical moves, laws and regulations, management team, staff turnover, resource availability, and cost — to name just a few possibilities. If all stakeholders do not  communicate and work together on an ongoing basis, the project team will be pulled in multiple directions…  Communication breakdowns can occur for a variety reasons, but most of these arise because of conflicting viewpoints of those involved. As I have written in a post on project communication, "A shared world-view –  which includes a common understanding of tools, terminology, culture, politics etc. –  is what enables effective communication within a group."  Implications The above examples show how common project risks have a social dimension, even if they are not entirely social constructs.  The social aspects of risk are often ignored because they are hard to handle –  it is much easier to follow a process than to deal with stakeholders who are (or might get) upset. Consequently risks such as communication breakdown or lack of management support are not broached because of the high cost of speaking up.  I contend that many project risks remain unaddressed because of this.  The implications of a social constructionist view of risk can be summed up as follows:  It is impossible, in principle, to compile authoritative lists of all possible risks. Risks should be studied in the context of a particular project and its environment. This includes technical, social and organizational aspects of the environment. Particular emphasis should be given to relationships between stakeholders as these may present barriers to an honest assessment of risks. These implications suggest that a participatory approach involving open deliberation is mandatory for successful  risk management. As Stahl et. al. put it:  Indeed, a checklist of risks is just a starting point for an ongoing debate on risk. Management should identify the stakeholder of risky decisions and engage in a free discourse about the nature and the evaluation of risks. The stakeholder discourse could be used to define responsibilities . The stakeholders as parties interested in the process are presumably best suited to identify risk factors. Ideally this process would lead to a consensus concerning the risks. Similar approaches have been suggested in the literature, however, we wish to emphasise that only when managers understand that risk is a social construct will the complex and costly process of stakeholder discourses as a way of dealing with risk make sense (my italics).  Summing up The mathematical and analytical machinery of risk management can obscure the fact that  managing risks  is as much an art as a science, calling for skills in dealing with people as well as probabilities. Organisational politics, individual perceptions and interactions between different stakeholder groups play a role in creating risks.  In other words, risks are not objective entities, they have a social dimension.Risk management is an important component of all project management frameworks and methodologies, so most project managers are well aware of the need to manage risks on their projects. However, most books and training courses offer little or no guidance about the relative importance of different categories of risks.  One useful way to look at risks is by whether they pose operational or strategic threats. The former category includes risks that impact project execution and the latter those that affect project goals. A recent paper entitled, Categorising Risks in Seven Large Projects – Which Risks do the Projects Focus On?, looks at how strategic and operational risks are treated in typical, real-life projects. This post is a summary and review of the paper.  Operational and strategic risks For the purpose of their study, the authors of the paper categorise risks as follows:  Operational risk: A risk that affects a project deliverable. Short term strategic risk: A risk that impacts an expected outcome of the project. That is, the results expected directly from a deliverable. For example, an order processing system (deliverable) might be expected to reduce processing time by 50% on average (outcome). Long term strategic risk: A risk that affects the strategic goal that the project is intended to address. For example, an expected  strategic outcome of a new order processing system  might be to boost sales by 25% over the next 2 years. It is also necessary to define unambiguous criteria by which risks can be assigned to one of the above categories. The authors use the following criteria to classify risks:  A risk is an operational risk if it can impact a deliverable that is set out in the project definition (scope document, charter etc.) or delivery contract. A risk is a short-term strategic if it can have an effect on functionality that is not clearly mentioned in the project documentation, but is required in order to achieve the project objectives. A risk is considered to be a long-term strategic if it affects the long-term goals of the project and does not fall into the prior two categories. The authors use the third category as a catch-all bucket for risks that do not fall into the first two categories.  Methodology The authors collected data from the risk registers of seven large projects. Prior to data collection, the conducted interviews with relevant project personnel to get an understanding of the goals and context of the project. Further interviews were conducted, as needed, mainly to clarify points that came up in the analysis.  A point to note is that the projects studied were all in progress, but in different phases ranging from initiation to  closure.  Results and discussion The authors' findings can be summed up in a line: the overwhelming majority of risks were operational. The fraction of risks that were classified as long-term strategic was less than 0.5 % of the total (with over 1300 risks were classified in all).  Why is the number of strategic risks so low? The authors offer the following reasons:  Strategic risks do not occur while a project is in progress: The authors argue that this is plausible because strategic risks are (or should be) handled prior to a project being given the go-ahead.  This makes sense, so in a well-vetted project strategic risks will occur only if there are substantial changes in the hosting organisation and/or its environment. Long term strategic risks are not the project's responsibility: This is a view taken by most project management methodologies: a project exists only to achieve its stated objectives; its long-term impact is irrelevant.  Put another way, the focus is on efficiency, not (organisational) effectiveness (I'll say more about this in a future post).  The authors recommend that project risk managers need to be aware of strategic issues, even though these are traditionally out of the purview of the project. Why? Well, because such issues  can have a  major impact on how the project is perceived by the organisation. Strategic risks are mainly the asset owner's (or sponsor's) responsibility: According to conventional management wisdom strategic risks are the responsibility of management, not the project team. In contrast, the authors suggest that the project team is perhaps better placed to identify some strategic risks long before they come to management's attention.  From personal experience I can vouch that this is true, but would add that it can be difficult to raise awareness of these risks in a politically acceptable way. Conclusion The main point that the article makes is that strategic risks, though often ignored, can have a huge effect on projects and how they are viewed by the larger organisation.  It is therefore in important that these risks are identified and escalated to sponsors and other decision makers in a timely manner.  This is a message that organisations would do well to heed, particularly those that have a "shoot the messenger" culture which discourages honest and open communication about such risks.Any future-directed activity has a degree of uncertainty, and uncertainty implies risk. Bad stuff happens – anticipated events don't unfold as planned and unanticipated events occur.  The main function of risk management is to deal with this negative aspect of uncertainty.  The events of the last few years suggest that risk management as practiced in many organisations isn't working.  A book by Douglas Hubbard entitled, The Failure of Risk Management – Why it's Broken and How to Fix It, discusses why many commonly used risk management practices are flawed and what needs to be done to fix them. This post is a summary and review of the book.  Interestingly, Hubbard began writing the book well before the financial crisis of 2008 began to unfold.  So although he discusses matters pertaining to risk management in finance, the book has a much broader scope. For instance, it will be of interest to project and  program/portfolio management professionals because many of the flawed risk management practices that Hubbard mentions are often used in project risk management.  The book is divided into three parts: the first part introduces the crisis in risk management; the second deals with why some popular risk management practices are flawed; the third discusses what needs to be done to fix these.  My review covers the main points of each section in roughly the same order as they appear in the book.  The crisis in risk management There are several risk management methodologies and techniques in use ;  a quick search will reveal some of them. Hubbard begins his book by asking the following simple questions about these:  Do these risk management methods work? Would any organisation that uses these techniques know if they didn't work? What would be the consequences if they didn't work His contention is that for most organisations the answers to the first two questions are negative.  To answer the third question, he gives the example of the crash of United Flight 232 in 1989. The crash was attributed to the simultaneous failure of three independent (and redundant) hydraulic systems. This happened because the systems were located at the rear of the plane and debris from a damaged turbine cut lines to all them.  This is an example of common mode failure – a single event causing multiple systems to fail.  The probability of such an event occurring was estimated to be less than one in a billion. However, the reason the turbine broke up was that it hadn't been inspected properly (i.e. human error).  The probability estimate hadn't considered human oversight, which is way more likely than one-in-billion.  Hubbard uses this example to make the point that a weak risk management methodology can have huge consequences.  Following a very brief history of risk management from historical times to the present, Hubbard presents a list of common methods of risk management. These are:  Expert intuition – essentially based on "gut feeling" Expert audit – based on expert intuition of independent consultants.  Typically involves the development  of checklists and also uses stratification methods (see next point) Simple stratification methods – risk matrices are the canonical example of stratification methods. Weighted scores – assigned scores for different criteria (scores usually assigned by expert intuition), followed by weighting based on perceived importance of each criterion. Non-probabilistic financial analysis –techniques such as computing the financial consequences of best and worst case scenarios Calculus of preferences – structured decision analysis techniques such as multi-attribute utility theory and analytic hierarchy process. These techniques are based on expert judgements. However, in cases where multiple judgements are involved these techniques ensure that the judgements are logically consistent  (i.e. do not contradict the principles of logic). Probabilistic models – involves building probabilistic models of risk events.  Probabilities can be based on historical data, empirical observation or even intuition.  The book essentially builds a case for evaluating risks using probabilistic models, and provides advice on how these should be built The book also discusses the state of risk management practice (at the end of 2008) as assessed by surveys carried out by The Economist, Protiviti and Aon Corporation. Hubbard notes that the surveys are based  largely on self-assessments of risk management effectiveness. One cannot place much confidence in these because self-assessments of risk are subject to well known psychological effects such as cognitive biases (tendencies to base judgements on flawed perceptions) and the Dunning-Kruger effect (overconfidence in one's abilities).  The acid test for any assessment  is whether or not it use sound quantitative measures.  Many of the firms surveyed fail on this count: they do not quantify risks as well as they claim they do. Assigning weighted scores to qualitative judgements does not count as a sound quantitative technique – more on this later.  So, what are some good ways of measuring the effectiveness of risk management? Hubbard lists the following:  Statistics based on large samples – the use of this depends on the availability of historical or other data that is similar to the situation at hand. Direct evidence – this is where the risk management technique actually finds some problem that would not have been found otherwise. For example, an audit that unearths dubious financial practices Component testing – even if one isn't able to test the method end-to-end, it may be possible to test specific components that make up the method. For example, if the method uses computer simulations, it may be possible to validate the simulations by applying them to known situations. Check of completeness – organisations need to ensure that their risk management methods cover the entire spectrum of risks, else there's a danger that mitigating one risk may increase the probability of another.  Further, as Hubbard states, "A risk that's not even on the radar cannot be managed at all." As far as completeness is concerned, there are four perspectives that need to be taken into account. These are: Internal completeness – covering all parts of the organisation External completeness – covering all external entities that the organisation interacts with. Historical completeness – this involves covering worst case scenarios and historical data. Combinatorial completeness – this involves considering combinations of events that may occur together; those that may lead to common-mode failure discussed earlier. Finally, Hubbard closes the first section with the observation that it is better not to use any formal methodology than to use one that is flawed. Why? Because a flawed methodology can lead to an incorrect decision being made  with high confidence.  Why it's broken Hubbard begins this section by identifying the four major players in the risk management game. These are:  Actuaries:  These are perhaps the first modern professional risk managers.  They use quantitative methods to manage risks in the insurance and pension industry.  Although the methods actuaries use are generally sound, the profession is slow to pick up new techniques. Further, many investment decisions that insurance companies do not come under the purview of actuaries. So, actuaries typically do not cover the entire spectrum of organizational risks. Physicists and mathematicians: Many rigorous risk management techniques came out of statistical research done during the second world war. Hubbard therefore calls this group War Quants. One of the notable techniques to come out of this effort is the Monte Carlo Method – originally proposed by Nick Metropolis, John Neumann and Stanislaw Ulam as a technique to calculate the averaged trajectories of neutrons in fissile material  (see this article by Nick Metropolis for a first-person account of how the method was developed). Hubbard believes that Monte Carlo simulations offer a sound, general technique for quantitative risk analysis. Consequently he spends a fair few pages discussing these methods, albeit at a very basic level. More about this later. Economists:  Risk analysts in investment firms often use quantitative techniques from economics.  Popular techniques include modern portfolio theory and models from options theory (such as the Black-Scholes model) . The problem is that these models are often based on questionable assumptions. For example, the Black-Scholes model assumes that the rate of return on a stock is normally distributed (i.e.  its value is lognormally distributed) – an assumption that's demonstrably incorrect as witnessed by the events of the last few years .  Another way in which economics plays a role in risk management is through behavioural studies,  in particular the recognition that decisions regarding future events (be they risks or stock prices) are subject to cognitive biases. Hubbard suggests that the role of cognitive biases in risk management has been consistently overlooked. See my post entitled Cognitive biases as meta-risks and its follow-up for more on this point. Management consultants: In Hubbard's view, management consultants and standards institutes are largely responsible for many of the ad-hoc approaches  to risk management. A particular favourite of these folks are ad-hoc scoring methods that involve ordering of risks based on subjective criteria. The scores assigned to risks are thus subject to cognitive bias. Even worse, some of the tools used in scoring can end up ordering risks incorrectly.  Bottom line: many of the risk analysis techniques used by consultants and standards have no justification. Following the discussion of the main players in the risk arena, Hubbard discusses the confusion associated with the definition of risk. There are a plethora of definitions of risk, most of which originated in academia. Hubbard shows how some of these contradict each other while others are downright non-intuitive and incorrect. In doing so, he clarifies some of the academic and professional terminology around risk. As an example, he takes exception to the notion of risk as a "good thing" – as in the PMI definition, which views risk as  "an uncertain event or condition that, if it occurs, has a positive or negative effect on a project objective."  This definition contradicts common (dictionary) usage of the term risk (which generally includes only bad stuff).  Hubbard's opinion on this may raise a few eyebrows (and hackles!) in project management circles, but I reckon he has a point.  In my opinion, the most important sections of the book are chapters 6 and 7, where Hubbard discusses why "expert knowledge and opinions" (favoured by standards and methodologies are flawed) and why a very popular scoring method (risk matrices) is "worse than useless."  See my posts on the  limitations of scoring techniques and Cox's risk matrix theorem for detailed discussions of these points.  A major problem with expert estimates is overconfidence. To overcome this, Hubbard advocates using calibrated probability assessments to quantify analysts' abilities to make estimates. Calibration assessments involve getting analysts to answer trivia questions and eliciting confidence intervals for each answer. The confidence intervals are then checked against the proportion of correct answers.  Essentially, this assesses experts' abilities to estimates by tracking how often they are right. It has been found that  people can improve their ability to make subjective estimates through calibration training – i.e. repeated calibration testing followed by feedback. See this site for more on probability calibration.  Next Hubbard tackles several "red herring" arguments that are commonly offered as reasons not to manage risks using rigorous quantitative methods.  Among these are arguments that quantitative risk analysis is impossible because:  Unexpected events cannot be predicted. Risks cannot be measured accurately. Hubbard states that the first objection is invalid because although some events (such as spectacular stockmarket crashes) may have been overlooked by models, it doesn't prove that quantitative risk as a whole is flawed. As he discusses later in the book, many models go wrong by assuming Gaussian probability distributions where fat-tailed ones would be more appropriate. Of course, given limited data it is difficult to figure out which distribution's the right one. So, although Hubbard's argument is correct, it offers little comfort to the analyst who has to model events before they occur.  As far as the second is concerned, Hubbard has written another book on how just about any business variable (even intangible ones) can be measured. The book makes a persuasive case that most quantities of interest can be measured, but there are difficulties.  First, figuring out the factors that affect a variable  is not a straightforward task.  It depends, among other things,  on the availability of reliable data, the analyst's experience etc. Second, much depends on the judgement of the analyst, and such judgements are subject to bias. Although calibration may help reduce certain biases such as overconfidence, it is by no means a panacea for all biases.  Third, risk-related measurements generally  involve events that are yet to occur.  Consequently, such measurements are  based on  incomplete information.  To make progress one often has to make additional assumptions which may not justifiable a priori.  Hubbard is a strong advocate for quantitative techniques such as Monte Carlo simulations in managing risks. However,  he believes that they are often used incorrectly.  Specifically:  They are often used without empirical data or validation – i.e. their inputs and results are not tested through observation. Are generally used piecemeal – i.e. used in some parts of an organisation only, and often to manage low-level, operational risks. They frequently focus on variables that are not important (because these are easier to measure) rather than those that are important. Hubbard calls this perverse occurrence measurement inversion. He contends that analysts often exclude the most important variables because these are considered to be "too uncertain." They use inappropriate probability distributions. The Normal distribution (or bell curve) is not always appropriate. For example, see my posts on the inherent uncertainty of project task estimates for an intuitive discussion of the form of the probability distribution for project task durations. They do not account for correlations between variables. Hubbard contends that many analysts simply ignore correlations between risk variables (i.e. they treat variables as independent when they actually aren't). This almost always leads to an underestimation of risk because correlations can cause feedback effects and common mode failures. Hubbard dismisses the argument that rigorous quantitative methods such as Monte Carlo are "too hard." I  agree, the principles behind Monte Carlo techniques aren't hard to follow – and I take the opportunity to plug my article entitled  An introduction to Monte Carlo simulations of project tasks  .  As far as practice is concerned,  there are several commercially available tools that automate much of the mathematical heavy-lifting. I won't recommend any, but a search using the key phrase monte carlo simulation tool will reveal many.  How to Fix it The last part of the book outlines Hubbard's recommendations for improving the practice of risk management. Most of the material presented here draws on the previous section of the book. His main suggestions are to:  Adopt the language, tools and philosophy of uncertain systems. To do this he recommends: Using calibrated probabilities to express uncertainties. Hubbard believes that any person who makes estimates that will be used in models should be calibrated. He offers some suggestions on people can improve their ability to estimate through calibration – discussed earlier and on this web site. Employing quantitative modelling techniques to model risks. In particular, he advocates the use of Monte Carlo methods to model risks. He also provides a list of commercially available PC-based Monte Carlo tools. Hubbard makes the point that modelling forces analysts to decompose the systems  of interest and understand the relationships between their components (see point 2 below). Developing an understanding of the basic rules of probability including independent events, conditional probabilities and Bayes' Theorem. He gives examples of situations in which these rules can help analysts extrapolate To this, I would also add that it is important to understand the idea that an estimate isn't a number, but a  probability distribution – i.e. a range of numbers, each with a probability attached to it.  Build, validate and test models using reality as the ultimate arbiter. Models should be built iteratively, testing each assumption against observation. Further, models need to incorporate mechanisms (i.e. how and why the observations are what they are), not just raw observations. This is often hard to do, but at the very least models should incorporate correlations between variables.  Note that correlations are often (but not always!) indicative of an underlying mechanism. See this post for an introductory example of Monte Carlo simulation involving correlated variables. Lobbying for risk management to be given appropriate visibility in organisation.s In the penultimate chapter of the book, Hubbard fleshes out the characteristics or traits of good risk analysts. As he mentions several times in the book, risk analysis is an empirical science – it arises from experience. So, although the analytical and mathematical  (modelling) aspects of risk are important,  a good analyst must, above all, be an empiricist – i.e. believe that knowledge about risks can only come from observation of reality. In particular, tesing models by seeing how well they match historical data and tracking model predictions are absolutely critical aspects of a risk analysts job. Unfortunately, many analysts do not measure the performance of their risk models. Hubbard offers some excellent suggestions on how analysts can refine and improve their models via observation.  Finally, Hubbard emphasises the importance of creating an organisation-wide approach to managing risks. This ensures that organisations will tackle the most important risks first, and that its risk management budgets  will be spent in the most effective way. Many of the tools and approaches that he suggests in the book are most effective if they are used in a consistent way across the entire organisation. In reality, though,  risk management languishes way down in the priorities of senior executives. Even those who profess to understanding the  importance of managing risks in a rigorous way, rarely offer risk managers the organisational visibility and support they need to do their jobs.Anticipating and dealing with risks is an important part of managing projects. So much so that most frameworks and methodologies devote a fair bit of attention to risk management:  for example, the PMI framework considers risk management to be one of the nine "knowledge areas" of project management.  Now, frameworks and methodologies are normative– that is. they us how risks should be managed – but they don't say anything about how are risks actually handled on projects.  It is perhaps too much  expect that all projects are run with  the full machinery of  formal risk management, but it is reasonable to  expect that most project managers deal with risks in some more or less systematic way.  However, project management lore is rife with stories of projects on which risks were managed inadequately, or not managed at all (see this post for some pertinent case studies).  This begs the question:  are there rational reasons for not managing risks on projects?   A paper by Elmar Kutsch and Mark Hall entitled, The Rational Choice of Not Applying Project Risk Management in Information Technology Projects,  addresses this question.  This post is a summary and review of the paper.  Background The paper begins with a brief overview of risk management as prescribed by various standards. Risk management is about making decisions in the face of uncertainty. To make the right decisions, project managers need to figure out which risks are the most significant. Consequently, most methodologies offer techniques to rank risks based on various criteria.  These techniques are based on many (rather strong) assumptions, which the authors summarise as follows:  An unambiguous identification of the problem (or risk) including its cause Perfect information about all relevant variables that affect the risk. A model of the risk that incorporates the aforementioned variables. A complete list of possible approaches to tackle the risks. An unambiguous, quantitative and internally consistent measure for the outcomes of each approach. Perfect knowledge of the consequences of each approach. Availability of resources for the successful implementation of the chosen solution. The presence of rational decision-makers (i.e. folks free from cognitive bias for example) Most formal methodologies assume the above to be "self-evidently correct" (note that some of them aren't correct, see my  posts on cognitive biases as project meta-risks and the limitations of scoring methods in risk analysis for more). Anyway, regardless of the validity of the assumptions,  it is clear that achieving all the above would require a great deal of commitment, effort and money.   This, according to the authors,  provides a hint as to why many projects are run without formal risk management. In their words:  …despite the existence of a self-evidently correct process to manage  project risk, some evidence suggests that project managers feel restricted in applying such an "optimal" process to manage risks. For example, Lyons and Skitmore (2004) investigated factors limiting the implementation of risk management in Australian construction projects. Similar findings about the barriers of using risk management in three Hong Kong industries were found in a further prominent study by Tummala, Leung, Burchett, and Leung (1997). The most dominant factors for constraining the use of project risk management are the lack of time, the problem of justifying the effort into project risk management, and the lack of information required to quantify/qualify risk estimates.  The authors review the research literature to find other factors that could reduce the likelihood of risk management being applied in projects. Based on their findings, they suggest the following as reasons  that project managers often offer as  justifications (or rationales)  for not managing risks:  The problem of hindsight: Most risk management methodologies rely on historical data to calculate probabilities of risk eventuation. However, many managers feel they cannot rely on such data for their specific (unique) project. The problem of ownership: Risks are often thought of as "someone else's problem". There is often a reluctance to take ownership of a risk because of the fear of blame in case the risk response fails to address the risk. The problem of cost justification: From the premises listed above it is clear that proper risk management is a time-consuming, effort-laden and expensive process. Many IT projects are run on tight budgets, and risk management is an area that's perceived as being an unnecessary expense. Lack of expertise: Project managers might be unaware of risk management technique.  I find this hard to believe, given that practically all textbooks and methodologies yammer on,  at great length, about the importance of managing risks. Besides, it is a pretty weak justification! The problem of anxiety:  By definition, risk management implies that one is considering things that can go wrong.  Sometimes, when informed about risks, stakeholders may decide not to go ahead with a project. Consequently, project managers may limit their risk identification efforts in an attempt to avoid making stakeholders nervous. When justifying the decision not to manage risks, the above factors are often presented as barriers or problems which prevent the project manager from using risk management. As an illustration of (5) above, a project manager might say, "I can't talk about risks on my project because the sponsor will freak out and throw me out of his office."  Research Method The authors started with an exploratory study aimed at developing an understanding of the problem from the perspective of IT project managers – i.e. how project managers actually experience the application of risk management on their projects. This study was done through face-to-face interviews. Based on patterns that emerged from this study, the authors developed a web-based survey that was administered to a wider group of project managers. The exploratory phase involved eighteen project managers whereas the in-depth survey was completed by just over a hundred  project managers all of whom were members of the PMI Risk Management Special Interest Group. Although the paper doesn't say so, I assume that project managers were asked questions in reference to a specific project they were involved in (perhaps the most recent one?).  I won't dwell any more on the research methodology;  the paper has all the  details.  Results and interpretation Four of the eighteen project managers interviewed in the exploratory study did not apply risk management processes on their projects. The reasons given were interpreted by the authors as cost justification, hindsight and anxiety. I've italicized the word "interpreted" in the previous sentence because I believe the responses given by the project managers could just as easily be interpreted another way. I've presented their arguments below so that readers can judge for themselves.  One interviewee mentioned that, "At the beginning, we had so much to do that no one gave a thought to tackling risks. It  simply did not happen." The authors conclude that the rationale for not managing risks in this case is one of cost justification, the chain of logic being that due to the lack of time, investment of resources in managing risks was not justified. To me this seems to read too much into the response. From the response it appears to me that the real reason is exactly what the interviewee states –  "no one thought of managing risks" – i.e. risks were  overlooked.  Another interviewee stated, "It would have been nice to do it differently, but because we were quite vulnerable in terms of software development, and because most of that was driven by the States, we were never in a position to be proactive. The Americans would say "We got an update to that system and we just released it to you," rather than telling us a week in advance that something was happening. We were never ahead enough to be able to plan." The authors interpret the lack of risk management in the this case as being due to the problem of hindsight – i.e.  because the risk that an update poses to other parts of the system could not have been anticipated, no risk management was possible. To me this interpretation seems a little thin – surely, most project managers understand the risks that arbitrary updates pose. From the response it appears that the real reason was that the project manager was not able to plan ahead because he/she had no advance warning of updates. This seems more a problem of a broken project management process rather than anything to do with risk management or hindsight. My point: the uncertainty here was known (high probability of regular updates),  so something could (and should) have been done about it whilst planning the project.  I've dwelt on these examples because it appears that the authors may have occasionally fallen into the trap of pigeon holing interviewee responses into their predefined rationales (the ones discussed in the previous section)  instead of listening to what was actually being said.  Of course, my impression is based on a reading of the paper and the data presented therein. The authors may well have other (unpublished) information to support their classification of interviewee responses. However, if that is the case, they should have presented the data in the paper  because the reliability of the second survey  depends on the set of predefined rationales being comprehensive  and correct.  The authors present a short discussion of the second phase of their study. They find that no formal risk management processes were used in about one third of the 102 cases studied. As the authors point out, that in itself is an interesting statistic, especially considering the money at stake in typical IT projects. In cases where no risk management was applied, respondents were asked to provide reasons why this was so. The reasons given were extremely varied but, once again, the authors pigeon-holed these into their predefined categories. I present some of the original responses and interpretations below so that readers can judge for themselves.  Consider the following reasons that were offered (by respondents) for not applying risk management:  "We haven't got time left." "No executive call for risk measurements." "Company doesn't see the value in adding the additional cycles to a project." (?) "Upper management did not think it required it." "Ignorance that such a thing was necessary." "An initial risk analysis was done, but the PM did not bother to follow up." "A single risk identification workshop was held early in the project before my arrival. Reason for not following the process was most probably the attitude of the members of the team." Interestingly, the authors interpret all the above responses (and a few more ) as being attributable to the cost justification rationale. However, it seems to me that there could be several other (more likely) interpretations.  For example: 2, 3, 4, 5 could be attributed to a lack of knowledge about the value of managing risks whereas 1, 6, 7 sound more like simple (and unfortunately, rather common!)  buck-passing.  Conclusion Towards the end of the paper  the authors make an excellent point about the  rationality of a decision not to apply risk management. From the perspective of formal methodoologies such a decision is irrational. However, rationality (or the lack of it) isn't so cut and dried. Here's what the authors say:  …a decision by an IT project manager not to apply project risk management may be described as irrational, at least if one accepts the premise that the project manager chose not to apply a "self-evidently" correct process to optimally reduce the impact of risk on the project outcome. On the other hand, … a person who focuses only on the statistical probability of threats and their impacts and ignores any other information would be truly irrational. Hence, a project manager would act sensibly by, for example, not applying project risk management because he or she rates the utility of not using project risk management as higher than the utility of confronting stakeholders with discomforting information…."  …or spending money to address issues that may not eventuate, for that matter. The point being  that people don't make decisions based on prescribed processes and procedures alone; there are other considerations.  The authors then go on to say,  PMI and APM claim that through the systematic identification, analysis, and response to risk, project managers can achieve the planned project outcome. However, the findings show that in more than one-third of all projects, the effectiveness of project risk management is virtually nonexistent because no formal project risk management process was applied due to the problem of cost justification.  Now, although it is undeniable that many projects are run with no risk management whatsoever,  I'm not sure I agree with the last statement in the quote. From the data presented in the paper, it seems more likely that a lack of knowledge  and "buck-passing"  are the prime reasons  for risk management being given short shrift on the projects surveyed. Even if cost justification was  offered as a rationale by some  interviewees,  their quotes suggest that the real reasons were quite different. This isn't surprising: it is but natural  to attribute to unacceptable costs that which should be attributed to oversight or failure.  I think this may be the case in a large number of projects on which risks aren't managed. However,  as the authors mention, it is impossible to make any generalisations based on small samples .  So, although it is incontrovertible that there are a significant number of projects on which risks aren't managed, why this is so remains an open question.A couple of months ago I wrote an article highlighting some of the pitfalls of using risk matrices. Risk matrices are an example of scoring methods , techniques which use ordinal scales to assess risks. In these methods,  risks are ranked by some predefined criteria such as impact or expected loss, and the ranking  is then used as the basis for  decisions on how the risks should be addressed. Scoring methods are popular because they are easy to use. However,  as Douglas Hubbard points out in his critique of current risk management practices, many commonly used scoring techniques are flawed. This post – based on Hubbard's critique and research papers quoted therein –  is a brief look at some of the flaws of risk scoring techniques.  Commonly used risk scoring techniques and problems associated with them Scoring techniques fall under two major categories:  Weighted scores: These use several ordered scales which are weighted according to perceived importance. For example: one might be asked to rate financial risk, technical risk and organisational risk on a scale of 1 to 5 for each, and then weight then by factors of 0.6, 0.3 and 0.1 respectively (possibly because the CFO – who happens to be the project sponsor – is more concerned about financial risk than any other risks ). The point is, the scores and weights assigned can be highly subjective – more on that below. Risk matrices: These rank risks along two dimensions – probability and impact – and assign them a qualitative ranking of high, medium or low depending on where they fall.  Cox's theorem shows such categorisations are internally inconsistent because the category boundaries are arbitrarily chosen. Hubbard makes the point that, although both the above methods are endorsed by many standards and methodologies (including those used in project management), they should be used with caution because they are flawed. To quote from his book:  Together these ordinal/scoring methods are the benchmark for the analysis of risks and/or decisions in at least some component of most large organizations. Thousands of people have been certified in methods based in part on computing risk scores like this. The major management consulting firms have influenced virtually all of these standards. Since what these standards all have in common is the used of various scoring schemes instead of actual quantitative risk analysis methods, I will call them collectively the "scoring methods." And all of them, without exception, are borderline or worthless. In practices, they may make many decisions far worse than they would have been using merely unaided judgements.  What is the basis for this claim? Hubbard points to the following:  Scoring methods do not make any allowance for flawed perceptions of analysts who assign scores – i.e. they do not consider the effect of cognitive bias. I won't dwell on this as I have  previously written  about the effect of cognitive biases in project risk management -see this post and this one, for example. Qualitative descriptions assigned to each score are understood differently by different people. Further, there is rarely any objective guidance as to how an analyst is to distinguish between a high or medium risk. Such advice may not even help: research by Budescu, Broomell and Po shows that there can be huge variances in understanding of qualitative descriptions, even when people are given specific guidelines what the descriptions or terms mean. Scoring methods add their own errors.  Below are brief descriptions of some of these: In his paper on the risk matrix theorem, Cox mentions that "Typical risk matrices can correctly and unambiguously compare only a small fraction (e.g., less than 10%) of randomly selected pairs of hazards. They can assign identical ratings to quantitatively very different risks." He calls this behaviour "range compression" – and it applies to any scoring technique that uses ranges. Assigned scores tend to cluster around the mid-low high range. Analysis by Hubbard shows that, on a 5 point scale, 75% of all responses are 3 or 4. This implies that changing a score from 3 to 4 or vice-versa can have a disproportionate effect on classification of risks. Scores implicitly assume that the magnitude of the quantity being assumed is directly proportional to the scale. For example, a score of 2 implies that the criterion being measured is twice as large as it would be for a score of 1. However, in reality, criteria are rarely linear as implied by such a scale. Scoring techniques often presume that the factors being scored are independent of each other – i.e. there are no correlations between factors. This assumption  is rarely tested or justified in any way. Many project management standards advocate the use of scoring techniques.  To be fair, in many situations they are adequate as long as they are used with an understanding of their limitations. Seen in this light, Hubbard's book is  an admonition to standards and textbook writers to be more critical of the methods they advocate, and a warning to practitioners that an uncritical adherence to standards and best practices is not the best way to manage project risks .  Scoring done right Just to be clear, Hubbard's criticism is directed against  scoring methods that use arbitrary, qualitative scales which are not justified by independent analysis. There are other techniques which, though superficially similar to these flawed scoring methods, are actually quite robust because they are:  Based on observations. Use real measures (as opposed to arbitrary ones – such as "alignment with business objectives" on a scale of 1 to 5, without defining what "alignment" means.) Validated after the fact (and hence refined with use). As an example  of a sound scoring technique, Hubbard quotes this paper by Dawes, which presents evidence that linear scoring models are superior to intuition in clinical judgements. Strangely, although the weights themselves can be obtained through intuition, the scoring model outperforms clinical intuition. This happens because human intuition is good at identifying important factors, but not so hot at evaluating the net effect of several, possibly competing factors. Hence simple linear scoring models can outperform intuition. The key here is that the models are validated by checking the predictions against reality.  Another class of techniques use axioms based on logic to reduce inconsistencies in decisions. An example of such a technique is multi-attribute utility theory. Since they are based on logic, these methods can also be considered to have a solid foundation unlike those discussed in the previous section.  Conclusions Many commonly used scoring methods in risk analysis are based on flaky theoretical foundations – or worse, none at all. To compound the problem, they are often used without any validation.  A particularly ubiquitous example is the well-known and loved risk matrix.  In his paper on risk matrices,  Tony Cox  shows how risk matrices can sometimes lead to decisions that are worse than those made on the basis of a coin toss.  The fact that this is a possibility – even if only a  small one – should worry anyone who uses risk matrices  (or other flawed scoring techniques) without an understanding of their limitations.Issue Based Information System (IBIS) is a notation invented by Horst Rittel and Werner Kunz in the early 1970s.  IBIS is best known for its use in dialogue mapping, a collaborative approach to tackling wicked problems (i.e. contentious issues) in organisations. It has a range of other applications as well – capturing knowledge is a good example, and I'll have much more to say about that later in this post.  Over the last five years or so, I have written a fair bit on IBIS on this blog and in a book that I co-authored with the dialogue mapping expert, Paul Culmsee.  The present post reprises an article I wrote five years ago on the "what" and "whence" of the notation:  its practical aspects – notation, grammar etc -, as well as its origins, advantages and limitations. My motivations for revisiting the piece are to revise and update the original discussion and, more important, to cover some recent developments in IBIS technology that open up interesting possibilities in the area of knowledge management.  To appreciate the power of the IBIS, it is best to begin by understanding the context in which the notation was invented.  I'll therefore start with a discussion of the origins of the notation followed by an introduction to it. Finally, I'll cover its development through the last 40 odd years, focusing on the recent developments that I mentioned above.  Wicked origins A good place to start is where it all started. IBIS was first described in a paper entitled, Issues as elements of Information Systems; written by Horst Rittel (the man who coined the term wicked problem) and Werner Kunz in July 1970. They state the intent behind IBIS in the very first line of the abstract of their paper:  Issue-Based Information Systems (IBIS) are meant to support coordination and planning of political decision processes. IBIS guides the identification, structuring, and settling of issues raised by problem-solving groups, and provides information pertinent to the discourse.  Rittel's preoccupation was the area of public policy and planning – which is also the context in which he defined the term wicked problem originally. Given the above background it is no surprise that Rittel and Kunz foresaw IBIS to be the:  …type of information system meant to support the work of cooperatives like governmental or administrative agencies or committees, planning groups, etc., that are confronted with a problem complex in order to arrive at a plan for decision…  The problems tackled by such cooperatives are paradigm-defining examples of wicked problems. From the start, then, IBIS was intended as a tool to facilitate a collaborative approach to solving…or better, managing a wicked problem by helping develop a shared perspective on it.  A brief introduction to IBIS The IBIS notation consists of the following three elements:  Issues(or questions): these are issues that are being debated. Typically, issues are framed as questions on the lines of "What should we do about X?" where X is the issue that is of interest to a group. For example, in the case of a group of executives, X might be rapidly changing market condition whereas in the case of a group of IT people, X could be an ageing system that is hard to replace. Ideas(or positions): these are responses to questions. For example, one of the ideas of offered by the IT group above might be to replace the said system with a newer one. Typically the whole set of ideas that respond to an issue in a discussion represents the spectrum of participant perspectives on the issue. Arguments: these can be Pros (arguments for) or Cons (arguments against) an issue. The complete set of arguments that respond to an idea represents the multiplicity of viewpoints on it. Compendium is a freeware tool that can be used to create IBIS maps– it can be downloaded here.  In Compendium, the IBIS elements described above are represented as nodes as shown in Figure 1: issues are represented by blue-green question marks; positions by yellow light bulbs; pros by green + signs and cons by red – signs.  Compendium supports a few other node types, but these are not part of the core IBIS notation. Nodes can be linked only in ways specified by the IBIS grammar as I discuss next.   Figure 1: IBIS elements Figure 1: IBIS elements  The IBIS grammar can be summarized in three simple rules:  Issues can be raised anew or can arise from other issues, positions or arguments. In other words, any IBIS element can be questioned.  In Compendium notation:  a question node can connect to any other IBIS node. Ideas can only respond to questions– i.e. in Compendium "light bulb" nodes can only link to question nodes. The arrow pointing from the idea to the question depicts the "responds to" relationship. Arguments  can only be associated with ideas– i.e. in Compendium "+" and "–"  nodes can only link to "light bulb" nodes (with arrows pointing to the latter) The legal links are summarized in Figure 2 below.  Figure 2: Legal links in IBIS Figure 2: Legal links in IBIS  Yes, it's as simple as that.  The rules are best illustrated by example-  follow the links below to see some illustrations of IBIS in action:  See this postfor a simple example of dialogue mapping. See this postor this one for examples of argument visualisation. (Note: using IBIS to map out the structure of written arguments is called issue mapping. See this one for an example Paul did with his children. This example also features in our book. that made an appearance in our book. Now that we know how IBIS works and have seen a few examples of it in action, it's time to trace the history of the notation from its early days the present.  Operation of early systems When Rittel and Kunz wrote their paper, there were three IBIS-type systems in operation: two in government agencies (in the US, one presumes) and one in a university environment (quite possibly Berkeley, where Rittel worked). Although it seems quaint and old-fashioned now, it is no surprise that these were manual, paper-based systems; the effort and expense involved in computerizing such systems in the early 70s would have been prohibitive and the pay-off questionable.  The Rittel-Kunz paper introduced earlier also offers a short description of how these early IBIS systems operated:  An initially unstructured problem area or topic denotes the task named by a "trigger phrase" ("Urban Renewal in Baltimore," "The War," "Tax Reform"). About this topic and its subtopics a discourse develops. Issues are brought up and disputed because different positions (Rittel's word for ideas or responses) are assumed. Arguments are constructed in defense of or against the different positions until the issue is settled by convincing the opponents or decided by a formal decision procedure. Frequently questions of fact are directed to experts or fed into a documentation system. Answers obtained can be questioned and turned into issues. Through this counterplay of questioning and arguing, the participants form and exert their judgments incessantly, developing more structured pictures of the problem and its solutions. It is not possible to separate "understanding the problem" as a phase from "information" or "solution" since every formulation of the problem is also a statement about a potential solution.  Even today, forty years later, this is an excellent description of how IBIS is used to facilitate a common understanding of complex problems.  Moreover, the process of reaching a shared understanding (whether using IBIS or not) is one of the key ways in which knowledge is created within organizations. To foreshadow a point I will elaborate on later, using IBIS to capture the key issues, ideas and arguments, and the connections between them, results in a navigable map of the knowledge that is generated in a discussion.  Fast forward a couple decades (and more!) In a paper published in 1988 entitled, gIBIS: A hypertext tool for exploratory policy discussion, Conklin and Begeman describe a prototype of a graphical, hypertext-based  IBIS-type system (called gIBIS) and its use in capturing design rationale (yes, despite the title of the paper, it is more about capturing design rationale than policy discussions). The development of gIBIS represents a key step between the original Rittel-Kunz version of IBIS and its more recent version as implemented in Compendium.  Amongst other things, IBIS was finally off paper and on to disk, opening up a world of new possibilities.  gIBIS aimed to offer users:  The ability to capture design rationale – the options discussed (including the ones rejected) and the discussion around the pros and cons of each. A platform for promoting computer-mediated collaborativedesign work – ideally in situations where participants were located at sites remote from each other. The ability to store a large amount of information and to be able to navigate through it in an intuitive way. The gIBIS prototype proved successful enough to catalyse the development of Questmap, a commercially available software tool that supported IBIS.  In a recent conversation Jeff Conklin mentioned to me that Questmap was one of the earliest Windows-based groupware tools available on the market…and it won a best-of-show award in that category. It is interesting to note that in contrast to Questmap (which no longer exists), Compendium is a single-user, desktop software.  The primary application of Questmap was in the area of sensemaking which is all about helping groups reach a collective understanding of complex situations that might otherwise lead them into tense or adversarial conditions. Indeed, that is precisely how Rittel and Kunz intended IBIS to be used.  The key advantage offered by computerized IBIS systems was that one could map dialogues in real-time, with the map representing the points raised in the conversation along with their logical connections. This proved to be a big step forward in the use of IBIS to help groups achieve a shared understanding of complex issues.  That said, although there were some notable early successes in the real-time use of IBIS in industry environments (see this paper, for example), these were not accompanied by widespread adoption of the technique. It is worth exploring the reasons for this briefly.  The tacitness of IBIS mastery The reasons for the lack of traction of IBIS-type techniques for real-time knowledge capture are discussed in a paper by Shum et. al. entitled, Hypermedia Support for Argumentation-Based Rationale: 15 Years on from gIBIS and QOC.  The reasons they give are:  For acceptance, any system must offer immediate value to the person who is using it. Quoting from the paper, "No designer can be expected to altruistically enter quality design rationale solely for the possible benefit of a possibly unknown person at an unknown point in the future for an unknown task. There must be immediate value." Such immediate value is not obvious to novice users of IBIS-type systems. There is some effort involved in gaining fluency in the use of IBIS-based software tools. It is only after this that users can gain an appreciation of the value of such tools in overcoming the limitations of mapping design arguments on paper, whiteboards etc. While the rules of IBIS are simple to grasp, the intellectual effort – or cognitive overhead in using IBIS in real time involves:  Teasing out issues, ideas and arguments from the dialogue. Classifying points raised into issues, ideas and arguments. Naming (or describing) the point succinctly. Relating (or linking) the point to the existing map (or anticipating how it will fit in later) Developing a sense for conversational patterns. Expertise in these skills can only be developed through sustained practice, so it is no surprise that beginners find it hard to use IBIS to map dialogues.  Indeed, the use of IBIS for real-time conversation mapping is a tacit skill, much like riding a bike or swimming – it can only be mastered by doing.  Making sense through IBIS Despite the difficulties of mastering IBIS, it is easy to see that it offers considerable advantages over conventional methods of documenting discussions. Indeed, Rittel and Kunz were well aware of this. Their paper contains a nice summary of the advantages, which I paraphrase below:  IBIS can bridge the gap between discussions and records of discussions (minutes, audio/video transcriptions etc.). IBIS sits between the two, acting as a short-term memory. The paper thus foreshadows the use of issue-based systems as an aid to organizational or project memory. Many elements (issues, ideas or arguments) that come up in a discussion have contextual meanings that are different from any pre-existing definitions. That is, the interpretation of points made or questions raised depends on the circumstances surrounding the discussion. What is more important is that contextual meaning is more important than formal meaning. IBIS captures the former in a very clear way – for example a response to a question "What do we mean by X?" elicits the meaning of X in the context of the discussion, which is then subsequently captured as an idea (position)". I'll have much more to say about this towards the end of this article. The reasoning used in discussions is made transparent, as is the supporting (or opposing) evidence. The state of the argument (discussion) at any time can be inferred at a glance (unlike the case in written records). See this post for more on the advantages of visual documentation over prose. Often times it happens that the commonality of issues with other, similar issues might be more important than its precise meaning. To quote from the paper, "…the description of the subject matter in terms of librarians or documentalists (sic) may be less significant than the similarity of an issue with issues dealt with previously and the information used in their treatment…"  This is less of an issue now because of search of technologies. However, search technologies are still largely based on keywords rather than context. A properly structured, context-searchable IBIS-based archive would be more useful than a conventional document-based system. As I'll discuss in the next section, the technology for this is now available. To sum up, then: although IBIS offers a means to map out arguments what is lacking is the ability to make these maps available and searchable across an organization.  IBIS in the enterprise It is interesting to note that Compendium, unlike its predecessor, Questmap, is a single-user, desktop tool – it does not, by itself, enable the sharing of maps across the enterprise. To be sure, it is possible work around this limitation but the workarounds are somewhat clunky.  A recent advance in IBIS technology addresses this issue rather elegantly: Seven Sigma, an Australian consultancy founded by Paul Culmsee, Chris Tomich and Peter Chow, has developed Glyma (pronounced "glimmer"): a product that makes IBIS available on collaboration platforms like Microsoft SharePoint. This is a game-changer because it enables sharing and searching of IBIS maps across the enterprise. Moreover, as we shall see below, the implications of this go beyond sharing and search.  Full Disclosure: As regular readers of this blog know, Paul is a good friend and we have jointly written a book and a few papers. However, at the time of writing, I have no commercial association with Seven Sigma.  My comments below are based on playing with beta version of the product that Paul was kind enough to give me to access to as well as some discussions that I have had with him.  Figure 3: IBIS in Glyma Figure 3: IBIS in Glyma (Click to see larger picture)  The look and feel of Glyma is much the same as Compendium (see Fig 3 above) – and the keystrokes and shortcuts are quite similar. I have trialled Glyma for a few weeks and feel that the overall experience is actually a bit better than in Compendium. For example one can navigate through a series of maps and sub-maps using a breadcrumb trail. Another example: documents and videos are embedded within the map – so one does not need to leave the map in order to view tagged media (unless of course one wants to see it at a higher resolution).  I won't go into any detail about product features etc. since that kind of information is best accessed at source – i.e. the product website and documentation. Instead, I will now focus on how Glyma addresses a longstanding gap in knowledge management systems.  Revisiting the problem of context In most organisations, knowledge artefacts (such as documents and audio-visual media) are stored in hierarchical or relational structures (for example, a folder within a directory structure or a relational database). To be sure, stored artefacts are often tagged with metadata indicating when, where and by whom they were created, but experience tells me that such metadata is not as useful as it should be.  The problem is that the context in which an artefact was created is typically not captured. Anyone who has read a document and wondered, "What on earth were the authors thinking when they wrote this?" has encountered the problem of missing context.  Context, though hard to capture, is critically important in understanding the content of a knowledge artefact. Any artefact, when accessed without an appreciation of the context in which it was created is liable to be misinterpreted or only partially understood.  Capturing context in the enterprise Glyma addresses the issue of context rather elegantly at the level of the enterprise.  I'll illustrate this point an inspiring case study on the innovative use of SharePoint in education that Paul has written about some time ago.  The case study  Here is the backstory in Paul's words:  Earlier this year, I met Louis Zulli Jnr – a teacher out of Florida who is part of a program called the Centre of Advanced Technologies. We were co-keynoting at a conference and he came on after I had droned on about common SharePoint governance mistakes…The majority of Lou's presentation showcased a whole bunch of SharePoint powered solutions that his students had written. The solutions were very impressive…We were treated to examples like:  IOS, Android and Windows Phone apps that leveraged SharePoint to display teacher's assignments, school events and class times; Silverlight based application providing a virtual tour of the campus; Integration of SharePoint with Moodle (an open source learning platform) An Academic Planner web application allowing students to plan their classes, submit a schedule, have them reviewed, track of the credits of the classes selected and whether a student's selections meet graduation requirements; All of this and more was developed by 16 to 18 year olds and all at a level of quality that I know most SharePoint consultancies would be jealous of…  Although the examples highlighted by Louis were very impressive, what Paul found more interesting were the anecdotes that Lou related about the dedication and persistence that students displayed in their work. Quoting again from Paul,  So the demos themselves were impressive enough, but that is actually not what impressed me the most. In fact, what had me hooked was not on the slide deck. It was the anecdotes that Lou told about the dedication of his students to the task and how they went about getting things done. He spoke of students working during their various school breaks to get projects completed and how they leveraged each other's various skills and other strengths. Lou's final slide summed his talk up brilliantly:  Students want to make a difference! Give them the right project and they do incredible things. Make the project meaningful. Let it serve a purpose for the campus community. Learn to listen. If your students have a better way, do it. If they have an idea, let them explore it. Invest in success early. Make sure you have the infrastructure to guarantee uptime and have a development farm. Every situation is different but there is no harm in failure. "I have not failed. I've found 10,000 ways that won't work" – Thomas A. Edison In brief:  these points highlight the fact that Lou's primary role as director of the center is to create the conditions that make it possible for students to do great work.  The commercial-level quality of work turned out by students suggests that Lou's knowledge on how to build high-performing teams is definitely worth capturing.  The question is: what's the best way to do this (short of getting him to visit you and talk about his experiences)?  Seeing the forest for the trees  Paul recently interviewed Lou with the intent of documenting Lou's experiences. The conversation was recorded on video and then "Glymafied" it – i.e the video was mapped using IBIS (see Figure 4 below).  Figure 4: Knowledge capture via Glyma Figure 4: Knowledge capture via Glyma (Click to see larger picture)  There are a few points worth noting here:  The content of the entire conversation is mapped out so one can "see" the conversation at a glance. The context in which a particular point (i.e. the content of a node) is made is clarified by the connections between a node and its neighbours. Moving left from a node gives a higher level picture, moving right drills down into details. Of course, the reader will have noted that these are core IBIS capabilities that are available in Compendium (or any other IBIS tool).  Glyma offers much more. Consider the following:  Relevant documents or audio visual media can be tagged to specific nodes to provide supplementary material. In this case the video recording was tagged to specific nodes that related to points made in the video. Clicking on the play icon attached to such a node plays the segment in which the content of the node is being discussed. This is a really nice feature as it saves the user from having to watch the whole video (or play an extended game of ffwd-rew to get to the point of interest). Moreover, this provides additional context that cannot (or is not) captured by in the map. For example, one can attach papers, links to web pages, Slideshare presentations etc. to fill in background and context. Glyma is integrated with an enterprise content management system by design. One can therefore link map and video content to the powerful built-in search and content aggregation features of these systems. For example, users would be able enter a search from their intranet home page and retrieve not only traditional content such as documents, but also stories, reflections and anecdotes from experts such as Lou. Another critical aspect to intranet integration is the ability to provide maps as contextual navigation. Amazon's ability to sell books that people never intended to buy is an example of the power of such navigation. The ability to execute the kinds of queries outlined in the previous point, along with contextual information such as user profile details, previous activity on the intranet and the area of an intranet the user is browsing, makes it possible to present recommendations of nodes or maps that may be of potential interest to users. Such targeted recommendations might encourage users to explore video (and other rich media) content. Technical Aside: An interesting under-the-hood feature of Glyma is that it uses an implementation of a hypergraph database to store maps. (Note: this is a database that can store representations of graphs (maps) in which an edge can connect to more than 2 vertices). These databases enable the storing of very general graph structures. What this means is that Glyma can be extended to store any kind of map (Mind Maps, Concept Maps, Argument Maps or whatever)…and nodes can be shared across maps. This feature has not been developed as yet, but I mention it because it offers some exciting possibilities in the future.  To summarise: since Glyma stores all its data in an enterprise-class database, maps can be made available across an organization.  It offers the ability to tag nodes with pretty much any kind of media (documents, audio/video clips etc.), and one can tag specific parts of the media that are relevant to the content of the node (a snippet of a video, for example). Moreover, the sophisticated search capabilities of the platform enable context aware search.  Specifically, we can search for nodes by keywords or tags, and once a node of interest is located, we can also view the map(s) in which it appears. The map(s) inform us of the context(s) relating to the node. The ability to display the "contextual environment" of a piece of information is what makes Glyma really interesting.  In metaphorical terms, Glyma enables us to see the forest for the trees.  …and so, to conclude My aim in this post has been to introduce readers to the IBIS notation and trace its history from its origins in issue mapping to recent developments in knowledge management.  The history of a technique is valuable because it gives insight into the rationale behind its creation, which leads to a better understanding of the different ways in which it can be used. Indeed, it would not be an exaggeration to say that the knowledge management applications discussed above are but an extension of Rittel's original reasons for inventing IBIS.  I would like to close this piece with a couple of observations from my experience with IBIS:  Firstly, the real surprise for me has been that the technique can capture most written arguments and conversations, despite having only three distinct elements and a very simple grammar. Yes, it does require some thought to do this, particularly when mapping discussions in real time. However, this cognitive overhead is good because it forces the mapper to think about what's being said instead of just writing it down blind. Thoughtful transcription is the aim of the game. When done right, this results in a map that truly reflects an understanding of a complex issue.  Secondly, although most current discussions of IBIS focus on its applications in dialogue mapping, it has a more important role to play in mapping organizational knowledge. Maps offer a powerful means to navigate the complex network of knowledge within an organisation. The (aspirational) end-goal of such an effort would be a "global" knowledge map that highlights interconnections between different kinds of knowledge that exists within an organization. To be sure, such a map will make little sense to the human eye, but powerful search capabilities could make it navigable. To the extent that this is a feasible direction, I foresee IBIS becoming an important skill in the repertoire of knowledge management professionals.A central myth about decision making in organisations is that it is a rational process.  The qualifier rational refers to decision-making methods that are based on the following broad steps:  Identify available options. Develop criteria for rating options. Rate options according to criteria developed. Select the top-ranked option. The truth is that decision making in organisations generally does not follow such a process. As I have pointed out in this post (which is based on this article by Tim van Gelder) decisions are often based on a mix of informal reasoning, personal beliefs and even leaps of faith. . Quoting from that post, formal (or rational) processes often cannot be applied for one or  more of the following reasons:  Real-world options often cannot be quantified or rated in a meaningful way. Many of life's dilemmas fall into this category. For example, a decision to accept or decline a job offer is rarely made on the basis of material gain alone. Even where ratings are possible, they can be highly subjective. For example, when considering a job offer, one candidate may give more importance to financial matters whereas another might consider lifestyle-related matters (flexi-hours, commuting distance etc.) to be paramount. Another complication here is that there may not be enough information to settle the matter conclusively. As an example, investment decisions are often made on the basis of quantitative information that is based on questionable assumptions. Finally, the problem may be wicked – i.e. complex, multi-faceted and difficult to analyse using formal decision making methods. Classic examples of wicked problems are climate change (so much so, that some say it is not even a problem) and city / town planning. Such problems cannot be forced into formal decision analysis frameworks in any meaningful way. The main theme running through all of these is uncertainty. Most of the decisions we are called upon to make in our professional lives are fraught with uncertainty – it is what makes it hard to rate options, adds to the subjectivity of ratings (where they are possible) and magnifies the wickedness of the issue.  Decision making in projects and the need for systematic deliberation The most important decisions in a project are generally made at the start, at what is sometimes called the "front-end of projects". Unfortunately this is where information availability is at its lowest and, consequently, uncertainty at its highest.  In such situations, decision makers feel that they are left with little choice but to base their decisions on instinct or intuition.  Now, even when one bases a decision on intuition, there is some deliberation involved – one thinks things through and weighs up options in some qualitative way. Unfortunately, in most situations, this is done in an unsystematic manner. Moreover, decision makers fail to record the informal reasoning behind their decisions. As a result the rationale behind decisions made remain opaque to those who want to understand why particular choices were made.  A brief introduction to IBIS Clearly, what one needs is a means to make the informal reasoning behind a decision explicit. Now there are a number of argument visualisation techniques available for this purpose, but I will focus on one that I have worked with for a while: Issue-Based Information System (IBIS). I will introduce the notation briefly below. Those who want a detailed introduction will find one in my article entitled, The what and whence of issue-based information systems.  IBIS consists of three main elements:  Issues (or questions): these are issues that need to be addressed. Positions (or ideas): these are responses to questions. Typically the set of ideas that respond to an issue represents the spectrum of perspectives on the issue. Arguments: these can be Pros (arguments supporting) or Cons (arguments against) an issue. The complete set of arguments that respond to an idea represents the multiplicity of viewpoints on it. The best IBIS mapping tool is Compendium – it can be downloaded here.  In Compendium, the IBIS elements described above are represented as nodes as shown in Figure 1: issues are represented by green question nodes; positions by yellow light bulbs; pros by green + signs and cons by red – signs.  Compendium supports a few other node types, but these are not part of the core IBIS notation. Nodes can be linked only in ways specified by the IBIS grammar as I discuss next.  IBIS Elements  The IBIS grammar can be summarized in a few simple rules:  Issues can be raised anew or can arise from other issues, positions or arguments. In other words, any IBIS element can be questioned.  In Compendium notation:  a question node can connect to any other IBIS node. Ideas can only respond to questions – i.e.  in Compendium "light bulb" nodes  can only link to question nodes. The arrow pointing from the idea to the question depicts the "responds to" relationship. Arguments  can only be associated with ideas –  i.e in Compendium + and –  nodes can only link to "light bulb" nodes (with arrows pointing to the latter) The legal links are summarized in Figure 2 below.  Figure 2: Legal Links in IBIS  The rules are best illustrated by example-  follow the links below to see some illustrations of IBIS in action:  See or this post or this one for examples of IBIS in mapping dialogue. See this post or this one for examples of argument visualisation (issue mapping) using IBIS. The case studies In my talk, I illustrated the use of IBIS by going through a couple of examples in detail, both of which I have described in detail in other articles. Rather than reproduce them here, I will provide links to the original sources below.  The first example was drawn from a dialogue mapping exercise I did for a data warehousing project. A detailed discussion of the context and process of mapping (along with figures of the map as it developed) are available in a paper entitled, Mapping project dialogues using IBIS – a case study and some reflections (PDF).  The second example, in which I described a light-hearted example of the use of IBIS in a non-work setting,  is discussed in my post, What should I do now – a bedtime story about dialogue mapping.  Benefits of IBIS The case studies serve to highlight how IBIS encourages collective deliberation of issues. Since the issues we struggle with in projects often have elements of wickedness, eliciting opinions from a group through dialogue improves our chances arriving at a "solution" that is acceptable to the group as a whole.  Additional benefits of using  IBIS in a group setting include:  It adds clarity to a discussion Serves as a simple and intuitive discussion summary (compare to meeting minutes!) Is a common point of reference to move a discussion forward. It captures the logic of a decision (decision rationale) Further still, IBIS disarms disruptive discussion tactics such as "death by repetition" – when a person brings up the same issue over and over again in a million and one different ways. In such situations the mapper simply points to the already captured issue and asks the person if they want to add anything to it. The disruptive behaviour becomes evident to all participants (including the offender).  The beauty of IBIS lies in its simplicity. It is easy to learn – four nodes with a very simple grammar. Moreover, participants don't need to learn the notation. I have found that most people can understand what's going on within a few minutes with just a few simple pointers from the mapper.  Another nice feature of IBIS is that it is methodology-neutral. Whatever your methodological persuasion – be it Agile or something  that's  BOKsed – you can use it to address decision problems in your project meetings.  Getting started The best way to learn IBIS is to map out the logic of articles in newspapers, magazines or even professional journals. Once you are familiar with the syntax and grammar, you can graduate to one-on-one conversations, and from there to small meetings. When using it in a meeting for the first time, tell the participants that you are simply taking notes. If things start to work well – i.e. if you are mapping the conversation successfully – the group will start interacting with the map, using it as a basis for their reasoning and as a means to move the dialogue forward. Once you get to this point, you are where you want to be – you are mapping the logic of the conversation.  Of course, there is much more to it than I've mentioned above. Check out the references at the end of this piece for more information on mapping dialogues using IBIS.  Wrap up As this post is essentially covers a talk I gave at a conference, I would like to wrap up with a couple of observations and comments from the audience.  I began my talk with the line, "A central myth about decision making in organisations is that it is a rational process."  I thought many in the audience would disagree. To my surprise, however, there was almost unanimous agreement! The points I made about uncertainty and problem wickedness also seemed to resonate. There were some great examples from the audience on wicked problems in IT – a particularly memorable one about an infrastructure project (which one would normally not think of as particularly wicked) displaying elements of wickedness soon after it was started.  It seems that although mainstream management ignores the sense-making aspect of the profession, many practitioners tacitly understand that making sense out of ambiguous situations is an important part of their work.  Moreover, they know that this is best done by harnessing the collective intelligence of a group rather than by enforcing a process or a solutionIt was about half past eight in the evening a couple of weeks ago; I was sitting at my computer at home, writing up some notes for a blog post on issue mapping.  "What are you drawing?" asked my eight year old, Rohan. I hadn't noticed him. He had snuck up behind me quietly, and was watching me draw an IBIS map. (Note: see my post entitled, the what and whence of issue-based information systems for a quick introduction to IBIS)  "Go to bed," I said, still looking at the screen. It was past his bedtime.  "…but what are you drawing. What are those questions and arrows and stuff?"  A few minutes won't hurt, I thought. I turned to him and explained the basics of the notation and how it worked.  "But what good is it," he asked.  "Good question," I said. "It has many uses, but one of the most important ones is that it can help people make good decisions."  "Decisions about what?"  "Anything, I said, "for example: you may want to decide what you should do right now. Well, IBIS can help you make that decision."  "How?"  "I'll have to show you," I said, "and I can't because you have to go to bed now."  What a cop out, I thought to myself, as I said those words.  "Come on, dad – just a few minutes. I really want to know how it can help me make a decision about what I should do now."  "You should go to bed."  "How do I know that's a good decision? Let's see what IBIS says," said the boy.  Brilliant! It was checkmate. I relented.    "OK, " I said, opening a new map in Compendium and drawing a question node. "Every IBIS map begins with a question – we call it the root question. Our root question is: What should I do now?"  I typed in the root question and asked him: "So, tell me: what are the different things you could do now."  He thought for a bit and said, "I could go to sleep but that's boring."  "Good. There are actually two things you've said there – an idea (go to sleep) and an argument against it  (its boring). Let's put that down in the map. In an IBIS map, an idea is shown as a light-bulb (as in a comic) and an argument against it by a  minus sign."  The map with the root question along with Rohan's  first response and argument is shown in Figure 1.  Figure 1  He looked at the map and said, "There's another minus I can think of – it is hard to sleep so early."  I put that point in and said, "I'm sure you could also think of some plus points for sleeping early."  "Yes," he said, "I can get up early and do stuff."  "What stuff?"  "I can play Wii before I go to school."  "OK let's put all that into the map," I said.  "See, an argument supporting an idea is shown as a plus sign. Then, I asked you to explain a bit more about why getting up early is a good thing. Your answer goes into  the map as another  idea. Notice, also, that the map develops from right to left, starting from the root question."  The map at this point of the discussion is shown in Figure 2.  Figure 2  "What else can you do now?"  "I can talk to you," said Rohan.  "And what are the plus points of that?" I asked.  "It is interesting to talk to you." Ah, the boy has the makings of a diplomat…  "The minus points?"  "You are tired and crabby"  OK, may be he isn't a diplomat…  The map at this point is shown in Figure 3.  Figure 3  "What else can you do," I asked, as I cleaned up the map a bit.  "I could spend some time with Vikram." (Vikram is Rohan's 4 month old brother).  "What are the plus points of doing that?"  "He does funny things and he's cute."  "That's two points, " I said, adding them to the map. Then I asked, "What kinds of funny things?"  "He gurgles, smiles and blows spit bubbles."  "Great," I said, adding those points as elaborations of "does funny things".  Rohan said, "I forgot. Vik is asleep so I can't play with him."  "OK, so that's a minus point that rules out the choice," I said, adding it as an  argument against the idea. The map at this point is shown in Figure 4.  Figure 4  "Can you think of anything else you can do?" I asked.  He thought for a while and replied, "I could read."  "OK," I said. "What are the plus and minus points of that."  "It's interesting," he said, and then in the same breath added, "but I have nothing to new to read."  I put these points in as arguments for and against reading. The map at this point is shown in Figure 5.  Figure 5  After finishing I asked, "Anything else you want to add."  "I could just stay up and watch a movie,"  he said.  I stopped myself from vetoing that outright. Instead, I put the point in and asked," Why do you want to stay up and watch a movie?"  "It's fun," he said.  "May be so, but a movie would take too long and you have school tomorrow."  "School's boring."  "I'll note your point," I said, "but I'm afraid I have to veto that option."  "I was just trying it out, dad."  "I know," I said,  as I updated the  map (see Figure 6).  Figure 6  "Can you think of anything else you could do?"  "No."  "OK, let's look at where we are. Have a look at the map and tell me what you think."  Rohan looked at the map for a bit and said, "It shows me all my choices and gives me reasons to choose or not to choose them."  "Does that help you decide what you should do now?"  "Sort of," he said, "I know I can't spend time with Vik because he's asleep. I can't talk to you because you're tired and might get crabby. I can't stay up and watch a movie because you won't let me."  "So what can you do?"  "I can read or go to sleep"  "But you have nothing new to read.," I pointed out.  "Yes, but I think I could find something that I would like to read again…Yes, I know what I will do –  I'll read  for a while and then go to sleep."  "Sounds like a good idea – that way you get to do two of the things on the list." I said.  "This IBIS stuff is cool. I think I'll talk about it at my news this Thursday. It is free choice."  (News is a 2-3 minute presentation that all kids in class get to do once a week. Most often the topic is assigned beforehand, but there's one free-choice session per term where the kids can talk about anything they want to)  "Great idea," I said, "I'll help you make some notes and map images tomorrow. Now you'd really better go off to bed before your mum comes in and gets upset at us both."  "Good night, dad"  "'night, Rohan"Jack could see that the discussion was going nowhere:  the group been talking about the pros and cons of competing approaches for over half an hour, but they kept going around in circles.  As he  mulled over this, Jack  got an  idea – he'd been playing around with a visual notation called IBIS (Issue-Based Information System) for a while, and was looking for an opportunity to use it to map out a discussion in real time (Editor's note:  readers unfamiliar with IBIS may want to have a look at this post and this one before proceeding).  "Why not give it a try," he thought, "I can't do any worse than this lot."  Decision made,  he waited for a break in the conversation and dived in when he got one…  "I have a suggestion," he said. "There's this conversation mapping tool that I've been exploring for a while. I believe it might help us reach a common understanding of the approaches we've been discussing. It may even help facilitate a decision. Do you mind if I try it?"  "Pfft  – I'm all for it if it helps us get to a decision." said Max. He'd clearly had enough too.  Jack looked around the table. Mary looked puzzled,  but nodded her assent. Rick seemed unenthusiastic, but didn't voice any objections. Andrew – the boss –  had a here-he-goes-again look on his face (Jack had a track record of  "ideas")  but, to Jack's surprise, said, "OK. Why not? Go for it."  "Give me a minute to get set up," said Jack. He hooked his computer to the data projector. Within a couple of minutes, he had a blank IBIS map displayed on-screen.  This done, he glanced up at the others: they were looking at screen with expressions ranging from curiosity (Mary) to skepticism (Max).  "Just a few words about what I'm going to do, he said. "I'll be using a notation called IBIS – or issue based information system – to capture our discussion. IBIS has three elements: issues, ideas and arguments.  I'll explain these as we go along. OK – Let's get started with figuring out what we want out of the discussion. What's our aim?" he asked.  His starting spiel done, Jack glanced at his colleagues: Max seemed a tad more skeptical than before; Rick ever more bored; Andrew and Mary stared at the screen. No one said anything.  Just as he was about to prompt them by asking another question, Mary said, "I'd say it's to explore options for implementing the new system and find the most suitable one. Phrased as a question: How do we implement system X?"  Jack glanced at the others. They all seemed to agree – or at least, didn't disagree – with Mary, "Excellent," he said, "I think that's a very good summary of what we're trying to do." He drew a question node on the map and continued: "Most discussions of this kind are aimed at resolving issues or questions. Our root question is: What are our options for implementing system X, or as Mary put it, How do we implement System X."  Figure 1 Figure 1  "So, what's next," asked Max. He still looked skeptical, but Jack could see that he was intrigued. Not bad, he thought to himself….  "Well, the next step is to explore ideas that address or resolve the issue.  So, ideas should be responses to the question:  how should we implement system X? Any suggestions?"  "We'd have to engage consultants," said Max. "We don't have in-house skills to implement it ourselves."  Jack created an idea node on the map and began typing. "OK – so we hire consultants," he said. He looked up at the others and continued, "In IBIS, ideas are depicted by light bulbs. Since ideas  respond  to questions, I draw an arrow from the idea to the root question, like so:  Figure 2 Figure 2  "I think doing it ourselves is an option," said Mary, "We'd need training and it might take us longer because we'd be learning on the job, but it is a viable option. "  "Good," said Jack, "You've given us another option and some ways in which we might go about implementing the option. Ideally, each node should represent a single – or atomic – point. So I'll capture what you've said like so." He typed as fast he could, drawing nodes and filling in detail.  As he wrote he said,  "Mary said we could do it ourselves – that's clearly a new idea – an implementation option. She also partly described how we might go about it: through training to learn the technology and by learning on the job. I've added in "How" as a question and the two points  that describe how we'd do it as ideas responding to the question." He paused and looked around to check that every one was with him, then continued. "But there's more: she also mentioned a shortcoming of doing it ourselves – it will take longer. I capture this as an argument against the idea; a con, depicted as red minus in the map."  He paused briefly to look at his handiwork on-screen, then asked, "Any questions?"  Figure 3 Figure 3  Rick asked, "Are there any rules governing how nodes are connected?"  "Good question!  In a nutshell: ideas respond to questions and arguments respond to ideas. Issues, on the other hand, can be generated from any type of node.  I can give you some links to references on the Web if you're interested."  "That might be useful for everyone," said Andrew. "Send it out to all of us."  "Sure. Let's move on. So, does anyone have any other options?"  "Umm..not sure how it would work, but what about co-development?" Suggested Rick.  "Do you mean collaborative development with external resources?" asked Jack as he began typing.  "Yes," confirmed Rick.  Figure 4 Figure 4  "What about costs? We have a limited budget for this," said Mary.  "Good point," said Jack as he started typing.  "This is a constraint that must be satisfied by all potential approaches."   He stopped typing and  looked up at the others, "This is important: criteria apply to all potential approaches, so the criterial question should hang off the root node," he said.  "Does this make sense to everyone?"  Figure 5 Figure 5  "I'm not sure I understand," said Andrew. "Why are the criteria separate from the approaches?"  "They aren't separate. They're a level higher than any specific approach because they apply to all solutions. Put another way, they relate to the root issue – How do we implement system X – rather than a specific solution."  "Ah, that makes sense," said Andrew. "This notation seems pretty powerful."  "It is, and I'll be happy to show you some more features later, but let's continue the discussion for now. Are there any other criteria?"  "Well, we must have all the priority 1 features described in the scoping document implemented by the end of the year,"  said Andrew. One can always count on the manager to emphasise constraints.  "OK, that's two criteria actually: must implement priority 1 features and must implement by year end," said Jack, as he added in the new nodes. "No surprises here," he continued, "we have the three classic project constraints – budget, scope and time."  Figure 6 Figure 6  The others were now engaged with the map, looking at it, making notes etc. Jack wanted to avoid driving the discussion, so instead of suggesting how to move things forward, he asked, "What should we consider next?"  "I can't think of any other approaches," said Mary. Does anyone have suggestions, or should we look at the pros and cons of the listed approaches?"  "I've said it before; I'll say it again: I think doing it ourselves is a dum..,.sorry,  not a good idea. It is fraught with too much risk…" started Max.  "No it isn't," countered Mary, "on the contrary, hiring externals is more risky because costs can blowout by  much more than if we did it ourselves."  "Good points," said Jack, as he noted Mary's point.  "Max, do you have any specific risks in mind?"  Figure 7 Figure 7  "Time – it can take much longer," said Max.  "We've already captured that as a con of the do-it-ourselves approach," said Jack.  "Hmm…that's true, but I would reword it to state that we have a hard deadline. Perhaps we could say – may not finish in allotted time – or something similar."  "That's a very good point," said Jack, as he changed the node to read: higher risk of not meeting deadline. The map was coming along nicely now, and had the full attention of folks in the room.  Figure 8 Figure 8  "Alright," he continued, "so are there any other cons? If not, what about pros – arguments in support of approaches?"  "That's easy, " said  Mary,  "doing it ourselves will improve our knowledge of the technology; we'll be able to support and maintain the system ourselves."  "Doing it through consultants will enable us to complete the project quicker," countered Max.  Jack added in the pros and paused, giving the group some time to reflect on the map.  Figure 9 Figure 9  Rick and Mary, who were sitting next to each other, had a whispered side-conversation going; Andrew and Max were writing something down. Jack waited.  "Mary and I have an idea," said Rick. "We could take an approach that combines the best of both worlds – external consultants and internal resources. Actually, we've already got it down as a separate approach –  co-development, but we haven't discussed it yet.." He had the group's attention now. "Co-development allows us to use consultants' expertise where we really need it and develop our own skills too. Yes, we'd need to put some thought into how it would work, but I think we could do it."  "I can't see how co-development will reduce the time risk – it will take longer than doing it through consultants." said Max.  "True," said Mary, "but it is better than doing it ourselves and, more important, it enables us to develop in-house skills that are needed to support and maintain the application. In the long run, this can add up to a huge saving. Just last week I read that maintenance can be anywhere between 60 to 80 percent of total system costs."  "So you're saying that it reduces implementation time and  results in a smaller exposure to cost blowout?" asked Jack.  "Yes," said Mary  Jack added in the pros and waited.  Figure 10 Figure 10  "I still don't see how it reduces time," said Max.  "It does, when compared to the option of doing it ourselves," retorted Mary.  "Wait a second guys," said Jack. "What if I reword the pros to read – Reduced implementation time compared to in-house option and Reduced cost compared to external option."  He looked at Mary and Max. – both seemed to OK with this, so he typed in the changes.  Figure 11 Figure 11  Jack asked, "So, are there any other issues, ideas or arguments that anyone would like to add?"  "From what's on the map, it seems that co-development is the best option," said Andrew.  He looked around to see what the others thought: Rick and Mary were nodding; Max still looked doubtful.  Max asked, "how are we going to figure out who does what?  It isn't easy to partition work cleanly when teams have different levels of expertise."  Jack typed this in as a con.  Figure 12 Figure 12  "Good point," said Andrew. "There may be ways to address this concern. Do you think it would help if we brought some consultants in on a day-long engagement to workshop a co-development approach with the team? "  Max nodded, "Yeah, that might work," he said. "It's worth a try anyway. I have my reservations,  but co-development seems the best of the three approaches."  "Great," said Andrew, "I'll get some consultants in next week to help us workshop an approach."  Jack typed in this exchange, as the others started to gather their things. "Anything else to add?"  he asked.  Everyone looked up at the map. "No,  that's it, I think," said Mary.  "Looks good,"  said Mike . "Be sure to send us a copy of the map."  Figure 13 Figure 13  "Sure, I'll print copies out right away," said Jack. "Since we've developed it together, there shouldn't be any points of disagreement."  "That's true,"  said Andrew, "another good reason to use this tool."  Gathering his papers, he asked, "Is there anything else? He looked around the table. " Alright then, I'll see you guys later,  I'm off to get some lunch before my next meeting."  Jack looked around the group.  Helped along by IBIS and  his facilitation, they'd overcome their differences and reached a collective decision. He had thought it wouldn't work, but it had.  " Jack, thanks  for your help with the discussion. IBIS seems to be a great way to capture discussions.  Don't forget to send us those references," said Mary, gathering her notes.  "I'll do that right away," said Jack, "and I'll also send you some information about Compendium – the open-source software tool I used to create the map."  "Great," said Mary. "I've got to run. See you later."  "See you later," replied Jack.Here's a question that's been on my mind for a while: Why do organisations find it difficult to capture and transmit knowledge created in projects?  I'll attempt an answer of sorts in this post and point to a solution of sorts too – taking a bit of a detour along the way, as I'm often guilty of doing…  Let us first consider how knowledge is generated on projects. In project environments, knowledge is typically created in response to a challenge faced. The challenge may be the project objective itself, in which case the knowledge created is embedded in the product or service design. On the other hand it could be a small part of the project:  say, a limitation of the technology used, in which case the knowledge might correspond to a workaround discovered by a project team member.  In either case, the existence of an issue or challenge is a precondition to the creation of knowledge. The project team (or a subset of it) decides on how the issue should be tackled. They do so incrementally; through exploration, trial and error and discussion – creating knowledge in the bargain.  In a nutshell: knowledge is created as project members go through a process of discovery.  This process may involve the entire team (as in the first example) or a very small subset thereof (as in the second example, where the programmer may be working alone). In both cases, though, the aim is to  understand the problem, explore potential solutions and find the "best" one  thus creating new knowledge.  Now, from the perspective of project and organisational learning,  project documentation must capture the solution and the process by which it was developed. Typically, most documentation tends to focus on the former, neglecting the latter. This is akin to presenting a solution to a high school mathematics problem without showing the intervening steps. My high school maths teacher would've awarded me a zero for such an effort regardless of the whether or not my answer was right. And with good reason too: the steps leading up to a solution are more important than the solution itself. Why? Because the steps illustrate the thought processes and reasoning behind the solution. So it is with project documentation. Ideally, documentation should provide the background, constraints, viewpoints, arguments and counter-arguments that go into the creation of project knowledge.  Unfortunately, it seldom does.  Why is this so? An answer can be found in a farsighted paper, entitled Designing Organizational Memory: Preserving Intellectual Assets in a Knowledge Economy, published by Jeff Conklin 1997. In the paper Conklin draws a distinction between formal and informal knowledge;  terms that correspond to the end-result and the process discussed in the previous paragraph.To quote from his paper, formal knowledge is the "…stuff of books, manuals, documents and training courses…the primary work product of the knowledge worker…" Conklin notes that, "Organisations routinely capture formal knowledge; indeed, they rely on it – without much success – as their organizational memory." On the other hand, informal knowledge is, "the knowledge that is created and used in the process of creating formal results…It includes ideas, facts, assumptions, meanings, questions, decisions, guesses, stories and points of view. It is as important in the work of the knowledge worker as formal knowledge is, but it is more ephemeral and transitory…" As a consequence, informal knowledge is elusive; it rarely makes it into documents.  Dr. Conklin lists two reasons why organisations (which includes temporary organisations such as projects) fail to capture informal knowledge. These are:  Business culture values results over process. In his words, "One reason for the widespread failure to capture informal knowledge is that Western culture has come to value results – the output of work process – far above the process itself, and to emphasise things over relationships." The tools of knowledge workers do not support the process of knowledge work – that is, most tools focus on capturing formal knowledge (the end result of knowledge work) rather than informal knowledge (how that end result was achieved – the "steps" in the solution, so to speak). The key to creating useful documentation thus lies in capturing informal knowledge. In order to do that, this elusive form of knowledge must first be made explicit – i.e. expressible in words and pictures.  Here's where the notion of shared understanding, discussed in my previous post comes into play.  To quote again from Conklin's paper, "One element of creating shared understanding is making informal knowledge explicit. This means surfacing key ideas, facts, assumptions, meanings, questions, decisions, guesses, stories, and points of view. It means capturing and organizing this informal knowledge so that everyone has access to it. It means changing the process of knowledge work so that the focus is on creating and managing a shared display of the group's informal thinking and learning. The shared display is the transparent vehicle for making informal knowledge explicit."  The notion of a shared display is central to the technique of dialogue mapping, a group facilitation technique that can help a diverse group achieve a shared understanding of  a problem.  Dialogue mapping uses a visual notation called IBIS (short for Issue Based Information System) to capture the issues, ideas and arguments that come up in a meeting (see my previous post for a quick introduction to dialogue mapping and a crash-course in IBIS).  As far as knowledge capture is concerned, I see two distinct uses of IBIS . They are:  As Conklin suggests,  IBIS maps can be used to surface and capture informal knowledge in real-time, as it is being generated in a discussion between two or more people. It can also be used in situations where one person researches and finds a solution to a challenge. In this case the person could use IBIS to capture background, approaches tried, the pros and cons of each – i.e. the process by which the solution was arrived at. This, together with the formal stuff – which most project organisations capture more than adequately – should result in documents that provide a more complete view of the knowledge generated in projects.  Now, in a previous post (linked above) I discussed an example of how dialogue mapping was used on a real-life project challenge:  how best to implement near real-time updates to a finance data mart. The final issue map, which was constructed using a mapping tool called Compendium, is reproduced below:  Final Map Example Issue Map  The map captures not only the decisions made, but also the options discussed and the arguments for and against each of the options. As such, it captures the decision along with the context, process and rationale behind it. In other words, it captures formal and informal knowledge pertaining to the decision. In this simple case, IBIS succeeded in making informal knowledge explicit. Project teams down the line can understand why the decision was made,  from both a technical and business perspective.  It is worth noting that Compendium maps can reference external documents via Reference Nodes, which, though less important for argumentation, are extremely useful for documentation. Here is an illustration of how references to external documents can be inserted in an IBIS map through reference node:  Links to external docs Links to external docs  As illustrated, this feature can be used to link out to a range of external documents – clicking on the reference node in a map (not the image above!) opens up the external document. Exploiting this, a project document can operate at two levels: the first being an IBIS map that depicts the relationships between issues, ideas and arguments, and the second being supporting documents that provide details on specific nodes or relationships. Much of the informal knowledge pertaining to the issue resides in the first level – i.e. in the relationships between nodes – and this knowledge is made explicit in the map.  To conclude, it is perhaps worth summarising the main points of this post. Projects generate new knowledge through a process in which issues and ideas are proposed, explored and trialled. Typically, most project documentation captures the outcome (formal knowledge), neglecting much of the context, background and process of discovery (informal knowledge).  IBIS maps offer the possibility of capturing both aspects of knowledge, resulting in a greatly improved project (and hence organisational)  memory.I've written a number of posts on complexity in projects, covering topics ranging from conceptual issues to models of project complexity. Despite all that verbiage, I've never addressed the key issue of how complexity should be handled. Methodologists claim, with some justification, that complexity can be tamed by adequate planning together with appropriate controlling and monitoring as the project progresses. Yet, personal experience – and the accounts of many others – suggests that the beast remains untamed. A few weeks ago, I read this brilliant series of articles by Paul Culmsee, where  he discusses a technique called Dialogue Mapping which, among other things, may prove to be a dragon-slayer. In this post I present an overview of the technique and illustrate its utility in real-life project situations.  First a brief history: dialogue mapping has its roots in wicked problems – problems that are hard to solve, or even define, because they satisfy one or more of these criteria. (Note that there is a relationship between problem wickedness and project complexity: projects that set out to address or solve wicked problems are generally complex but the converse is not necessarily true – see this post for more). Over three decades ago, Horst Rittel – the man who coined the term "wicked problem" – and his colleague Werner Kunz developed a technique called Issue Based Information System (IBIS) to aid in the understanding of such problems. IBIS is based on the premise that wicked problems – or any contentious issues – can be understood by discussing them in terms of three essential elements: issues (or questions), ideas (or answers) and arguments (for or against ideas). IBIS was subsequently refined over the years by various research groups and independent investigators. Jeff Conklin, the inventor of dialogue mapping, was one of the main contributors to this effort.  Now, the beauty of IBIS is that it is very easy to learn. Basically it has only the three elements mentioned earlier – issues, ideas and arguments – and these can be connected only in ways specified by the IBIS grammar. The elements and syntax of the language can be illustrated in half a page – as I shall do in a minute. Before doing so, I should mention that there is an excellent, free software tool – Compendium – that supports the IBIS notation. I use it in the discussion and demo below. I recommend that you download and install Compendium before proceeding any further.  Go on, I'll wait for you…  OK, let's begin. IBIS has three elements which are illustrated in the Compendium map below:  IBIS Elements Figure 1: IBIS Elements  The elements are:  Question: an issue that's being discussed or analysed. Note that the term "question" is synonymous with "issue"  Idea: a response to a question. An idea responds to a question in the sense that it offers a potential resolution or clarification of the question.  Argument: an argument in favour of or against an idea (a pro or a con)  The arrows show links or relationships between elements.  That's it as far as elements of IBIS are concerned.  The IBIS grammar specifies the legal ways in which elements can be linked. The rules are nicely summarised in the following diagram:  Figure 2: Legal Links in IBIS Figure 2: Legal Links in IBIS  In a nutshell, the rules are:  Any element (question, idea or agument) can be questioned. Ideas respond to questions. Arguments make the case for and against ideas. Note that questions cannot be argued! Simple, isn't it? Essentially that's all there is to IBIS.  So what's IBIS got to do with dialogue mapping? Well, dialogue mapping is facilitation of a group discussion using a shared display – a display that everyone participating in the discussion can see and add to. Typically the facilitator drives – i.e. modifies the display – seeking input from all participants, using the IBIS notation to capture the issues, ideas and arguments that come up. . This synchronous, or real-time, application of IBIS is described in Conklin's book, Dialogue Mapping: Building Shared Understanding of Wicked Problems (An absolute must-read if you manage on complex projects with diverse stakeholders). For completeness, it is worth pointing out that IBIS can also be used asynchronously – for example, in dissecting arguments presented in papers and articles. This application of IBIS – which is essentially dialogue mapping minus facilitation – is sometimes called issue mapping.  I now describe a simple but realistic application of dialogue mapping, adapted from a real-life case. For brevity, I won't reproduce the entire dialogue. Instead I'll describe how a dialogue map develops as a discussion progresses. The example is a simple illustration of how IBIS can be used to facilitate a shared understanding (and solution) of a problem.  All set? OK, let's go…  The situation: Our finance data mart is updated overnight through a batch job that takes a few hours. This is good enough for most purposes. However, a small (but very vocal!) number of users need to be able to report on transactions that have occurred within the last hour or so – waiting until the next day, especially during month-end, is simply not an option. The dev team had to figure out the best way to do this.  The location: my office.  The players: two colleagues from IT, one from finance, myself.  The shared display: Compendium running on my computer, visible to all the players.  The discussion was launched with the issue stated up-front: How should we update our data mart during business hours? My colleagues in the dev team came up with several ideas to address the issue. After capturing the issue and responding ideas, the map looked like this:  Figure 3: Map - stage 1 Figure 3: Map - stage 1  In brief, the options were to:  Use our messaging infrastructure to carry out the update. Write database triggers on transaction tables. These triggers would update the data mart tables directly or indirectly. Write custom T-SQL procedures (or an SSIS package) to carry out the update (the database is SQL Server 2005). Run the relevant (already existing) Extract, Transform, Load (ETL) procedures at more frequent intervals – possibly several times during the day. As the discussion progressed, the developers raised arguments for and against the ideas. A little later the map looked like this:  Figure 4: Map - stage 2 Figure 4: Map - stage 2  Note that data latency refers to the time interval between when the transaction occurs and its propagation to the data mart, and complexity is a rough measure of the effort required (in time) to implement that option. I won't go through the arguments in detail, as they are largely self-explanatory.  The business rep then asked how often the ETL could be run.  "The relevant portions can be run hourly, if you really want to," replied our ETL queen.  "That's good enough," said the business rep.  …voilà, we had a solution!  The final map looked much like the previous one: the only additions were the business rep's question, the developer's response and a node marking the decision made:  Figure 5: Final Map Figure 5: Final Map  Note that a decision node is simply an idea that is accepted (by all parties) as a decision on the issue being discussed.  Admittedly, my little example is nowhere near a complex project. However, before dismissing it outright, consider the following benefits afforded by the process of dialogue mapping:  Everyone's point of view was taken into account. The shared display served as a focal point of the discussion: the entire group contributed to the development of the map. Further,  all points and arguments made were represented in the map. The display and discussion around it ensured a common (or shared) understanding of the problem. Once a shared understanding was achieved – between the business and IT in this case – the solution was almost obvious. The finished map serves as an intuitive summary of the discussion – any participant can go back to it and recall the essential structure of the discussion in a way that's almost impossible through a written document. If you think that's a tall claim, here's a challenge: try reconstructing a meeting from the written minutes. Enough said, I think.  But perhaps my simple example leaves you unconvinced.  If so, I  urge you to read Jeff Conklin's reflections on an "industrial strength" case-study of dialogue mapping. Despite my limited practical experience with the technique, I believe it is an useful way to address issues that arise on complex projects, particularly those involving stakeholders with diverse points of view.  That's not to say that it is a panacea for project complexity – but then,  nothing is.  From a purely pragmatic perspective, it may be viewed as an addition to a project manager's tool-chest of communication techniques.  For, as I've noted elsewhere, "A shared world-view – which includes a common understanding of tools, terminology, culture, politics etc. – is what enables effective communication within a (project) group." Dialogue mapping provides a practical means to achieve such a shared understanding.The aim of an opinion piece writer is to convince his or her readers that a particular idea or point of view is reasonable or right.  Typically, such pieces  weave facts , interpretations and reasoning into prose, wherefrom it can be hard to pick out the essential thread of argumentation.  In an earlier post I showed how an issue map can help in clarifying the central arguments in a "difficult" piece of writing by mapping out Fred Brooks' classic article No Silver Bullet.  Note that I use the word "difficult" only because the article has, at times, been misunderstood and misquoted; not because it is particularly hard to follow.  Still, Brooks' article borders on the academic; the arguments presented therein are of interest to a relatively small group of people within the software development community. Most developers and architects aren't terribly interested in the essential difficulties of the profession – they just want to get on with their jobs. In the present post, I develop an issue map of a piece that is of potentially wider interest to the IT community – Nicholas Carr's 2003 article, IT Doesn't Matter.  The main point of Carr's article is that IT is becoming a utility,  much like electricity, water or rail. As this trend towards commoditisation gains momentum, the strategic advantage offered by in-house IT will diminish, and organisations will be better served by buying IT services from "computing utility" providers than by maintaining their own IT shops.  Although Carr makes a persuasive case, he glosses over a key difference between IT and other utilities (see this post for more). Despite this, many business and IT leaders have taken his words as the way things will be. It is therefore important for all IT professionals to understand Carr's arguments. The consequences are likely to affect them some time soon, if they haven't already.  Some preliminaries before proceeding with the map. First, the complete article is available here – you may want to have a read of it before proceeding (but this isn't essential). Second, the discussion assumes a basic knowledge of  IBIS (Issue-Based Information System) –  see  this post for a quick tutorial on IBIS.  Third, the map is constructed using the open-source tool Compendium which can be downloaded here.  With the preliminaries out of the way, let's get on with issue mapping Carr's article.  So, what's the root  (i.e. central) question that Carr poses in the article?  The title of the piece is  "IT Doesn't Matter" – so one possible root question is, "Why doesn't IT matter?" But there are other candidates:  "On what basis is IT an infrastructural technology?" or  "Why is the strategic value of IT diminishing?" for example. From this it should be clear that there's a fair degree of subjectivity at every step of constructing an issue map. The visual representation that I construct here is but one interpretation of Carr's argument.  Out of the above (and many other possibles),  I choose  "Why doesn't IT matter?" as the root question. Why? Well,  in my view the whole  point of the piece  is to convince the reader that IT doesn't matter because it is an infrastructural technology and consequently has no strategic significance. This point should become clearer as our development of the issue map progresses.  The ideas that respond to this question aren't immediately obvious. This isn't unusual:  as I've mentioned elsewhere, points can only be made sequentially – one after the other – when expressed in prose.  In some cases one may have to read a piece in its entirety to figure out the elements that respond to a root (or any other) question.  In the case at hand, the response to the root question stands out clearly after a quick browse through the article. It is:  IT is an infrastructural technology.  The map with the root question and the response is shown in Figure 1.  Figure 1: Issue Map Stage 1 Figure 1: Issue Map Stage 1  Moving on, what arguments does Carr offer for (pros) and against (cons) this idea? A reading of the article reveals one con and four pros. Let's look at the cons first:  IT (which I take to mean software) is complex and malleable, unlike other infrastructural technologies. This point is mentioned, in passing, on the third page of the paper: "Although more complex and malleable than its predecessors, IT has all the hallmarks of an infrastructural technology…" The arguments supporting the idea that IT is an infrastructural technology are:  The evolution of IT closely mirrors that of other infrastructural technologies such as electricity and rail. Although this point encompasses the other points made below, I think it merits a separate mention because the analogies are quite striking. Carr makes a very persuasive, well-researched case supporting this point. IT is highly replicable. This is point needs no further elaboration, I think. IT is a transport mechanism for digital information. This is true, at least as far as network and messaging infrastructure is concerned. Cost effectiveness increases as IT services are shared. This is true too, providing it is understood that flexibility is lost when services are shared. The map, incorporating the pros and cons is shown in Figure 2.  Figure 2: Issue Map Stage 2 Figure 2: Issue Map Stage 2  Now that the arguments for and against the notion that IT is an infrastructural technology are laid out, lets look at the article again, this time with an eye out for any other issues  (questions)  raised.  The first question is an obvious one: What are the consequences of IT being an infrastructural technology?  Another point to be considered is the role of proprietary technologies, which – by definition – aren't infrastructural. The same holds true for  custom built applications. So, this begs the question, if IT is an infrastructural technology, how do proprietary and custom built applications fit in?  The map, with these questions  added in is shown in Figure 3.  Figure 3: Issue Map Stage 3 Figure 3: Issue Map Stage 3  Let's now look at the ideas that respond to these two questions.  A point that Carr makes early in the article is that the strategic value of IT is diminishing. This is essentially a consequence of the notion that IT is an infrastructural technology. This idea is supported by the following arguments:  IT is ubiquitous – it is everywhere, at least in the business world. Everyone uses it in the same way. This implies that no one gets a strategic advantage from using it. What about proprietary technologies and custom apps?.  Carr reckons these are:  Doomed to economic obsolescence. This idea is supported by the argument that these apps are too expensive and are hard to maintain. Related to the above, these will be replaced by generic apps that incorporate best practices. This trend is already evident in the increasing number of enterprise type applications that offered as services. The advantages of these are that they a) cost little b) can be offered over the web and c) spare the client all those painful maintenance headaches. The map incorporating these ideas and their supporting arguments is shown in Figure 4.  Figure 4: Issue Map Stage 4 Figure 4: Issue Map Stage 4  Finally, after painting this somewhat gloomy picture (to a corporate IT minion, such as me) Carr asks and answers the question: How should organisations deal with the changing role of IT (from strategic to operational)? His answers are:  Reduce IT spend. Buy only proven technology – follow don't lead. Focus on (operational) vulnerabilities rather than (strategic) opportunities. The map incorporating this question and the ideas that respond to it is shown in Figure 5, which is also the final map (click on the graphic to view  a full-sized image).  Figure 5: Final Issue Map Figure 5: Issue Map Stage 5  Map completed, I'm essentially done with this post. Before closing, however, I'd like to mention a couple of general points that arise from issue mapping of prose pieces.  Figure 5 is my interpretation of the article. I should emphasise that my interpretation may not coincide with what Carr intended to convey (in fact, it probably doesn't). This highlights an important, if obvious, point: what a writer intends to convey in his or her writing may not coincide with how readers interpret it. Even worse, different readers may interpret a piece differently. Writers need to write with an awareness of the potential for being misunderstood.  So, my  first point is that issue maps can help writers clarify and improve the quality of their reasoning  before they cast it in prose.  Issue maps sketch out the logical skeleton or framework of argumentative prose. As such, they  can help highlight weak points of arguments. For example, in the above article Carr glosses over the complexity and malleability of software. This is a weak point of the argument, because it is a key difference between IT and traditional infrastructural technologies. Thus my second point is that issue maps can help readers visualise weak links in arguments which might have been obscured by rhetoric and persuasive writing.  To conclude,  issue maps are valuable to writers and readers:  writers can use  issue maps to  improve the quality of their  arguments before committing them in writing, and  readers can use such maps to understand arguments that have been thus committed.The term machine learning gets a lot of airtime in the popular and trade press these days. As I started writing this article, I did a quick search for recent news headlines that contained this term. Here are the top three results with datelines within three days of the search:  http://venturebeat.com/2017/02/01/beyond-the-gimmick-implementing-effective-machine-learning-vb-live/  http://www.infoworld.com/article/3164249/artificial-intelligence/new-big-data-tools-for-machine-learning-spring-from-home-of-spark-and-mesos.html  http://www.infoworld.com/article/3163525/analytics/review-the-best-frameworks-for-machine-learning-and-deep-learning.html  The truth about hype usually tends to be quite prosaic and so it is in this case. Machine learning, as Professor Yaser Abu-Mostafa  puts it, is simply about "learning from data."  And although the professor is referring to computers, this is so for humans too – we learn through patterns discerned from sensory data. As he states in the first few lines of his wonderful (but mathematically demanding!) book entitled, Learning From Data:  If you show a picture to a three-year-old and ask if there's a tree in it, you will likely get a correct answer. If you ask a thirty year old what the definition of a tree is, you will likely get an inconclusive answer. We didn't learn what a tree is by studying a [model] of what trees [are]. We learned by looking at trees. In other words, we learned from data.  In other words, the three year old forms a model of what constitutes a tree through a process of discerning a common pattern between all objects that grown-ups around her label "trees." (the data). She can then "predict" that something is (or is not) a tree by applying this model to new instances presented to her.  This is exactly what happens in machine learning: the computer (or more correctly, the algorithm) builds a predictive model of a variable (like "treeness") based on patterns it discerns in data.  The model can then be applied to predict the value of the variable (e.g. is it a tree  or not) in new instances.  With that said for an introduction, it is worth contrasting this machine-driven process of model building with the traditional approach of building mathematical models to predict phenomena as in, say,  physics and engineering.  What are models good for? Physicists and engineers model phenomena using physical laws and mathematics. The aim of such modelling is both to understand and predict natural phenomena.  For example, a physical law such as Newton's Law of Gravitation is itself a model – it helps us understand how gravity works and make predictions about (say) where Mars is going to be six months from now.  Indeed, all theories and laws of physics are but models that have wide applicability.  (Aside: Models are typically expressed via differential equations.  Most differential equations are hard to solve analytically (or exactly), so scientists use computers to solve them numerically.  It is important to note that in this case computers are used as calculation tools, they play no role in model-building.)  As mentioned earlier, the role of models in the sciences is twofold – understanding and prediction. In contrast, in machine learning the focus is usually on prediction rather than understanding.  The predictive successes of machine learning have led certain commentators to claim that scientific theory building is obsolete and science can advance by crunching data alone.  Such claims are overblown, not to mention, hubristic, for although a data scientist may be able to predict with accuracy, he or she may not be able to tell you why a particular prediction is obtained. This lack of understanding can mislead and can even have harmful consequences, a point that's worth unpacking in some detail…  Assumptions, assumptions A model of a real world process or phenomenon is necessarily a simplification. This is essentially because it is impossible to isolate a process or phenomenon from the rest of the world. As a consequence it is impossible to know for certain that the model one has built has incorporated all the interactions that influence the process / phenomenon of interest. It is quite possible that potentially important variables have been overlooked.  The selection of variables that go into a model is based on assumptions. In the case of model building in physics, these assumptions are made upfront and are thus clear to anybody who takes the trouble to read the underlying theory. In machine learning, however, the assumptions are harder to see because they are implicit in the data and the algorithm. This can be a problem when data is biased or an algorithm opaque.  Problem of bias and opacity become more acute as datasets increase in size and algorithms become more complex, especially when applied to social issues that have serious human consequences. I won't go into this here, but for examples the interested reader may want to have a look at Cathy O'Neil's book, Weapons of Math Destruction, or my article on the dark side of data science.  As an aside, I should point out that although assumptions are usually obvious in traditional modelling, they are often overlooked out of sheer laziness or, more charitably, lack of awareness. This can have disastrous consequences. The global financial crisis of 2008 can – to some extent – be blamed on the failure of trading professionals to understand assumptions behind the model that was used to calculate the value of collateralised debt obligations.  It all starts with a straight line…. Now that we've taken a tour of some of the key differences between model building in the old and new worlds, we are all set to start talking about machine learning proper.  I should begin by admitting that I overstated the point about opacity: there are some machine learning algorithms that are transparent as can possibly be. Indeed, chances are you know the  algorithm I'm going to discuss next, either from an introductory statistics course in university or from plotting relationships between two variables in your favourite spreadsheet.  Yea, you may have guessed that I'm referring to linear regression.  In its simplest avatar, linear regression attempts to fit a straight line to a set of data points in two dimensions. The two dimensions correspond to a dependent variable (traditionally denoted by y) and an independent variable (traditionally denoted by x).   An example of such a fitted line is shown in Figure 1.  Once such a line is obtained, one can "predict" the value of the dependent variable for any value of the independent variable.  In terms of our earlier discussion, the line is the model.  Figure 1: Linear Regression Figure 1: Linear Regression  Figure 1 also serves to illustrate that linear models are going to be inappropriate in most real world situations (the straight line does not fit the data well). But it is not so hard to devise methods to fit more complicated functions.  The important point here is that since machine learning is about finding functions that accurately predict dependent variables for as yet unknown values of the independent variables, most algorithms make explicit or implicit choices about the form of these functions.  Complexity versus simplicity At first sight it seems a no-brainer that complicated functions will work better than simple ones. After all, if we choose a nonlinear function with lots of parameters, we should be able to fit a complex data set better than a linear function can (See Figure 2 – the complicated function fits the datapoints better than the straight line).  But there's catch: although the ability to fit a dataset increases with the flexibility of the fitting function,  increasing complexity beyond a point will invariably reduce predictive power.  Put another way, a complex enough function may fit the known data points perfectly but, as a consequence, will inevitably perform poorly on unknown data. This is an important point so let's look at it in greater detail.  Figure 2: Simple and complex fitting functions Figure 2: Simple and complex fitting function (courtesy: Wikimedia)  Recall that the aim of machine learning is to predict values of the dependent variable for as yet unknown values of the independent variable(s).  Given a finite (and usually, very limited) dataset, how do we build a model that we can have some confidence in? The usual strategy is to partition the dataset into two subsets, one containing 60 to 80% of the data (called the training set) and the other containing the remainder (called the test set). The model is then built – i.e. an appropriate function fitted – using the training data and verified against the test data. The verification process consists of comparing the predicted values of the dependent variable with the known values for the test set.  Now, it should be intuitively clear that the more complicated the function, the better it will fit the training data.  Question: Why?  Answer: Because complicated functions have more free parameters – for example, linear functions of a single (dependent) variable have two parameters (slope and intercept), quadratics have three, cubics four and so on.  The mathematician, John von Neumann is believed to have said, "With four parameters I can fit an elephant, and with five I can make him wiggle his trunk." See this post for a nice demonstration of the literal truth of his words.  Put another way, complex functions are wrigglier than simple ones, and – by suitable adjustment of parameters – their "wriggliness" can be adjusted to fit the training data better than functions that are less wriggly. Figure 2 illustrates this point well.  This may sound like you can have your cake and eat it too: choose a complicated enough function and you can fit both the training and test data well. Not so! Keep in mind that the resulting model (fitted function) is built using the training set alone, so a good fit to the test data is not guaranteed.  In fact, it is intuitively clear that a function that fits the training data perfectly (as in Figure 2) is likely to do a terrible job on the test data.  Question: Why?  Answer:  Remember, as far as the model is concerned, the test data is unknown. Hence, the greater the wriggliness in the trained model, the less likely it is to fit the test data well. Remember, once the model is fitted to the training data, you have no freedom to tweak parameters any further.  This tension between simplicity and complexity of models is one of the key principles of machine learning and is called the bias-variance tradeoff. Bias here refers to lack of flexibility and variance, the reducible error. In general simpler functions have greater bias and lower variance and complex functions, the opposite.  Much of the subtlety of machine learning lies in developing an understanding of how to arrive at the right level of complexity for the problem at hand –  that is, how to tweak parameters so that the resulting function fits the training data just well enough so as to generalise well to unknown data.  Note: those who are curious to learn more about the bias-variance tradeoff may want to have a look at this piece.  For details on how to achieve an optimal tradeoff, search for articles on regularization in machine learning.  Unlocking unstructured data The discussion thus far has focused primarily on quantitative or enumerable data (numbers and categories) that's stored in  a structured format – i.e. as columns and rows in a spreadsheet or database table). This is fine as it goes, but the fact is that much of the data in organisations is unstructured, the most common examples being text documents and audio-visual media. This data is virtually impossible to analyse computationally using relational database technologies  (such as SQL) that are commonly used by organisations.  The situation has changed dramatically in the last decade or so. Text analysis techniques that once required expensive software and high-end computers have now been implemented in open source languages such as Python and R, and can be run on personal computers.  For problems that require computing power and memory beyond that, cloud technologies make it possible to do so cheaply. In my opinion, the ability to analyse textual data is the most important advance in data technologies in the last decade or so. It unlocks a world of possibilities for the curious data analyst. Just think, all those comment fields in your survey data can now be analysed in a way that was never possible in the relational world!  There is a general impression that text analysis is hard.  Although some of the advanced techniques can take a little time to wrap one's head around, the basics are simple enough. Yea, I really mean that – for proof, check out my tutorial on the topic.  Wrapping up I could go on for a while. Indeed, I was planning to delve into a few algorithms of increasing complexity (from regression to trees and forests to neural nets) and then close with a brief peek at some of the more recent headline-grabbing developments like deep learning. However, I realised that such an exploration would be too long and (perhaps more importantly) defeat the main intent of this piece which is to give starting students an idea of what machine learning is about, and how it differs from preexisting techniques of data analysis. I hope I have succeeded, at least partially, in achieving that aim.  For those who are interested in learning more about machine learning algorithms, I can suggest having a look at my "Gentle Introduction to Data Science using R" series of articles. Start with the one on text analysis (link in last line of previous section) and then move on to clustering, topic modelling, naive Bayes, decision trees, random forests and support vector machines. I'm slowly adding to the list as I find the time, so please do check back again from time to time. This article is based on my exploration of the basic text mining capabilities of  R, the open source statistical software. It is intended primarily as a tutorial  for novices in text mining as well as R. However, unlike conventional tutorials,  I spend a good bit of time setting the context by describing the problem that led me to text mining and thence  to R. I also talk about the limitations of  the techniques I describe,  and point out directions for further exploration for those who are interested. Indeed, I'll likely explore some of these myself in future articles.  If you have no time and /or wish to cut to the chase,  please go straight to the section entitled, Preliminaries – installing R and RStudio. If you have already installed R and have worked with it, you may want to stop reading as I  doubt there's anything I can tell you that you don't already know   A couple of warnings are in order before we proceed. R and the text mining options we explore below are open source software. Version differences in open source can be significant and are not always documented in a way that corporate IT types are used to. Indeed, I was tripped up by such differences in an earlier version of this article (now revised). So, just for the record, the examples below were run on version 3.2.0 of R and version 0.6-1 of the tm (text mining) package for R. A second point follows from this: as is evident from its version number, the tm package is still in the early stages of its evolution. As a result – and we will see this below – things do not always work as advertised. So assume nothing, and inspect the results in detail at every step. Be warned that I do not always do this below, as my aim is introduction rather than accuracy.  Background and motivation Traditional data analysis is based on the relational model in which data is stored in tables. Within tables, data is stored in rows – each row representing a  single record of an entity of interest (such as a customer or an account). The columns represent attributes of the entity. For example, the customer table might consist of columns such as name, street address, city, postcode, telephone number .  Typically these  are defined upfront, when the data model is created. It is possible to add columns after the fact, but this tends to be messy because one also has to update existing rows with information pertaining to the added attribute.  As long as one asks for information that is based  only on existing attributes – an example being,  "give me a list of customers based  in Sydney" –  a database analyst can use Structured Query Language (the defacto language of relational databases ) to  get an answer.  A problem arises, however, if one asks for information that is based on attributes that are not included in the database. An example  in the above case would be: "give me a list of  customers who have made a complaint in the last twelve months."  As a result of the above, many data modelers will include  a "catch-all"  free text column that can be used to capture additional information in an ad-hoc way. As one might imagine, this column will often end up containing several lines, or even paragraphs of text that are near impossible to analyse with the tools available in relational databases.  (Note: for completeness I should add that most database vendors have incorporated text mining capabilities into their products. Indeed, many  of them now include R…which is another good reason to learn it.)  My story Over the last few months, when time permits, I've been doing an in-depth exploration of  the data captured by my organisation's IT service management tool.  Such tools capture all support tickets that are logged, and track their progress until they are closed.  As it turns out,  there are a number of cases where calls are logged against categories that are too broad to be useful – the infamous catch-all category called "Unknown."  In such cases, much of the important information is captured in a free text column, which is difficult to analyse unless one knows what one is looking for. The problem I was grappling with was to identify patterns and hence define sub-categories that would enable support staff to categorise these calls meaningfully.  One way to do this  is  to guess what the sub-categories might be…and one can sometimes make pretty good guesses if one knows the data well enough.  In general, however, guessing is a terrible strategy because one does not know what one does not know. The only sensible way to extract subcategories is to  analyse  the content of the free text column systematically. This is a classic text mining problem.  Now, I knew a bit about the theory of text mining, but had little practical experience with it. So the logical place for me to  start was to look for a suitable text mining tool. Our vendor (who shall remain unnamed) has a "Rolls-Royce" statistical tool that has a good text mining add-on. We don't have licenses for the tool, but the vendor was willing to give us a trial license for a few months…with the understanding that this was on an intent-to-purchase basis.  I therefore started looking at open source options. While doing so, I stumbled on an interesting paper by Ingo Feinerer that describes a text mining framework for the R environment. Now, I knew about R, and was vaguely aware that it offered text mining capabilities, but I'd not looked into the details.  Anyway, I started reading the paper…and kept going until I finished.  As I read, I realised that this could be the answer to my problems. Even better, it would not require me trade in assorted limbs for a license.  I decided to give it a go.  Preliminaries – installing R and RStudio R can be downloaded from the R Project website. There is a Windows version available, which installed painlessly on my laptop. Commonly encountered installation issues are answered in the (very helpful)  R for Windows FAQ.  RStudio is an integrated development environment (IDE) for R. There is a commercial version of the product, but there is also a free open source version. In what follows, I've used the free version. Like R, RStudio installs painlessly and also detects your R installation.  RStudio has  the following panels:  A script editor in which you can create R scripts (top left). You can also open a new script editor window by going to File > New File > RScript. The console where you can execute R commands/scripts (bottom left) Environment and history (top right) Files in the current working directory, installed R packages, plots and a help display screen (bottom right). Check out this short video for a quick introduction to RStudio.  You can access help anytime (within both R and RStudio) by typing a question mark before a command. Exercise: try this by typing ?getwd() and ?setwd() in the console.  I should reiterate that the installation process for both products was seriously simple…and seriously impressive.  "Rolls-Royce" business intelligence vendors could take a lesson from  that…in addition to taking a long hard look at the ridiculous prices they charge.  There is another small step before we move on to the fun stuff.  Text mining  and certain plotting packages are not installed by default so one has to install them manually The relevant packages are:  tm – the text mining package (see documentation). Also check out this excellent introductory article on tm. SnowballC – required for stemming (explained below). ggplot2 – plotting capabilities (see documentation) wordcloud – which is self-explanatory (see documentation) . (Warning for Windows users: R is case-sensitive so Wordcloud != wordcloud)  The simplest way to install packages is to use RStudio's built in capabilities (go to Tools > Install Packages in the menu). If you're working on Windows 7 or 8, you might run into a permissions issue when installing packages. If you do, you might find this advice from the R for Windows FAQ helpful.  Preliminaries – The example dataset The data I had from our service management tool isn't  the best dataset to learn with as it is quite messy. But then, I  have a reasonable data source in my virtual backyard:  this blog. To this end, I converted all posts I've written since Dec 2013 into plain text form (30 posts in all). You can download the zip file of these here .  I suggest you create a new folder called – called, say, TextMining – and unzip the files in that folder.  That done, we're good to start…  Preliminaries – Basic Navigation A few things to note before we proceed:  In what follows, I enter the commands directly in the console. However,  here's a little RStudio tip that you may want to consider: you can enter an R command or code fragment in the script editor and then hit Ctrl-Enter  (i.e. hit the Enter key while holding down the Control key) to copy the line to the console.  This will enable you to save the script as you go along. In the code snippets below, the functions / commands to be typed in the R console are in blue font.  The output is in black. I will also denote references to  functions / commands in the body of the article by italicising them as in "setwd()".  Be aware that I've omitted the command prompt ">" in the code snippets below! It is best not to cut-n-paste commands directly from the article as quotes are sometimes not rendered correctly. A text file of all the code in this article is available here. The > prompt in the RStudio console indicates that R is ready to process commands.  To see the current working directory type in getwd() and hit return. You'll see something like:  getwd() [1] "C:/Users/Documents"  The exact output will of course depend on your working directory.  Note the forward slashes in the path. This is because of R's Unix heritage (backslash is an escape character in R.). So, here's how would change the working directory to C:\Users:  setwd("C:/Users") You can now use getwd()to check that setwd() has done what it should.  getwd() [1]"C:/Users"  I won't say much more here about R  as I want to get on with the main business of the article.  Check out this very short introduction to R for a quick crash course.  Loading data into R Start RStudio and open the TextMining project you created earlier.  The next step is to load the tm package as this is not loaded by default.  This is done using the library() function like so:  library(tm) Loading required package: NLP  Dependent packages are loaded automatically – in this case the dependency is on the NLP (natural language processing) package.  Next, we need to create a collection of documents (technically referred to as a Corpus) in the R environment. This basically involves loading the files created in the TextMining folder into a Corpus object. The tm package provides the Corpus() function to do this. There are several ways to  create a Corpus (check out the online help using ? as explained earlier). In a nutshell, the  Corpus() function can read from various sources including a directory. That's the option we'll use:  #Create Corpus docs <- Corpus(DirSource("C:/Users/Kailash/Documents/TextMining"))  At the risk of stating the obvious, you will need to tailor this path as appropriate.  A couple of things to note in the above. Any line that starts with a # is a comment, and the "<-" tells R to assign the result of the command on the right hand side to the variable on the left hand side. In this case the Corpus object created is stored in a variable called docs.  One can also use the equals sign (=)  for assignment if one wants to.  Type in docs to see some information about the newly created corpus:  docs <<VCorpus>> Metadata: corpus specific: 0, document level (indexed): 0 Content: documents: 30  The summary() function gives more details, including a complete listing of files…but it isn't particularly enlightening.  Instead, we'll examine a particular document in the corpus.  #inspect a particular document writeLines(as.character(docs[[30]])) …output not shown… Which prints the entire content of 30th document in the corpus to the console.  Pre-processing Data cleansing, though tedious, is perhaps the most important step in text analysis.  As we will see, dirty data can play havoc with the results.  Furthermore, as we will also see, data cleaning is invariably an iterative process as there are always problems that are overlooked the first time around.  The tm package offers a number of transformations that ease the tedium of cleaning data. To see the available transformations  type getTransformations() at the R prompt:  > getTransformations() [1] "removeNumbers" "removePunctuation" "removeWords" "stemDocument" "stripWhitespace"   Most of these are self-explanatory. I'll explain those that aren't as we go along.  There are a few preliminary clean-up steps we need to do before we use these powerful transformations. If you inspect some documents in the corpus (and you know how to do that now), you will notice that I have some quirks in my writing. For example, I often use colons and hyphens without spaces between the words separated by them. Using the removePunctuation transform  without fixing this will cause the two words on either side of the symbols  to be combined. Clearly, we need to fix this prior to using the transformations.  To fix the above, one has to create a custom transformation. The tm package provides the ability to do this via the content_transformer function. This function takes a function as input, the input function should specify what transformation needs to be done. In this case, the input function would be one that replaces all instances of a character by spaces. As it turns out the gsub() function does just that.  Here is the R code to build the content transformer, which  we will call toSpace:  #create the toSpace content transformer toSpace <- content_transformer(function(x, pattern) {return (gsub(pattern, " ", x))}) Now we can use  this content transformer to eliminate colons and hypens like so:  docs <- tm_map(docs, toSpace, "-") docs <- tm_map(docs, toSpace, ":")  Inspect random sections f corpus to check that the result is what you intend (use writeLines as shown earlier). To reiterate something I mentioned in the preamble, it is good practice to inspect the a subset of the corpus after each transformation.  If it all looks good, we can now apply the removePunctuation transformation. This is done as follows:  #Remove punctuation – replace punctuation marks with " " docs <- tm_map(docs, removePunctuation)  Inspecting the corpus reveals that several  "non-standard" punctuation marks have not been removed. These include the single curly quote marks and a space-hyphen combination. These can be removed using our custom content transformer, toSpace. Note that you might want to copy-n-paste these symbols directly from the relevant text file to ensure that they are accurately represented in toSpace.  docs <- tm_map(docs, toSpace, "'") docs <- tm_map(docs, toSpace, "‘") docs <- tm_map(docs, toSpace, " -")  Inspect the corpus again to ensure that the offenders have been eliminated. This is also a good time to check for any other special symbols that may need to be removed manually.  If all is well, you can move  to the next step which is  to:  Convert the corpus to lower case Remove all numbers. Since R is case sensitive, "Text" is not equal to "text" – and hence the rationale for converting to a standard case.  However, although there is a tolower transformation, it is not a part of the standard tm transformations (see the output of getTransformations() in the previous section). For this reason, we have to convert tolower into a transformation that can handle a corpus object properly. This is done with the help of our new friend, content_transformer.  Here's the relevant code:  #Transform to lower case (need to wrap in content_transformer) docs <- tm_map(docs,content_transformer(tolower)) Text analysts are typically not interested in numbers since these do not usually contribute to the meaning of the text. However, this may not always be so. For example, it is definitely not the case if one is interested in getting a count of the number of times a particular year appears in a corpus. This does not need to be wrapped in content_transformer as it is a standard transformation in tm.  #Strip digits (std transformation, so no need for content_transformer) docs <- tm_map(docs, removeNumbers) Once again, be sure to inspect the corpus before proceeding.  The next step is to remove common words  from the text. These  include words such as articles (a, an, the), conjunctions (and, or but etc.), common verbs (is), qualifiers (yet, however etc) . The tm package includes  a standard list of such stop words as they are referred to. We remove stop words using the standard removeWords transformation like so:  #remove stopwords using the standard list in tm docs <- tm_map(docs, removeWords, stopwords("english"))  Finally, we remove all extraneous whitespaces using the stripWhitespace transformation:  #Strip whitespace (cosmetic?) docs <- tm_map(docs, stripWhitespace)  Stemming Typically a large corpus will contain  many words that have a common root – for example: offer, offered and offering.  Stemming is the process of reducing such related words to their common root, which in this case would be the word offer.  Simple stemming algorithms (such as the one in tm) are relatively crude: they work by chopping off the ends of words. This can cause problems: for example, the words mate and mating might be reduced to mat instead of mate.  That said, the overall benefit gained from stemming more than makes up for the downside of such special cases.  To see what stemming does, let's take a look at the  last few lines  of the corpus before and after stemming.  Here's what the last bit looks  like prior to stemming (note that this may differ for you, depending on the ordering of the corpus source files in your directory):  writeLines(as.character(docs[[30]])) flexibility eye beholder action increase organisational flexibility say redeploying employees likely seen affected move constrains individual flexibility dual meaning characteristic many organizational platitudes excellence synergy andgovernance interesting exercise analyse platitudes expose difference espoused actual meanings sign wishing many hours platitude deconstructing fun  Now let's stem the corpus and reinspect it.  #load library library(SnowballC) #Stem document docs <- tm_map(docs,stemDocument) writeLines(as.character(docs[[30]])) flexibl eye behold action increas organis flexibl say redeploy employe like seen affect move constrain individu flexibl dual mean characterist mani organiz platitud excel synergi andgovern interest exercis analys platitud expos differ espous actual mean sign wish mani hour platitud deconstruct fun  A careful comparison of the two paragraphs reveals the benefits and tradeoff of this relatively crude process.  There is a more sophisticated procedure called lemmatization that takes grammatical context into account. Among other things, determining the lemma of a word requires a knowledge of its part of speech (POS) – i.e. whether it is a noun, adjective etc. There are POS taggers that automate the process of tagging terms with their parts of speech. Although POS taggers are available for R (see this one, for example), I will not go into this topic here as it would make a long post even longer.  On another important note, the output of the corpus also shows up a problem or two. First, organiz and organis are actually variants of the same stem organ. Clearly, they should be merged. Second, the word andgovern should be separated out into and and govern (this is an error in the original text).  These (and other errors of their ilk) can and should be fixed up before proceeding.  This is easily done using gsub() wrapped in content_transformer. Here is the code to  clean up these and a few other issues  that I found:  docs <- tm_map(docs, content_transformer(gsub), pattern = "organiz", replacement = "organ") docs <- tm_map(docs, content_transformer(gsub), pattern = "organis", replacement = "organ") docs <- tm_map(docs, content_transformer(gsub), pattern = "andgovern", replacement = "govern") docs <- tm_map(docs, content_transformer(gsub), pattern = "inenterpris", replacement = "enterpris") docs <- tm_map(docs, content_transformer(gsub), pattern = "team-", replacement = "team")  Note that I have removed the stop words and and in in the 3rd and 4th transforms above.  There are definitely other errors that need to be cleaned up, but I'll leave these for you to detect and remove.  The document term matrix The next step in the process is the creation of the document term matrix  (DTM)– a matrix that lists all occurrences of words in the corpus, by document. In the DTM, the documents are represented by rows and the terms (or words) by columns.  If a word occurs in a particular document, then the matrix entry for corresponding to that row and column is 1, else it is 0 (multiple occurrences within a document are recorded – that is, if a word occurs twice in a document, it is recorded as "2" in the relevant matrix entry).  A simple example might serve to explain the structure of the TDM more clearly. Assume we have a simple corpus consisting of two documents, Doc1 and Doc2, with the following content:  Doc1: bananas are yellow  Doc2: bananas are good  The DTM for this corpus would look like:  bananas	are	yellow	good Doc1	1	1	1	0 Doc2	1	1	0	1  Clearly there is nothing special about rows and columns – we could just as easily transpose them. If we did so, we'd get a term document matrix (TDM) in which the terms are rows and documents columns. One can work with either a DTM or TDM. I'll use the DTM in what follows.  There are a couple of general points worth making before we proceed. Firstly, DTMs (or TDMs) can be huge – the dimension of the matrix would be number of document  x the number of words in the corpus.  Secondly, it is clear that the large majority of words will appear only in a few documents. As a result a DTM is invariably sparse – that is, a large number of its entries are 0.  The business of creating a DTM (or TDM) in R is as simple as:  dtm <- DocumentTermMatrix(docs)  This creates a term document matrix from the corpus and stores the result in the variable dtm. One can get summary information on the matrix by typing the variable name in the console and hitting return:  dtm <<DocumentTermMatrix (documents: 30, terms: 4209)>> Non-/sparse entries: 14252/112018 Sparsity : 89% Maximal term length: 48 Weighting : term frequency (tf)  This is a 30 x 4209 dimension matrix in which 89% of the rows are zero.  One can inspect the DTM, and you might want to do so for fun. However, it isn't particularly illuminating because of the sheer volume of information that will flash up on the console. To limit the information displayed, one can inspect a small section of it like so:  inspect(dtm[1:2,1000:1005]) <<DocumentTermMatrix (documents: 2, terms: 6)>> Non-/sparse entries: 0/12 Sparsity : 100% Maximal term length: 8 Weighting : term frequency (tf) Docs                     creation creativ credibl credit crimin crinkl BeyondEntitiesAndRelationships.txt      0      0    0    0    0    0 bigdata.txt                     0      0    0    0    0    0 This command displays terms 1000 through 1005 in the first two rows of the DTM. Note that your results may differ.  Mining the corpus Notice that in constructing the TDM, we have converted a corpus of text into a mathematical object that can be analysed using quantitative techniques of matrix algebra.  It should be no surprise, therefore, that the TDM (or DTM) is the starting point for quantitative text analysis.  For example, to get the frequency of occurrence of each word in the corpus, we simply sum over all rows to give column sums:  freq <- colSums(as.matrix(dtm))  Here we have  first converted the TDM into a mathematical matrix using the as.matrix() function. We have then summed over all rows to give us the totals for each column (term). The result is stored in the (column matrix) variable freq.  Check that the dimension of freq equals the number of terms:  #length should be total number of terms length(freq) [1] 4209  Next, we sort freq in descending order of term count:  #create sort order (descending) ord <- order(freq,decreasing=TRUE)  Then list the most and least frequently occurring terms:  #inspect most frequently occurring terms freq[head(ord)] one organ can manag work system 314 268   244  222  202  193 #inspect least frequently occurring terms freq[tail(ord)] yield yorkshir  youtub    zeno    zero   zulli 1      1      1      1      1      1  The  least frequent terms can be more interesting than one might think. This is  because terms that occur rarely are likely to be more descriptive of specific documents. Indeed, I can recall the posts in which I have referred to Yorkshire, Zeno's Paradox and  Mr. Lou Zulli without having to go back to the corpus, but I'd have a hard time enumerating the posts in which I've used the word system.  There are at least a couple of ways to simple ways to strike a balance between frequency and specificity. One way is to use so-called  inverse document frequencies. A simpler approach is to  eliminate words that occur in a large fraction of corpus documents.  The latter addresses another issue that is evident in the above. We deal with this now.  Words like "can" and "one"  give us no information about the subject matter of the documents in which they occur. They can therefore be eliminated without loss. Indeed, they ought to have been eliminated by the stopword removal we did earlier. However, since such words occur very frequently – virtually in all documents – we can remove them by enforcing bounds when creating the DTM, like so:  dtmr <-DocumentTermMatrix(docs, control=list(wordLengths=c(4, 20), bounds = list(global = c(3,27))))  Here we have told R to include only those words that occur in  3 to 27 documents. We have also enforced  lower and upper limit to length of the words included (between 4 and 20 characters).  Inspecting the new DTM:  dtmr <<DocumentTermMatrix (documents: 30, terms: 1290)>> Non-/sparse entries: 10002/28698 Sparsity : 74% Maximal term length: 15 Weighting : term frequency (tf) The dimension is reduced to 30 x 1290.  Let's calculate the cumulative frequencies of words across documents and sort as before:  freqr <- colSums(as.matrix(dtmr)) #length should be total number of terms length(freqr) [1] 1290 #create sort order (asc) ordr <- order(freqr,decreasing=TRUE) #inspect most frequently occurring terms freqr[head(ordr)] organ manag work system project problem 268    222  202   193    184    171 #inspect least frequently occurring terms freqr[tail(ordr)] wait warehous welcom whiteboard wider widespread 3        3    3       3    3       3  The results make sense: the top 6 keywords are pretty good descriptors of what my blogs is about – projects, management and systems. However, not all high frequency words need be significant. What they do, is give you an idea of potential classification terms.  That done, let's take get a list of terms that occur at least a  100 times in the entire corpus. This is easily done using the findFreqTerms() function as follows:  findFreqTerms(dtmr,lowfreq=80) [1] "action" "approach" "base" "busi" "chang" "consult" "data" "decis" "design" [10] "develop" "differ" "discuss" "enterpris" "exampl" "group" "howev" "import" "issu" [19] "like" "make" "manag" "mani" "model" "often" "organ" "peopl" "point" [28] "practic" "problem" "process" "project" "question" "said" "system" "thing" "think" [37] "time" "understand" "view" "well" "will" "work"  Here I have asked findFreqTerms() to return all terms that occur more than 80 times in the entire corpus. Note, however, that the result is ordered alphabetically, not by frequency.  Now that we have the most frequently occurring terms in hand, we can check for correlations between some of these and other terms that occur in the corpus.  In this context, correlation is a quantitative measure of the co-occurrence of words in multiple documents.  The tm package provides the findAssocs() function to do this.  One needs to specify the DTM, the term of interest and the correlation limit. The latter is a number between 0 and 1 that serves as a lower bound for  the strength of correlation between the  search and result terms. For example, if the correlation limit is 1, findAssocs() will return only  those words that always co-occur with the search term. A correlation limit of 0.5 will return terms that have a search term co-occurrence of at least  50% and so on.  Here are the results of  running findAssocs() on some of the frequently occurring terms (system, project, organis) at a correlation of 60%.   findAssocs(dtmr,"project",0.6) project inher 0.82 handl 0.68 manag 0.68 occurr 0.68 manager' 0.66 findAssocs(dtmr,"enterpris",0.6) enterpris agil     0.80 realist   0.78 increment  0.77 upfront   0.77 technolog  0.70 neither   0.69 solv     0.69 adapt    0.67 architectur 0.67 happi    0.67 movement  0.67 architect  0.66 chanc    0.65 fine     0.64 featur    0.63 findAssocs(dtmr,"system",0.6) system design  0.78 subset  0.78 adopt  0.77 user   0.77 involv  0.71 specifi 0.71 function 0.70 intend  0.67 softwar 0.67 step   0.67 compos  0.66 intent  0.66 specif  0.66 depart  0.65 phone  0.63 frequent 0.62 today  0.62 pattern 0.61 cognit 0.60 wherea 0.60  An important point to note that the presence of a term in these list is not indicative of its frequency.  Rather it is a measure of the frequency with which the two (search and result term)  co-occur (or show up together) in documents across . Note also, that it is not an indicator of nearness or contiguity. Indeed, it cannot be because the document term matrix does not store any information on proximity of terms, it is simply a "bag of words."  That said, one can already see that the correlations throw up interesting combinations – for example, project and manag, or enterpris and agil or architect/architecture, or system and design or adopt. These give one further insights into potential classifications.  As it turned out,  the very basic techniques listed above were enough for me to get a handle on the original problem that led me to text mining – the analysis of free text problem descriptions in my organisation's service management tool.  What I did was to work my way through the top 50 terms and find their associations. These revealed a number of sets of keywords that occurred in multiple problem descriptions,  which was good enough for me to define some useful sub-categories.  These are currently being reviewed by the service management team. While they're busy with that that, I'm looking into refining these further using techniques such as  cluster analysis and tokenization.  A simple case of the latter would be to look at two-word combinations in the text (technically referred to as bigrams). As one might imagine, the dimensionality of the DTM will quickly get out of hand as one considers larger multi-word combinations.  Anyway,  all that and more will topics have to wait for future  articles as this piece is much too long already. That said, there is one thing I absolutely must touch upon before signing off. Do stay, I think you'll find  it interesting.  Basic graphics One of the really cool things about R is its graphing capability. I'll do just a couple of simple examples to give you a flavour of its power and cool factor. There are lots of nice examples on the Web that you can try out for yourself.  Let's first do a simple frequency histogram. I'll use the ggplot2 package, written by Hadley Wickham to do this. Here's the code:  wf=data.frame(term=names(freqr),occurrences=freqr) library(ggplot2) p <- ggplot(subset(wf, freqr>100), aes(term, occurrences)) p <- p + geom_bar(stat="identity") p <- p + theme(axis.text.x=element_text(angle=45, hjust=1)) p  Figure 1 shows the result.  Fig 1: Term-occurrence histogram (freq>100) Fig 1: Term-occurrence histogram (freq>100)  The first line creates a data frame – a list of columns of equal length. A data frame also contains the name of the columns – in this case these are term and occurrence respectively.  We then invoke ggplot(), telling it to consider plot only those terms that occur more than 100 times.  The aes option in ggplot describes plot aesthetics – in this case, we use it to specify the x and y axis labels. The stat="identity" option in geom_bar () ensures  that the height of each bar is proportional to the data value that is mapped to the y-axis  (i.e occurrences). The last line specifies that the x-axis labels should be at a 45 degree angle and should be horizontally justified (see what happens if you leave this out). Check out the voluminous ggplot documentation for more or better yet, this quick introduction to ggplot2 by Edwin Chen.  Finally, let's create a wordcloud for no other reason than everyone who can seems to be doing it.  The code for this is:  #wordcloud library(wordcloud) #setting the same seed each time ensures consistent look across clouds set.seed(42) #limit words by specifying min frequency wordcloud(names(freqr),freqr, min.freq=70)  The result is shown Figure 2.   Fig 2: Wordcloud (freq>70) Fig 2: Wordcloud (freq>70)  Here we first load the wordcloud package which is not loaded by default. Setting a seed number ensures that you get the same look each time (try running it without setting a seed). The arguments of the wordcloud() function are straightforward enough. Note that one can specify the maximum number of words to be included instead of the minimum frequency (as I have done above).  See the word cloud  documentation for more.  This word cloud also makes it clear that stop word removal has not done its job well, there are a number of words it has missed (also and however, for example). These can be removed by augmenting the built-in stop word list with a custom one. This is left as an exercise for the reader :-).  Finally, one can make the wordcloud more visually appealing by adding colour as follows:  #…add color wordcloud(names(freqr),freqr,min.freq=70,colors=brewer.pal(6,"Dark2″))  The result is shown Figure 3.    Fig 3: Wordcloud (freq > 70) Fig 3: Wordcloud (freq > 70)  You may need to load the RColorBrewer package to get this to work. Check out the brewer documentation to experiment with more colouring options.  Wrapping up This brings me to the end of this rather long  (but I hope, comprehensible) introduction to text mining R.  It should be clear that despite the length of the article, I've covered only the most rudimentary basics.  Nevertheless, I hope I've succeeded in conveying  a sense of the possibilities in the vast and rapidly-expanding discipline of text analytics.Welcome to the second part of my introductory series on text analysis using R (the first article can be accessed here).  My aim in the present piece is to provide a  practical introduction to cluster analysis. I'll begin with some background before moving on to the nuts and bolts of clustering. We have a fair bit to cover, so let's get right to it.  A common problem when analysing large collections of documents is to categorize them in some meaningful way. This is easy enough if one has a predefined classification scheme that is known to fit the collection (and if the collection is small enough to be browsed manually). One can then simply scan the documents, looking for keywords appropriate to each category and classify the documents based on the results. More often than not, however, such a classification scheme is not available and the collection too large. One then needs to use algorithms that can classify documents automatically based on their structure and content.  The present post is a practical introduction to a couple of automatic text categorization techniques, often referred to as clustering algorithms.  As the Wikipedia article on clustering tells us:  Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) are more similar (in some sense or another) to each other than to those in other groups (clusters).  As one might guess from the above, the results of clustering depend rather critically on the method one uses to group objects. Again, quoting from the Wikipedia piece:  Cluster analysis itself is not one specific algorithm, but the general task to be solved. It can be achieved by various algorithms that differ significantly in their notion of what constitutes a cluster and how to efficiently find them. Popular notions of clusters include groups with small distances [Note: we'll use distance-based methods] among the cluster members, dense areas of the data space, intervals or particular statistical distributions [i.e. distributions of words within documents and the entire collection].  …and a bit later:  …the notion of a "cluster" cannot be precisely defined, which is one of the reasons why there are so many clustering algorithms. There is a common denominator: a group of data objects. However, different researchers employ different cluster models, and for each of these cluster models again different algorithms can be given. The notion of a cluster, as found by different algorithms, varies significantly in its properties. Understanding these "cluster models" is key to understanding the differences between the various algorithms.  An upshot of the above is that it is not always straightforward to interpret the output of clustering algorithms. Indeed, we will see this in the example discussed below.  With that said for an introduction, let's move on to the nut and bolts of clustering.  Preprocessing the corpus In this section I cover the steps required to create the R objects necessary in order to do clustering. It goes over territory that I've covered in detail in the first article in this series – albeit with a few tweaks, so you may want to skim through even if you've read my previous piece.  To begin with I'll assume you have R and RStudio (a free development environment for R) installed on your computer and are familiar with the basic functionality in the text mining ™ package.  If you need help with this, please look at the instructions in my previous article on text mining.  As in the first part of this series,  I will use 30 posts from my blog as the example collection (or corpus, in text mining-speak). The corpus can be downloaded here. For completeness, I will run through the entire sequence of steps – right from loading the corpus into R, to running the two clustering algorithms.  Ready? Let's go…  The first step is to fire up RStudio and navigate to the directory in which you have unpacked the example corpus. Once this is done, load the text mining package, tm.  Here's the relevant code (Note: a complete listing of the code in this article can be accessed here):  getwd() [1] "C:/Users/Kailash/Documents" #set working directory – fix path as needed! setwd("C:/Users/Kailash/Documents/TextMining") #load tm library library(tm) Loading required package: NLP Note: R commands are in blue, output in black or red; lines that start with # are comments.  If you get an error here, you probably need to download and install the tm package. You can do this in RStudio by going to Tools > Install Packages and entering "tm". When installing a new package, R automatically checks for and installs any dependent packages.  The next step is to load the collection of documents into an object that can be manipulated by functions in the tm package.  #Create Corpus docs <- Corpus(DirSource("C:/Users/Kailash/Documents/TextMining")) #inspect a particular document writeLines(as.character(docs[[30]])) … The next step is to clean up the corpus. This includes things such as transforming to a consistent case, removing non-standard symbols & punctuation, and removing numbers (assuming that numbers do not contain useful information, which is the case here):  #Transform to lower case docs <- tm_map(docs,content_transformer(tolower)) #remove potentiallyy problematic symbols toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))}) docs <- tm_map(docs, toSpace, "-") docs <- tm_map(docs, toSpace, ":") docs <- tm_map(docs, toSpace, "‘") docs <- tm_map(docs, toSpace, "•") docs <- tm_map(docs, toSpace, "•   ") docs <- tm_map(docs, toSpace, " -") docs <- tm_map(docs, toSpace, """) docs <- tm_map(docs, toSpace, """) #remove punctuation docs <- tm_map(docs, removePunctuation) #Strip digits docs <- tm_map(docs, removeNumbers) Note: please see my previous article for more on content_transformer and the toSpace function defined above.  Next we remove stopwords – common words (like "a" "and" "the", for example) and eliminate extraneous whitespaces.  #remove stopwords docs <- tm_map(docs, removeWords, stopwords("english")) #remove whitespace docs <- tm_map(docs, stripWhitespace) writeLines(as.character(docs[[30]])) flexibility eye beholder action increase organisational flexibility say redeploying employees likely seen affected move constrains individual flexibility dual meaning characteristic many organizational platitudes excellence synergy andgovernance interesting exercise analyse platitudes expose difference espoused actual meanings sign wishing many hours platitude deconstructing fun At this point it is critical to inspect the corpus because  stopword removal in tm can be flaky. Yes, this is annoying but not a showstopper because one can remove problematic words manually once one has identified them – more about this in a minute.  Next, we stem the document – i.e. truncate words to their base form. For example, "education", "educate" and "educative" are stemmed to "educat.":  docs <- tm_map(docs,stemDocument) Stemming works well enough, but there are some fixes that need to be done due to my inconsistent use of British/Aussie and US English. Also, we'll take this opportunity to fix up some concatenations like "andgovernance" (see paragraph printed out above). Here's the code:  docs <- tm_map(docs, content_transformer(gsub),pattern = "organiz", replacement = "organ") docs <- tm_map(docs, content_transformer(gsub), pattern = "organis", replacement = "organ") docs <- tm_map(docs, content_transformer(gsub), pattern = "andgovern", replacement = "govern") docs <- tm_map(docs, content_transformer(gsub), pattern = "inenterpris", replacement = "enterpris") docs <- tm_map(docs, content_transformer(gsub), pattern = "team-", replacement = "team") The next step is to remove the stopwords that were missed by R. The best way to do this  for a small corpus is to go through it and compile a list of words to be eliminated. One can then create a custom vector containing words to be removed and use the removeWords transformation to do the needful. Here is the code (Note:  + indicates a continuation of a statement from the previous line):  myStopwords <- c("can", "say","one","way","use", +            "also","howev","tell","will", +            "much","need","take","tend","even", +            "like","particular","rather","said", +            "get","well","make","ask","come","end", +            "first","two","help","often","may", +            "might","see","someth","thing","point", +            "post","look","right","now","think","'ve ", +            "'re ") #remove custom stopwords docs <- tm_map(docs, removeWords, myStopwords) Again, it is a good idea to check that the offending words have really been eliminated.  The final preprocessing step is to create a document-term matrix (DTM) – a matrix that lists all occurrences of words in the corpus.  In a DTM, documents are represented by rows and the terms (or words) by columns.  If a word occurs in a particular document n times, then the matrix entry for corresponding to that row and column is n, if it doesn't occur at all, the entry is 0.  Creating a DTM is straightforward– one simply uses the built-in DocumentTermMatrix function provided by the tm package like so:  dtm <- DocumentTermMatrix(docs) #print a summary dtm Non-/sparse entries: 13312/110618 Sparsity        : 89% Maximal term length: 48 Weighting       : term frequency (tf)  This brings us to the end of the preprocessing phase. Next, I'll briefly explain how distance-based algorithms work before going on to the actual work of clustering.  An intuitive introduction to the algorithms As mentioned in the introduction, the basic idea behind document or text clustering is to categorise documents into groups based on likeness. Let's take a brief look at how the algorithms work their magic.  Consider the structure of the DTM. Very briefly, it is a matrix in which the documents are represented as rows and words as columns. In our case, the corpus has 30 documents and 4131 words, so the DTM is a 30 x 4131 matrix.  Mathematically, one can think of this matrix as describing a 4131 dimensional space in which each of the words represents a coordinate axis and each document is represented as a point in this space. This is hard to visualise of course, so it may help to illustrate this via a two-document corpus with only three words in total.  Consider the following corpus:  Document A: "five plus five"  Document B: "five plus six"  These two  documents can be represented as points in a 3 dimensional space that has the words "five" "plus" and "six" as the three coordinate axes (see figure 1).  Figure 1: Documents A and B as points in a 3-word space Figure 1: Documents A and B as points in a 3-word space  Now, if each of the documents can be thought of as a point in a space, it is easy enough to take the next logical step which is to define the notion of a distance between two points (i.e. two documents). In figure 1 the distance between A and B  (which I denote as D(A,B))is the length of the line connecting the two points, which is simply, the sum of the squares of the differences between the coordinates of the two points representing the documents.  D(A,B) = \sqrt{(2-1)^2 + (1-1)^2+(0-1)^2} = \sqrt 2  Generalising the above to the 4131 dimensional space at hand, the distance between two documents (let's call them X and Y) have coordinates (word frequencies)  (x_1,x_2,...x_{4131}) and (y_1,y_2,...y_{4131}), then one can define the straight line distance (also called Euclidean distance)  D(X,Y) between them as:  D(X,Y) = \sqrt{(x_1 - y_1)^2+(x_2 - y_2)^2+...+(x_{4131} - y_{4131})^2}  It should be noted that the Euclidean distance that I have described is above is not the only possible way to define distance mathematically. There are many others but it would take me too far afield to discuss them here – see this article for more  (and don't be put off by the term metric,  a metric  in this context is merely a distance)  What's important here is the idea that one can define a numerical distance between documents. Once this is grasped, it is easy to understand the basic idea behind how (some) clustering algorithms work – they group documents based on distance-related criteria.  To be sure, this explanation is simplistic and glosses over some of the complicated details in the algorithms. Nevertheless it is a reasonable, approximate explanation for what goes on under the hood. I hope purists reading this will agree!  Finally, for completeness I should mention that there are many clustering algorithms out there, and not all of them are distance-based.  Hierarchical clustering The first algorithm we'll look at is hierarchical clustering. As the Wikipedia article on the topic tells us, strategies for hierarchical clustering fall into two types:  Agglomerative: where we start out with each document in its own cluster. The algorithm  iteratively merges documents or clusters that are closest to each other until the entire corpus forms a single cluster. Each merge happens at a different (increasing) distance.  Divisive:  where we start out with the entire set of documents in a single cluster. At each step  the algorithm splits the cluster recursively until each document is in its own cluster. This is basically the inverse of an agglomerative strategy.  The algorithm we'll use is hclust which does agglomerative hierarchical clustering. Here's a simplified description of how it works:  Assign each document to its own (single member) cluster Find the pair of clusters that are closest to each other and merge them. So you now have one cluster less than before. Compute distances between the new cluster and each of the old clusters. Repeat steps 2 and 3 until you have a single cluster containing all documents. We'll need to do a few things before running the algorithm. Firstly, we need to convert the DTM into a standard matrix which can be used by dist, the distance computation function in R (the DTM is not stored as a standard matrix). We'll also shorten the document names so that they display nicely in the graph that we will use to display results of hclust (the names I have given the documents are just way too long). Here's the relevant code:  #convert dtm to matrix m <- as.matrix(dtm) #write as csv file (optional) write.csv(m,file="dtmEight2Late.csv") #shorten rownames for display purposes rownames(m) <- paste(substring(rownames(m),1,3),rep("..",nrow(m)), +               substring(rownames(m), nchar(rownames(m))-12,nchar(rownames(m))-4)) #compute distance between document vectors d <- dist(m) Next we run hclust. The algorithm offers several options check out the documentation for details. I use a popular option called Ward's method – there are others, and I suggest you experiment with them  as each of them gives slightly different results making interpretation somewhat tricky (did I mention that clustering is as much an art as a science??). Finally, we visualise the results in a dendogram (see Figure 2 below).  #run hierarchical clustering using Ward's method groups <- hclust(d,method="ward.D") #plot dendogram, use hang to ensure that labels fall below tree plot(groups, hang=-1) Figure 2: Dendogram from hierarchical clustering of corpus Figure 2: Dendogram from hierarchical clustering of corpus  A few words on interpreting dendrograms for hierarchical clusters: as you work your way down the tree in figure 2, each branch point you encounter is the distance at which a cluster merge occurred. Clearly, the most well-defined clusters are those that have the largest separation; many closely spaced branch points indicate a lack of dissimilarity (i.e. distance, in this case) between clusters. Based on this, the figure reveals that there are 2 well-defined clusters – the first one consisting of the three documents at the right end of the cluster and the second containing all other documents. We can display the clusters on the graph using the rect.hclust function like so:  #cut into 2 subtrees – try 3 and 5 rect.hclust(groups,2) The result is shown in the figure below.  Figure 3: 2 cluster solution Figure 3: 2 cluster grouping  The figures 4 and 5 below show the grouping for 3,  and 5 clusters.  Figure 4: 3 cluster solution Figure 4: 3 cluster grouping  Figure 5: 5 cluster solution Figure 5: 5 cluster grouping  I'll make just one point here: the 2 cluster grouping seems the most robust one as it happens at large distance, and is cleanly separated (distance-wise) from the 3 and 5 cluster grouping. That said, I'll leave you to explore the ins and outs of hclust on your own and move on to our next algorithm.  K means clustering In hierarchical clustering we did not specify the number of clusters upfront. These were determined by looking at the dendogram after the algorithm had done its work.  In contrast, our next algorithm – K means –  requires us to define the number of clusters upfront (this number being the "k" in the name). The algorithm then generates k document clusters in a way that ensures the within-cluster distances from each cluster member to the centroid (or geometric mean) of the cluster is minimised.  Here's a simplified description of the algorithm:  Assign the documents randomly to k bins Compute the location of the centroid of each bin. Compute the distance between each document and each centroid Assign each document to the bin corresponding to the centroid closest to it. Stop if no document is moved to a new bin, else go to step 2. An important limitation of the k means method is that the solution found by the algorithm corresponds to a local rather than global minimum (this figure from Wikipedia explains the difference between the two in a nice succinct way). As a consequence it is important to run the algorithm a number of times (each time with a different starting configuration) and then select the result that gives the overall lowest sum of within-cluster distances for all documents.  A simple check that a solution is robust is to run the algorithm for an increasing number of initial configurations until the result does not change significantly. That said, this procedure does not guarantee a globally optimal solution.  I reckon that's enough said about the algorithm, let's get on with it using it. The relevant function, as you might well have guessed is kmeans. As always, I urge you to check the documentation to understand the available options. We'll use the default options for all parameters excepting nstart which we set to 100. We also plot the result using the clusplot function from the cluster library (which you may need to install. Reminder you can install packages via the Tools>Install Packages menu in RStudio)  #k means algorithm, 2 clusters, 100 starting configurations kfit <- kmeans(d, 2, nstart=100) #plot – need library cluster library(cluster) clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0) The plot is shown in Figure 6.  Figure 6: principal component plot (k=2) Figure 6: principal component plot (k=2)  The cluster plot shown in the figure above needs a bit of explanation. As mentioned earlier, the clustering algorithms work in a mathematical space whose dimensionality equals the number of words in the corpus (4131 in our case). Clearly, this is impossible to visualize.  To handle this, mathematicians have invented a dimensionality reduction technique called Principal Component Analysis which reduces the number of dimensions to 2 (in this case) in such a way that the reduced dimensions capture as much of the variability between the clusters as possible (and hence the comment, "these two components explain 69.42% of the point variability" at the bottom of the plot in figure 6)  (Aside  Yes I realize the figures are hard to read because of the overly long names, I leave it to you to fix that. No excuses, you know how…:-))  Running the algorithm and plotting the results for k=3 and 5 yields the figures below.  Figure 7: Principal component plot (k=3) Figure 7: Principal component plot (k=3)  Figure 8: Principal component plot (k=5) Figure 8: Principal component plot (k=5)  Choosing k Recall that the k means algorithm requires us to specify k upfront. A natural question then is: what is the best choice of k? In truth there is no one-size-fits-all answer to this question, but there are some heuristics that might sometimes help guide the choice. For completeness I'll describe one below even though it is not much help in our clustering problem.  In my simplified description of the k means algorithm I mentioned that the technique attempts to minimise the sum of the distances between the points in a cluster and the cluster's centroid. Actually, the quantity that is minimised is the total of the within-cluster sum of squares (WSS) between each point and the mean. Intuitively one might expect this quantity to be maximum when k=1 and then decrease as k increases, sharply at first and then less sharply as k reaches its optimal value.  The problem with this reasoning is that it often happens that the within cluster sum of squares never shows a slowing down in decrease of the summed WSS. Unfortunately this is exactly what happens in the case at hand.  I reckon a picture might help make the above clearer. Below is the R code to draw a plot of summed WSS as a function of k for k=2 all the way to 29 (1-total number of documents):  #kmeans – determine the optimum number of clusters (elbow method) #look for "elbow" in plot of summed intra-cluster distances (withinss) as fn of k wss <- 2:29 for (i in 2:29) wss[i] <- sum(kmeans(d,centers=i,nstart=25)$withinss) plot(2:29, wss[2:29], type="b", xlab="Number of Clusters",ylab="Within groups sum of squares") …and the figure below shows the resulting plot.  Figure 10: WSS as a function of k ("elbow plot")  The plot clearly shows that there is no k for which the summed WSS flattens out (no distinct "elbow").  As a result this method does not help. Fortunately, in this case  one can get a sensible answer using common sense rather than computation:  a choice of 2 clusters seems optimal because both algorithms yield exactly the same clusters and show the clearest cluster separation at this point (review the dendogram and cluster plots for k=2).  The meaning of it all Now I must acknowledge an elephant in the room that I have steadfastly ignored thus far. The odds are good that you've seen it already….  It is this: what topics or themes do the (two) clusters correspond to?  Unfortunately this question does not have a straightforward answer. Although the algorithms suggest a 2-cluster grouping, they are silent on the topics or themes related to these.  Moreover,  as you will see if you experiment, the results of clustering depend on:  The criteria for the construction of the DTM  (see the documentation for DocumentTermMatrix for options). The clustering algorithm itself. Indeed, insofar as clustering is concerned, subject matter and corpus knowledge is the best way to figure out cluster themes. This serves to reinforce (yet again!) that clustering is as much an art as it is a science.  In the case at hand, article length seems to be an important differentiator between the 2 clusters found by both algorithms. The three articles in the smaller cluster are in the top 4 longest pieces in the corpus.  Additionally, the three pieces are related to sensemaking and dialogue mapping. There are probably other factors as well, but none that stand out as being significant. I should mention, however, that the fact that article length seems to play a significant role here suggests that it may be worth checking out the effect of scaling distances by word counts or using other measures such a cosine similarity – but that's a topic for another post! (Note added on Dec 3 2015: check out my article on visualizing relationships between documents using network graphs for a detailed discussion on cosine similarity)  The take home lesson is that  is that the results of clustering are often hard to interpret. This should not be surprising – the algorithms cannot interpret meaning, they simply chug through a mathematical optimisation problem. The onus is on the analyst to figure out what it means…or if it means anything at all.  Conclusion This brings us to the end of a long ramble through clustering.  We've explored the two most common methods:  hierarchical and k means clustering (there are many others available in R, and I urge you to explore them). Apart from providing the detailed steps to do clustering, I have attempted to provide an intuitive explanation of how the algorithms work.  I hope I have succeeded in doing so. As always your feedback would be very welcome.  Finally, I'd like to reiterate an important point:  the results of our clustering exercise do not have a straightforward interpretation, and this is often the case in cluster analysis. Fortunately I can close on an optimistic note. There are other text mining techniques that do a better job in grouping documents based on topics and themes rather than word frequencies alone.  I'll discuss this in the next article in this series.  Until then, I wish you many enjoyable hours exploring the ins and outs of clustering.The standard way to search for documents on the internet is via keywords or keyphrases. This is pretty much what Google and other search engines do routinely…and they do it well.  However, as useful as this is, it has its limitations. Consider, for example, a situation in which you are confronted with a large collection of documents but have no idea what they are about. One of the first things you might want to do is to classify these documents into topics or themes. Among other things this would help you figure out if there's anything interest while also directing you to the relevant subset(s) of the corpus. For small collections, one could do this by simply going through each document but this is clearly infeasible for corpuses containing thousands of documents.  Topic modeling – the theme of this post – deals with the problem of automatically classifying sets of documents into themes  The article is organised as follows: I first provide some background on topic modelling. The algorithm that I use, Latent Dirichlet Allocation (LDA), involves some pretty heavy maths which I'll avoid altogether. However, I will provide an intuitive explanation of how LDA works before moving on to a practical example which uses the topicmodels library in R. As in my previous articles in this series (see this post and this one), I will discuss the steps in detail along with explanations and provide accessible references for concepts that cannot be covered in the space of a blog post.  (Aside: Beware, LDA is also an abbreviation for Linear Discriminant Analysis a classification technique that I hope to cover later in my ongoing series on text and data analytics).  Latent Dirichlet Allocation – a math-free introduction In essence, LDA is a technique that facilitates the automatic discovery of themes in a collection of documents.  The basic assumption behind LDA is that each of the documents in a collection consist of a mixture of collection-wide topics. However, in reality we observe only documents and words, not topics – the latter are part of the hidden (or latent) structure of documents. The aim is to infer the latent topic structure given the words and document.  LDA does this by recreating the documents in the corpus by adjusting the relative importance of topics in documents and words in topics iteratively.  Here's a brief explanation of how the algorithm works, quoted directly from this answer by Edwin Chen on Quora:  Go through each document, and randomly assign each word in the document to one of the K topics. (Note: One of the shortcomings of LDA is that one has to specify the number of topics, denoted by K, upfront. More about this later.) This assignment already gives you both topic representations of all the documents and word distributions of all the topics (albeit not very good ones). So to improve on them, for each document d… ….Go through each word w in d… ……..And for each topic t, compute two things: 1) p(topic t | document d) = the proportion of words in document d that are currently assigned to topic t, and 2) p(word w | topic t) = the proportion of assignments to topic t over all documents that come from this word w. Reassign w a new topic, where you choose topic t with probability p(topic t | document d) * p(word w | topic t) (according to our generative model, this is essentially the probability that topic t generated word w, so it makes sense that we resample the current word's topic with this probability).  (Note: p(a|b) is the conditional probability of a given that b has already occurred – see this post for more on conditional probabilities) ……..In other words, in this step, we're assuming that all topic assignments except for the current word in question are correct, and then updating the assignment of the current word using our model of how documents are generated. After repeating the previous step a large number of times, you'll eventually reach a roughly steady state where your assignments are pretty good. So use these assignments to estimate the topic mixtures of each document (by counting the proportion of words assigned to each topic within that document) and the words associated to each topic (by counting the proportion of words assigned to each topic overall). For another simple explanation of how LDA works in, check out  this article by Matthew Jockers. For a more technical exposition, take a look at this video by David Blei, one of the inventors of the algorithm.  The iterative process described in the last point above is implemented using a technique called Gibbs sampling.  I'll say a bit more about Gibbs sampling later, but you may want to have a look at this paper by Philip Resnick and Eric Hardesty that explains the nitty-gritty of the algorithm (Warning: it involves a fair bit of math, but has some good intuitive explanations as  well).  As a general point, I should also emphasise that you do not need to understand the ins and outs of an algorithm to use it but it does help to understand, at least at a high level, what the algorithm is doing. One needs to develop a feel for algorithms even if one doesn't understand the details. Indeed, most people working in analytics do not know the details of the algorithms they use, but that doesn't stop them from using algorithms intelligently. Purists may disagree. I think they are wrong.  Finally – because you're no doubt wondering – the term "Dirichlet" in LDA refers to the fact that topics and words are assumed to follow Dirichlet distributions. There is no "good" reason for this apart from convenience – Dirichlet distributions provide good approximations to word distributions in documents and, perhaps more important, are computationally convenient.  Preprocessing As in my previous articles on text mining, I will use a collection of 30 posts from this blog as an example corpus. The corpus can be downloaded here. I will assume that you have R and RStudio installed. Follow this link if you need help with that.  The preprocessing steps are much the same as described in my previous articles.  Nevertheless, I'll risk boring you with a detailed listing so that you can reproduce my results yourself: #load text mining library library(tm) #set working directory (modify path as needed) setwd("C:\\Users\\Kailash\\Documents\\TextMining") #load files into corpus #get listing of .txt files in directory filenames <- list.files(getwd(),pattern="*.txt") #read files into a character vector files <- lapply(filenames,readLines) #create corpus from vector docs <- Corpus(VectorSource(files)) #inspect a particular document in corpus writeLines(as.character(docs[[30]])) #start preprocessing #Transform to lower case docs <-tm_map(docs,content_transformer(tolower)) #remove potentially problematic symbols toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))}) docs <- tm_map(docs, toSpace, "-") docs <- tm_map(docs, toSpace, "'") docs <- tm_map(docs, toSpace, "‘") docs <- tm_map(docs, toSpace, "•") docs <- tm_map(docs, toSpace, """) docs <- tm_map(docs, toSpace, """) #remove punctuation docs <- tm_map(docs, removePunctuation) #Strip digits docs <- tm_map(docs, removeNumbers) #remove stopwords docs <- tm_map(docs, removeWords, stopwords("english")) #remove whitespace docs <- tm_map(docs, stripWhitespace) #Good practice to check every now and then writeLines(as.character(docs[[30]])) #Stem document docs <- tm_map(docs,stemDocument) #fix up 1) differences between us and aussie english 2) general errors docs <- tm_map(docs, content_transformer(gsub), pattern = "organiz", replacement = "organ") docs <- tm_map(docs, content_transformer(gsub), pattern = "organis", replacement = "organ") docs <- tm_map(docs, content_transformer(gsub), pattern = "andgovern", replacement = "govern") docs <- tm_map(docs, content_transformer(gsub), pattern = "inenterpris", replacement = "enterpris") docs <- tm_map(docs, content_transformer(gsub), pattern = "team-", replacement = "team") #define and eliminate all custom stopwords myStopwords <- c("can", "say","one","way","use", "also","howev","tell","will", "much","need","take","tend","even", "like","particular","rather","said", "get","well","make","ask","come","end", "first","two","help","often","may", "might","see","someth","thing","point", "post","look","right","now","think","‘ve ", "‘re ","anoth","put","set","new","good", "want","sure","kind","larg","yes,","day","etc", "quit","sinc","attempt","lack","seen","awar", "littl","ever","moreov","though","found","abl", "enough","far","earli","away","achiev","draw", "last","never","brief","bit","entir","brief", "great","lot") docs <- tm_map(docs, removeWords, myStopwords) #inspect a document as a check writeLines(as.character(docs[[30]])) #Create document-term matrix dtm <- DocumentTermMatrix(docs) #convert rownames to filenames rownames(dtm) <- filenames #collapse matrix by summing over columns freq <- colSums(as.matrix(dtm)) #length should be total number of terms length(freq) #create sort order (descending) ord <- order(freq,decreasing=TRUE) #List all terms in decreasing order of freq and write to disk freq[ord] write.csv(freq[ord],"word_freq.csv") Check out the  preprocessing section in either this article or this one for detailed explanations of the code. The document term matrix (DTM) produced by the above code will be the main input into the LDA algorithm of the next section.  Topic modelling using LDA We are now ready to do some topic modelling. We'll use the topicmodels package written by Bettina Gruen and Kurt Hornik. Specifically, we'll use the LDA function with the Gibbs sampling option mentioned earlier, and I'll say  more about it in a second. The LDA function has a fairly large number of parameters. I'll describe these briefly below. For more, please check out this vignette by Gruen and Hornik.  For the most part, we'll use the default parameter values supplied by the LDA function,custom setting only the parameters that are required by the Gibbs sampling algorithm.  Gibbs sampling works by performing a random walk in such a way that reflects the characteristics of a desired distribution. Because the starting point of the walk is chosen at random, it is necessary to discard the first few steps of the walk (as these do not correctly reflect the properties of distribution). This is referred to as the burn-in period. We set the burn-in parameter to  4000. Following the burn-in period, we perform 2000 iterations, taking every 500th  iteration for further use.  The reason we do this is to avoid correlations between samples. We use 5 different starting points (nstart=5) – that is, five independent runs. Each starting point requires a seed integer (this also ensures reproducibility),  so I have provided 5 random integers in my seed list. Finally I've set best to TRUE (actually a default setting), which instructs the algorithm to return results of the run with the highest posterior probability.  Some words of caution are in order here. It should be emphasised that the settings above do not guarantee  the convergence of the algorithm to a globally optimal solution. Indeed, Gibbs sampling will, at best, find only a locally optimal solution, and even this is hard to prove mathematically in specific practical problems such as the one we are dealing with here. The upshot of this is that it is best to do lots of runs with different settings of parameters to check the stability of your results. The bottom line is that our interest is purely practical so it is good enough if the results make sense. We'll leave issues  of mathematical rigour to those better qualified to deal with them As mentioned earlier,  there is an important parameter that must be specified upfront: k, the number of topics that the algorithm should use to classify documents. There are mathematical approaches to this, but they often do not yield semantically meaningful choices of k (see this post on stackoverflow for an example). From a practical point of view, one can simply run the algorithm for different values of k and make a choice based by inspecting the results. This is what we'll do.  OK, so the first step is to set these parameters in R… and while we're at it, let's also load the topicmodels library (Note: you might need to install this package as it is not a part of the base R installation).  #load topic models library library(topicmodels) #Set parameters for Gibbs sampling burnin <- 4000 iter <- 2000 thin <- 500 seed <-list(2003,5,63,100001,765) nstart <- 5 best <- TRUE #Number of topics k <- 5 That done, we can now do the actual work – run the topic modelling algorithm on our corpus. Here is the code:  #Run LDA using Gibbs sampling ldaOut <-LDA(dtm,k, method="Gibbs", control=list(nstart=nstart, seed = seed, best=best, burnin = burnin, iter = iter, thin=thin)) #write out results #docs to topics ldaOut.topics <- as.matrix(topics(ldaOut)) write.csv(ldaOut.topics,file=paste("LDAGibbs",k,"DocsToTopics.csv")) #top 6 terms in each topic ldaOut.terms <- as.matrix(terms(ldaOut,6)) write.csv(ldaOut.terms,file=paste("LDAGibbs",k,"TopicsToTerms.csv")) #probabilities associated with each topic assignment topicProbabilities <- as.data.frame(ldaOut@gamma) write.csv(topicProbabilities,file=paste("LDAGibbs",k,"TopicProbabilities.csv")) #Find relative importance of top 2 topics topic1ToTopic2 <- lapply(1:nrow(dtm),function(x) sort(topicProbabilities[x,])[k]/sort(topicProbabilities[x,])[k-1]) #Find relative importance of second and third most important topics topic2ToTopic3 <- lapply(1:nrow(dtm),function(x) sort(topicProbabilities[x,])[k-1]/sort(topicProbabilities[x,])[k-2]) #write to file write.csv(topic1ToTopic2,file=paste("LDAGibbs",k,"Topic1ToTopic2.csv")) write.csv(topic2ToTopic3,file=paste("LDAGibbs",k,"Topic2ToTopic3.csv")) The LDA algorithm returns an object that contains a lot of information. Of particular interest to us are the document to topic assignments, the top terms in each topic and the probabilities associated with each of those terms. These are printed out in the first three calls to write.csv above. There are a few important points to note here:  Each document is considered to be a mixture of all topics (5 in this case). The assignments in the first file list the top topic – that is, the one with the highest probability (more about this in point 3 below). Each topic contains all terms (words) in the corpus, albeit with different probabilities. We list only the top  6 terms in the second file. The last file lists the probabilities with  which each topic is assigned to a document. This is therefore a 30 x 5 matrix – 30 docs and 5 topics. As one might expect, the highest probability in each row corresponds to the topic assigned to that document.  The "goodness" of the primary assignment (as discussed in point 1) can be assessed by taking the ratio of the highest to second-highest probability and the second-highest to the third-highest probability and so on. This is what I've done in the last nine lines of the code above. Take some time to examine the output and confirm for yourself that that the primary topic assignments are best when the ratios of probabilities discussed in point 3 are highest. You should also experiment with different values of k to see if you can find better topic distributions. In the interests of space I will restrict myself to k = 5.  The table below lists the top 6 terms in topics 1 through 5.  Topic 1	Topic 2	Topic 3	Topic 4	Topic 5 1	work	question	chang	system	project 2	practic	map	organ	data	manag 3	mani	time	consult	model	approach 4	flexibl	ibi	manag	design	organ 5	differ	issu	work	process	decis 6	best	plan	problem	busi	problem The table below lists the document to (primary) topic assignments: Document	Topic BeyondEntitiesAndRelationships.txt	4 bigdata.txt	4 ConditionsOverCauses.txt	5 EmergentDesignInEnterpriseIT.txt	4 FromInformationToKnowledge.txt	2 FromTheCoalface.txt	1 HeraclitusAndParmenides.txt	3 IroniesOfEnterpriseIT.txt	3 MakingSenseOfOrganizationalChange.txt	5 MakingSenseOfSensemaking.txt	2 ObjectivityAndTheEthicalDimensionOfDecisionMaking.txt	5 OnTheInherentAmbiguitiesOfManagingProjects.txt	5 OrganisationalSurprise.txt	5 ProfessionalsOrPoliticians.txt	3 RitualsInInformationSystemDesign.txt	4 RoutinesAndReality.txt	4 ScapegoatsAndSystems.txt	5 SherlockHolmesFailedProjects.txt	3 sherlockHolmesMgmtFetis.txt	3 SixHeresiesForBI.txt	4 SixHeresiesForEnterpriseArchitecture.txt	3 TheArchitectAndTheApparition.txt	3 TheCloudAndTheGrass.txt	2 TheConsultantsDilemma.txt	3 TheDangerWithin.txt	5 TheDilemmasOfEnterpriseIT.txt	3 TheEssenceOfEntrepreneurship.txt	1 ThreeTypesOfUncertainty.txt	5 TOGAFOrNotTOGAF.txt	3 UnderstandingFlexibility.txt	1 From a quick perusal of the two tables it appears that the algorithm has done a pretty decent job. For example,topic 4 is about data and system design, and the documents assigned to it are on topic. However, it is far from perfect – for example, the interview I did with Neil Preston on organisational change (MakingSenseOfOrganizationalChange.txt) has been assigned to topic 5, which seems to be about project management. It ought to be associated with Topic 3, which is about change. Let's see if we can resolve this by looking at probabilities associated with topics.  The table below lists the topic probabilities by document:  Topic 1	Topic 2	Topic 3	Topic 4	Topic 5 BeyondEn	0.071	0.064	0.024	0.741	0.1 bigdata.	0.182	0.221	0.182	0.26	0.156 Conditio	0.144	0.109	0.048	0.205	0.494 Emergent	0.121	0.226	0.204	0.236	0.213 FromInfo	0.096	0.643	0.026	0.169	0.066 FromTheC	0.636	0.082	0.058	0.086	0.138 Heraclit	0.137	0.091	0.503	0.162	0.107 IroniesO	0.101	0.088	0.388	0.26	0.162 MakingSe	0.13	0.206	0.262	0.089	0.313 MakingSe	0.09	0.715	0.055	0.067	0.074 Objectiv	0.216	0.078	0.086	0.242	0.378 OnTheInh	0.18	0.234	0.102	0.12	0.364 Organisa	0.089	0.095	0.07	0.092	0.655 Professi	0.155	0.064	0.509	0.128	0.144 RitualsI	0.103	0.064	0.044	0.676	0.112 Routines	0.108	0.042	0.033	0.69	0.127 Scapegoa	0.135	0.088	0.043	0.185	0.549 Sherlock	0.093	0.082	0.398	0.195	0.232 sherlock	0.108	0.136	0.453	0.123	0.18 SixHeres	0.159	0.11	0.078	0.516	0.138 SixHeres	0.104	0.111	0.366	0.212	0.207 TheArchi	0.111	0.221	0.522	0.088	0.058 TheCloud	0.185	0.333	0.198	0.136	0.148 TheConsu	0.105	0.184	0.518	0.096	0.096 TheDange	0.114	0.079	0.037	0.079	0.69 TheDilem	0.125	0.128	0.389	0.261	0.098 TheEssen	0.713	0.059	0.031	0.113	0.084 ThreeTyp	0.09	0.076	0.042	0.083	0.708 TOGAFOrN	0.158	0.232	0.352	0.151	0.107 Understa	0.658	0.065	0.072	0.101	0.105 In the table, the highest probability in each row is in bold. Also, in cases where the maximum and the second/third largest probabilities are close, I have highlighted the second (and third) highest probabilities in red. It is clear that Neil's interview (9th document in the above table) has 3  topics with comparable probabilities – topic 5 (project management), topic 3 (change) and topic 2 (issue mapping / ibis), in decreasing order of probabilities. In general, if a document has multiple topics with comparable probabilities, it simply means that the document speaks to all those topics in proportions indicated by the probabilities. A reading of Neil's interview will convince you that our conversation did indeed range over all those topics.  That said, the algorithm is far from perfect. You might have already noticed a few poor assignments. Here is one – my post on Sherlock Holmes and the case of the failed project has been assigned to topic 3; I reckon it belongs in topic 5. There are a number of others, but I won't belabor the point, except to reiterate that this precisely why you definitely want to experiment with different settings of the iteration parameters (to check for stability) and, more important, try a range of different values of k to find the optimal number of topics.  To conclude Topic modelling provides a quick and convenient way to perform unsupervised classification of a corpus of documents.  As always, though, one needs to examine the results carefully to check that they make sense.  I'd like to end with a general observation. Classifying documents is an age-old concern that cuts across disciplines. So it is no surprise that topic modelling has got a look-in from diverse communities. Indeed, when I was reading up and learning about LDA, I found that some of the best introductory articles in the area have been written by academics working in English departments! This is one of the things I love about working in text analysis, there is a wealth of material on the web written from diverse perspectives. The term cross-disciplinary often tends to be a platitude , but in this case it is simply a statement of fact.  I hope that I have been able to convince you to explore this rapidly evolving field. Exciting times ahead, come join the fun.Graph theory is the an area of mathematics that analyses relationships between pairs of objects. Typically graphs consist of nodes (points representing objects) and edges (lines depicting relationships between objects). As one might imagine, graphs are extremely useful in visualizing relationships between objects. In this post, I provide a detailed introduction to network graphs using  R, the premier open source tool statistics package for calculations and the excellent Gephi software for visualization.  The article is organised as follows: I begin by defining the problem and then spend some time developing the concepts used in constructing the graph  Following this,  I do the data preparation in R  and then finally build the network graph using Gephi.  The problem In an introductory article on cluster analysis, I provided an in-depth introduction to a couple of algorithms that can be used to categorise documents automatically.  Although these techniques are useful, they do not provide a feel for the relationships between different documents in the collection of interest.  In the present piece I show network graphs can be used to to visualise similarity-based relationships within a corpus.  Document similarity There are many ways to quantify similarity between documents. A popular method is to use the notion of distance between documents. The basic idea is simple: documents that have many words in common are "closer" to each other than those that share fewer words. The problem with distance, however, is that it can be skewed by word count: documents that have an unusually high word  count will show up as outliers even though they may be similar (in terms of words used) to other documents in the corpus. For this reason, we will use another related measure of similarity that does not suffer from this problem – more about this in a minute.  Representing documents mathematically  As I explained in my article on cluster analysis, a document can be represented as a point in a conceptual space that has dimensionality equal to the number of distinct words in the collection of documents. I revisit and build on that explanation below.  Say one has a simple document consisting of the words "five plus six", one can represent it mathematically in a 3 dimensional space in which the individual words are represented by the three axis (See Figure 1). Here each word is a coordinate axis (or dimension).  Now, if one connects the point representing the document (point A in the figure) to the origin of the word-space, one has a vector, which in this case is a directed line connecting the point in question to the origin.  Specifically, the point A can be represented by the coordinates (1, 1, 1) in this space. This is a nice quantitative representation of the fact that the words five, plus and one appear in the document exactly once. Note, however, that we've assumed the order of words does not matter. This is a reasonable assumption in some cases, but not always so.  Figure 1 Figure 1  As another example consider document, B, which consists of only two words: "five plus" (see Fig 2). Clearly this document shares some similarity with document but it is not identical.  Indeed, this becomes evident when we note that document (or point) B is simply the point $latex(1, 1, 0)$ in this space, which tells us that it has two coordinates (words/frequencies) in common with document (or point) A.  Figure 2 Figure 2  To be sure, in a realistic collection of documents we would have a large number of distinct words, so we'd have to work in a very high dimensional space. Nevertheless, the same principle holds: every document in the corpus can be represented as a vector consisting of a directed line from the origin to the point to which the document corresponds.  Cosine similarity  Now it is easy to see that two documents are identical if they correspond to the same point. In other words, if their vectors coincide. On the other hand, if they are completely dissimilar (no words in common), their vectors will be at right angles to each other.  What we need, therefore, is a quantity that varies from 0 to 1 depending on whether two documents (vectors) are dissimilar(at right angles to each other) or similar (coincide, or are parallel to each other).  Now here's the ultra-cool thing, from your high school maths class, you know there is a trigonometric ratio which has exactly this property – the cosine!  What's even cooler is that the cosine of the angle between two vectors is simply the dot product  of the two vectors, which is sum of the products of the individual elements of the vector,  divided by the product of the  lengths of the two vectors. In three dimensions this can be expressed mathematically as:  \cos(\theta)= \displaystyle \frac{x_1 x_2+y_1 y_2+z_1 z_2}{\sqrt{x_1^2+y_1^2+z_1^2}\sqrt{x_2^2+y_2^2+z_2^2}}...(1)  where the two vectors are (x_{1},y_{1},z_{1})  and (x_{2},y_{2},z_{2}) , and \theta is the angle between the two vectors (see Fig 2).  The upshot of the above is that the cosine of the angle between the vector representation of two documents is a reasonable measure of similarity between them. This quantity, sometimes referred to as cosine similarity, is what we'll take as our similarity measure in the rest of this article.  The adjacency matrix If we have a collection of N documents, we can calculate the similarity between every pair of documents as we did for A and B in the previous section. This would give us a set of N^2 numbers between 0 and 1, which can be conveniently represented as a matrix.  This is sometimes called the adjacency matrix. Beware, though, this term has many different meanings in the math literature. I use it in the sense specified above.  Since every document is identical to itself, the diagonal elements of the matrix will all be 1. These similarities are trivial (we know that every document is identical to itself!)  so we'll set the diagonal elements to zero.  Another important practical point is that visualizing every relationship is going to make  a very messy graph. There would be N(N-1) edges in such a graph, which would make it impossible to make sense of if we have more than a handful of documents. For this reason, it is normal practice to choose a cutoff value of similarity below which it is set to zero.  Building the adjacency matrix using R We now have enough background to get down to the main point of this article – visualizing relationships between documents.  The first step is to build the adjacency matrix.  In order to do this, we have to build the document term matrix (DTM) for the collection of documents,  a process which I have dealt with at length in my  introductory pieces on text mining and topic modeling. In fact, the steps are actually identical to those detailed in the second piece. I will therefore avoid lengthy explanations here. However,  I've listed all the code below with brief comments (for those who are interested in trying this out, the document corpus can be downloaded here and a pdf listing of the R code can be obtained here.)  OK, so here's the code listing:  #load text mining library library(tm) #set working directory (modify path as needed) setwd("C:\\Users\\Kailash\\Documents\\TextMining") #load files into corpus #get listing of .txt files in directory filenames <- list.files(getwd(),pattern="*.txt") #read files into a character vector files <- lapply(filenames,readLines) #create corpus from vector docs <- Corpus(VectorSource(files)) #inspect a particular document in corpus writeLines(as.character(docs[[30]])) #start preprocessing #Transform to lower case docs <-tm_map(docs,content_transformer(tolower)) #remove potentially problematic symbols toSpace <- content_transformer(function(x, pattern) { return (gsub(pattern, " ", x))}) docs <- tm_map(docs, toSpace, "-") docs <- tm_map(docs, toSpace, "'") docs <- tm_map(docs, toSpace, "‘") docs <- tm_map(docs, toSpace, "•") docs <- tm_map(docs, toSpace, """) docs <- tm_map(docs, toSpace, """) #remove punctuation docs <- tm_map(docs, removePunctuation) #Strip digits docs <- tm_map(docs, removeNumbers) #remove stopwords docs <- tm_map(docs, removeWords, stopwords("english")) #remove whitespace docs <- tm_map(docs, stripWhitespace) #Good practice to check every now and then writeLines(as.character(docs[[30]])) #Stem document docs <- tm_map(docs,stemDocument) #fix up 1) differences between us and aussie english 2) general errors docs <- tm_map(docs, content_transformer(gsub), pattern = "organiz", replacement = "organ") docs <- tm_map(docs, content_transformer(gsub), pattern = "organis", replacement = "organ") docs <- tm_map(docs, content_transformer(gsub), pattern = "andgovern", replacement = "govern") docs <- tm_map(docs, content_transformer(gsub), pattern = "inenterpris", replacement = "enterpris") docs <- tm_map(docs, content_transformer(gsub), pattern = "team-", replacement = "team") #define and eliminate all custom stopwords myStopwords <- c("can", "say","one","way","use", "also","howev","tell","will", "much","need","take","tend","even", "like","particular","rather","said", "get","well","make","ask","come","end", "first","two","help","often","may", "might","see","someth","thing","point", "post","look","right","now","think","‘ve ", "‘re ","anoth","put","set","new","good", "want","sure","kind","larg","yes,","day","etc", "quit","sinc","attempt","lack","seen","awar", "littl","ever","moreov","though","found","abl", "enough","far","earli","away","achiev","draw", "last","never","brief","bit","entir","brief", "great","lot") docs <- tm_map(docs, removeWords, myStopwords) #inspect a document as a check writeLines(as.character(docs[[30]])) #Create document-term matrix dtm <- DocumentTermMatrix(docs) The  rows of a DTM are document vectors akin to the vector representations of documents A and B discussed earlier. The DTM therefore contains all the information we need to calculate the cosine similarity between every pair of documents in the corpus (via equation 1). The R code below implements this, after taking care of a few preliminaries.  #convert dtm to matrix m<-as.matrix(dtm) #write as csv file write.csv(m,file="dtmEight2Late.csv") #Map filenames to matrix row numbers #these numbers will be used to reference #files in the network graph filekey <- cbind(rownames(m),filenames) write.csv(filekey,"filekey.csv") #compute cosine similarity between document vectors #converting to distance matrix sets diagonal elements to 0 cosineSim <- function(x){ as.dist(x%*%t(x)/(sqrt(rowSums(x^2) %*% t(rowSums(x^2))))) } cs <- cosineSim(m) write.csv(as.matrix(cs),file="csEight2Late.csv") #adjacency matrix: set entries below a certain threshold to 0. #We choose half the magnitude of the largest element of the matrix #as the cutoff. This is an arbitrary choice cs[cs < max(cs)/2] <- 0 cs <- round(cs,3) write.csv(as.matrix(cs),file="AdjacencyMatrix.csv") A few lines need a brief explanation:  First up, although the DTM is a matrix, it is internally stored in a special form suitable for sparse matrices. We therefore have to explicitly convert it into a proper matrix before using it to calculate similarity.  Second, the names I have given the documents are way too long to use as labels in the network diagram. I have therefore mapped the document names to the row numbers which we'll use in our network graph later. The mapping back to the original document names is stored in filekey.csv. For future reference, the mapping is shown in Table 1 below.  File number	Name 1	BeyondEntitiesAndRelationships.txt 2	bigdata.txt 3	ConditionsOverCauses.txt 4	EmergentDesignInEnterpriseIT.txt 5	FromInformationToKnowledge.txt 6	FromTheCoalface.txt 7	HeraclitusAndParmenides.txt 8	IroniesOfEnterpriseIT.txt 9	MakingSenseOfOrganizationalChange.txt 10	MakingSenseOfSensemaking.txt 11	ObjectivityAndTheEthicalDimensionOfDecisionMaking.txt 12	OnTheInherentAmbiguitiesOfManagingProjects.txt 13	OrganisationalSurprise.txt 14	ProfessionalsOrPoliticians.txt 15	RitualsInInformationSystemDesign.txt 16	RoutinesAndReality.txt 17	ScapegoatsAndSystems.txt 18	SherlockHolmesFailedProjects.txt 19	sherlockHolmesMgmtFetis.txt 20	SixHeresiesForBI.txt 21	SixHeresiesForEnterpriseArchitecture.txt 22	TheArchitectAndTheApparition.txt 23	TheCloudAndTheGrass.txt 24	TheConsultantsDilemma.txt 25	TheDangerWithin.txt 26	TheDilemmasOfEnterpriseIT.txt 27	TheEssenceOfEntrepreneurship.txt 28	ThreeTypesOfUncertainty.txt 29	TOGAFOrNotTOGAF.txt 30	UnderstandingFlexibility.txt Table 1: File mappings  Finally, the distance function (as.dist) in the cosine similarity function sets the diagonal elements to zero  because the distance between a document and itself is zero…which is just a complicated way of saying that a document is identical to itself 🙂  The last three lines of code above simply implement the cutoff that I mentioned in the previous section. The comments explain the details so I need say no more about it.  …which finally brings us to Gephi.  Visualizing document similarity using Gephi Gephi is an open source, Java based network analysis and visualisation tool. Before going any further, you may want to download and install it. While you're at it you may also want to download this excellent quick start tutorial.  Go on, I'll wait for you…  To begin with, there's a little formatting quirk that we need to deal with. Gephi expects separators in csv files to be semicolons (;) . So, your first step is to open up the adjacency matrix that you created in the previous section (AdjacencyMatrix.csv) in a text editor and replace commas with semicolons.  Once you've done that, fire up Gephi, go to File > Open,  navigate to where your Adjacency matrix is stored and load the file. If it loads successfully, you should see a feedback panel as shown in Figure 3.  By default Gephi creates a directed graph (i.e one in which the edges have arrows pointing from one node to another). Change this to undirected and click OK.  Figure 3: Gephi import feedback Figure 3: Gephi import feedback  Once that is done, click on overview (top left of the screen). You should end up with something like Figure 4.  Figure 4: Initial overview after loading adjacency matrix Figure 4: Initial overview after loading adjacency matrix  Gephi has sketched out an initial network diagram which depicts the relationships between documents…but it needs a bit of work to make it look nicer and more informative. The quickstart tutorial mentioned earlier describes various features that can be used to manipulate and prettify the graph. In the remainder of this section, I list some that I found useful. Gephi offers many more. Do explore, there's much more than  I can cover in an introductory post.  First some basics. You can:  Zoom and pan using mouse wheel and right button. Adjust edge thicknesses using the slider next to text formatting options on bottom left of main panel. Re-center graph via the magnifying glass icon on left of display panel (just above size adjuster). Toggle node labels on/off by clicking on grey T symbol on bottom left panel. Figure 5 shows the state of the diagram after labels have been added and edge thickness adjusted (note that your graph may vary in appearance).  Figure 5: graph with node labels and adjusted edge thicknesses Figure 5: graph with node labels and adjusted edge thicknesses  The default layout of the graph is ugly and hard to interpret. Let's work on fixing it up. To do this, go over to the layout panel on the left. Experiment with different layouts to see what they do. After some messing around, I found the Fruchtermann-Reingold and Force Atlas options to be good for this graph. In the end I used Force Atlas with a Repulsion Strength of 2000 (up from the default of 200) and an Attraction Strength of 1 (down from the default of 10). I also adjusted the figure size and node label font size from the graph panel in the center. The result is shown in Figure 6.  Figure 6: Graph after using Force Atlas layout Figure 6: Graph after using Force Atlas layout  This is much better. For example, it is now evident that document 9 is the most connected one (which table 9 tells us is a transcript of a conversation with Neil Preston on organisational change).  It would be nice if we could colour code edges/nodes and size nodes by their degree of connectivity. This can be done via the ranking panel above the layout area where you've just been working.  In the Nodes tab select Degree as  the rank parameter (this is the degree of connectivity of the node) and hit apply. Select your preferred colours via the small icon just above the colour slider. Use the colour slider to adjust the degree of connectivity at which colour transitions occur.  Do the same for edges, selecting weight as the rank parameter(this is the degree of similarity between the two douments connected by the edge). With a bit of playing around, I got the graph shown in the screenshot below (Figure 7).  Figure 7: Connectivity-based colouring of edges and nodes. Figure 5: Connectivity-based colouring of edges and nodes.  If you want to see numerical values for the rankings, hit the results list icon on the bottom left of the ranking panel. You can see numerical ranking values for both nodes and edges as shown in Figures 8 and 9.  Figure 8: Node ranking Figure 8: Node ranking (see left of figure)  Figure 9: Edge ranking Figure 9: Edge ranking  It is easy to see from the figure that documents 21 and 29 are the most similar in terms of cosine ranking. This makes sense, they are pieces in which I have ranted about the current state of enterprise architecture – the first article is about EA in general and the other about the TOGAF framework. If you have a quick skim through, you'll see that they have a fair bit in common.  Finally, it would be nice if we could adjust node size to reflect the connectedness of the associated document. You can do this via the "gem" symbol on the top right of the ranking panel. Select appropriate min and max sizes (I chose defaults) and hit apply. The node size is now reflective of the connectivity of the node – i.e. the number of other documents to which it is cosine similar to varying degrees. The thickness of the edges reflect the degree of similarity. See Figure 10.  Figure 10: Node sizes reflecting connectedness Figure 10: Node sizes reflecting connectedness  Now that looks good enough to export. To do this, hit the preview tab on main panel and make following adjustments to the default settings:  Under Node Labels: 1. Check Show Labels 2. Uncheck proportional size 3. Adjust font to required size  Under Edges: 1. Change thickness to 10 2. Check rescale weight  Hit refresh after making the above adjustments. You should get something like Fig 11.  Figure 11: Export preview Figure 11: Export preview  All that remains now is to do the deed: hit export SVG/PDF/PNG to export the diagram. My output is displayed in Figure 12. It clearly shows the relationships between the different documents (nodes) in the corpus. The nodes with the highest connectivity are indicated via node size and colour  (purple for high, green for low) and strength of similarity is indicated by edge thickness.  Figure 12: Gephi network graph Figure 12: Gephi network graph of document corpus  …which brings us to the end of this journey.  Wrapping up The techniques of text analysis enable us to quantify relationships between documents. Document similarity is one such relationship. Numerical measures are good, but the comprehensibility of these can be further enhanced through meaningful visualisations.  Indeed, although my stated objective in this article was to provide an introduction to creating network graphs using Gephi and R (which I hope I've succeeded in doing), a secondary aim was to show how document similarity can be quantified and visualised. I sincerely hope you've found the discussion interesting and useful.  Many thanks for reading! As always, your feedback would be greatly appreciated.The story of sociotechnical systems began a little over half a century ago, in a somewhat unlikely setting: the coalfields of Yorkshire.  The British coal industry had just been nationalised and new mechanised mining methods were being introduced in the mines. It was thought that nationalisation would sort out the chronic labour-management issues and mechanisation would address the issue of falling productivity.  But things weren't going as planned. In the words of Eric Trist, one of the founders of the Tavistock Institute:  …the newly nationalized industry was not doing well. Productivity failed to increase in step with increases in mechanization. Men were leaving the mines in large numbers for more attractive opportunities in the factory world. Among those who remained, absenteeism averaged 20%. Labour disputes were frequent despite improved conditions of employment.  – excerpted from, The evolution of Socio-technical systems – a conceptual framework and an action research program, E. Trist (1980)  Trist and his colleagues were asked by the National Coal Board to come in and help. To this end, they did a comparative study of two mines that were similar except that one had high productivity and morale whereas the other suffered from low performance and had major labour issues.  Their job was far from easy: they were not welcome at the coalface because workers associated them with management and the Board.  Trist recounts that around the time the study started, there were a number of postgraduate fellows at the Tavistock Institute. One of them, Ken Bamforth, knew the coal industry well as he had been a miner himself.  Postgraduate fellows who had worked in the mines were encouraged to visit their old workplaces after  a year and  write up their impressions, focusing on things that had changed since they had worked there.  After one such visit, Bamforth reported back with news of a workplace innovation that had occurred at a newly opened seam at Haighmoor. Among other things, morale and productivity at this particular seam was high compared to other similar ones.  The team's way of working was entirely novel, a world away from the hierarchically organised set up that was standard in most mechanised mines at the time. In Trist's words:  The work organization of the new seam was, to us, a novel phenomenon consisting of a set of relatively autonomous groups interchanging roles and shifts and regulating their affairs with a minimum of supervision. Cooperation between task groups was everywhere in evidence; personal commitment was obvious, absenteeism low, accidents infrequent, productivity high. The contrast was large between the atmosphere and arrangements on these faces and those in the conventional areas of the pit, where the negative features characteristic of the industry were glaringly apparent. Excerpted from the paper referenced above.  To appreciate the radical nature of practices at this seam, one needs to understand the backdrop against which they occurred. To this end, it is helpful to compare the  mechanised work practices introduced in the post-war years with the older ones from the pre-mechanised era of mining.  In the days before mines were mechanised, miners would typically organise themselves into workgroups of six miners, who would cover three work shifts in teams of two. Each miner was able to do pretty much any job at the seam and so could pick up where his work-mates from the previous shift had left off. This was necessary in order to ensure continuity of work between shifts. The group negotiated the price of their mined coal directly with management and the amount received was shared equally amongst all members of the group.  This mode of working required strong cooperation and trust within the group, of course.  However, as workgroups were reorganised from time to time due to attrition or other reasons, individual miners understood the importance of maintaining their individual reputations as reliable and trustworthy workmates. It was important to get into a good workgroup because such groups were more likely to get more productive seams to work on. Seams were assigned by bargaining, which was typically the job of the senior miner on the group. There was considerable competition for the best seams, but this was generally kept within bounds of civility via informal rules and rituals.  This traditional way of working could not survive mechanisation. For one, mechanised mines encouraged specialisation because they were organised like assembly lines, with clearly defined job roles each with different responsibilities and pay scales. Moreover, workers in a shift would perform only part of the extraction process leaving those from subsequent shifts to continue where work was left off.  As miners were paid by the job they did rather than the amount of coal they produced, no single group had end-to-end responsibility for the product.  Delays due to unexpected events tended to get compounded as no one felt the need to make up time. As a result, it would often happen that work that was planned for a shift would not be completed. This meant that the next shift (which could well be composed of a group with completely different skills) could not or would not start their work because they did not see it as their job to finish the work of the earlier shift. Unsurprisingly, blame shifting and scapegoating was rife.  From a supervisor's point of view, it was difficult to maintain the same level of oversight and control in underground mining work as was possible in an assembly line. The environment underground is simply not conducive to close supervision and is also more uncertain in that it is prone to unexpected events.  Bureaucratic organisational structures are completely unsuited to dealing with these because decision-makers are too far removed from the coalface (literally!).  This is perhaps the most important insight to come out of the Tavistock coal mining studies.  As Claudio Ciborra  puts it in his classic book on teams:  Since the production process at any seam was much more prone to disorganisation than due to uncertainty and complexity of underground conditions, any ‘bureaucratic' allocation of jobs could be easily disrupted. Coping with emergencies and coping with coping became part of worker's and supervisors' everyday activities. These activities would lead to stress, conflict and low productivity because they continually clashed with the technological arrangements and the way they were planned and subdivided around them.  Thus we see that the new assembly-line bureaucracy inspired work organisation was totally unsuited to the work environment because there was no end-to-end responsibility, and decision making was far removed from the action. In contrast, the traditional workgroup of six was able to deal with uncertainties and complexities of underground work because team members had a strong sense of responsibility for the performance of the team as a whole. Moreover, teams were uniquely placed to deal with unexpected events because they were actually living them as they occurred and could therefore decide on the best way to deal with them.  What Bamforth found at the Haighmoor seam was that it was possible to recapture the spirit of the old ways of working by adapting these to the larger specialised groups that were necessary in the mechanised mines. As Ciborra describes it in his book:  The new form of work organisation features forty one men who allocate themselves to tasks and shifts. Although tasks and shifts those of the conventional mechanised system, management and supervisors do not monitor, enforce and reward single task executions. The composite group takes over some of the managerial tasks, as it had in the pre-mechanised marrow group, such as the selection of group members and the informal monitoring of work…Cycle completion, not task execution becomes a common goal that allows for mutual learning and support…There is basic wage and a bonus linked to the overall productivity of the group throughout the whole cycle rather than a shift.  The competition between shifts that plagued the conventional mechanised method is effectively eliminated…  Bamforth and Trist's studies on Haighmoor convinced them that there were viable (and better!) alternatives to those that were typical of mid to late 20th century work places.  Their work led them to the insight that the best work arrangements come out of seeking a match between technical and social elements of the modern day workplace, and thus was born the notion of sociotechnical systems.  Ever since the assembly-line management philosophies of Taylor and Ford, there has been an increasing trend towards division of labour, bureaucratisation and mechanisation / automation of work processes.  Despite the early work of the Tavistock school and others who followed, this trend continues to dominate management practice, arguably even more so in recent years. The Haighmoor innovation described above was one of the earliest demonstrations that there is a better way.  This message has since been echoed by many academics and thinkers,  but remains largely under-appreciated or ignored by professional managers who have little idea – or have completely forgotten – what it is like to work at the coalface.Much can be learnt about an organization by observing what management does when things go wrong.  One reaction is to hunt for a scapegoat, someone who can be held responsible for the mess.  The other is to take a systemic view that focuses on finding the root cause of the issue and figuring out what can be done in order to prevent it from recurring.  In a highly cited paper published in 2000, James Reason compared and contrasted the two approaches to error management in organisations. This post is an extensive summary of the paper.  The author gets to the point in the very first paragraph:  The human error problem can be viewed in two ways: the person approach and the system approach. Each has its model of error causation and each model gives rise to quite different philosophies of error management. Understanding these differences has important practical implications for coping with the ever present risk of mishaps in clinical practice.  Reason's paper was published in the British Medical Journal and hence his focus on the practice of medicine. His arguments and conclusions, however, have a much wider relevance as evidenced by the diverse areas in which his paper has been cited.  The person approach – which, I think is more accurately called the scapegoat approach – is based on the belief that any errors can and should be traced back to an individual or a group, and that the party responsible should then be held to account for the error. This is the approach taken in organisations that are colloquially referred to as having a "blame culture."  To an extent, looking around for a scapegoat is a natural emotional reaction to an error. The oft unstated reason behind scapegoating, however, is to avoid management responsibility.  As the author tells us:  People are viewed as free agents capable of choosing between safe and unsafe modes of behaviour.  If something goes wrong, it seems obvious that an individual (or group of individuals) must have been responsible. Seeking as far as possible to uncouple a person's unsafe acts from any institutional responsibility is clearly in the interests of managers. It is also legally more convenient…  However, the scapegoat approach has a couple of serious problems that hinder effective risk management.  Firstly, an organization depends on its frontline staff to report any problems or lapses. Clearly, staff will do so only if they feel that it is safe to do so – something that is simply not possible in an organization that takes scapegoat approach. The author suggests that the Chernobyl disaster can be attributed to the lack of a "reporting culture" within the erstwhile Soviet Union.  Secondly, and perhaps more important, is that the focus on a scapegoat leaves the underlying cause of the error unaddressed. As the author puts it, "by focusing on the individual origins of error it [the scapegoat approach] isolates unsafe acts from their system context." As a consequence, the scapegoat approach overlooks systemic features of errors – for example, the empirical fact that the same kinds of errors tend to recur within a given system.  The system approach accepts that human errors will happen. However, in contrast to the scapegoat approach, it views these errors as being triggered by factors that are built into the system. So, when something goes wrong, the system approach focuses on the procedures that were used rather than the people who were executing them. This difference from the scapegoat approach makes a world of difference.  The system approach looks for generic reasons why errors or accidents occur. Organisations usually have a series of measures in place to prevent errors – e.g. alarms, procedures, checklists, trained staff etc. Each of these measures can be looked upon as a "defensive layer" against error. However, as the author notes, each defensive layer has holes which can let errors "pass through" (more on how the holes arise a bit later).  A good way to visualize this is as a series of slices of Swiss Cheese (see Figure 1).  Figure 1: The Swiss cheese model (from: http://patientsafetyed.duhs.duke.edu/module_e/swiss_cheese.html) Figure 1: The Swiss cheese model (from: http://patientsafetyed.duhs.duke.edu/module_e/swiss_cheese.html)  The important point is that the holes on a given slice are not at a fixed position; they keep opening, closing and even shifting around, depending on the state of the organization.  An error occurs when the ephemeral holes on different layers temporarily line up to "let an error through".  There are two reasons why holes arise in defensive layers:  Active errors: These are unsafe acts committed by individuals. Active errors could be violations of set procedures or momentary lapses. The scapegoat approach focuses on identifying the active error and the person responsible for it. However, as the author points out, active errors are almost always caused by conditions built into the system, which brings us to… Latent conditions: These are flaws that are built into the system. The author uses the term resident pathogens to describe these – a nice metaphor that I have explored in a paper review I wrote some years ago. These "pathogens" are usually baked into the system by poor design decisions and flawed procedures on the one hand, and ill-thought-out management decisions on the other. Manifestations of the former include faulty alarms, unrealistic or inconsistent procedures or poorly designed equipment; manifestations of the latter include things such as unrealistic targets, overworked staff and the lack of  funding for appropriate equipment. The important thing to note is that latent conditions can lie dormant for a long period before they are noticed. Typically a latent condition comes to light only when an error caused by it occurs…and only if the organization does a root cause analysis of the error – something that is simply not done in an organization takes a scapegoat approach.  The author draws a nice analogy that clarifies the link between active errors and latent conditions:  …active failures are like mosquitoes. They can be swatted one by one, but they still keep coming. The best remedies are to create more effective defences and to drain the swamps in which they breed. The swamps, in this case, are the ever present latent conditions.  "Draining the swamp" is not a simple task.  The author draws upon studies of high performance organisations (combat units, nuclear power plants and air traffic control centres) to understand how they minimised active errors by reducing system flaws. He notes that these organisations:  Accept that errors will occur despite standardised procedures, and train their staff to deal with and learn from them. Practice responses to known error scenarios and try to imagine new ones on a regular basis. Delegate responsibility and authority, especially in crisis situations. Do a root cause analysis of any error that occurs and address the underlying problem by changing the system if needed. In contrast, an organisation that takes a scapegoat approach assumes that standardisation will eliminate errors, ignores the possibility of novel errors occurring, centralises control and, above all, focuses on finding scapegoats instead of fixing the system.Management, as it is practiced, is largely about "getting things done."  Consequently management education and research tends to focus on improving the means by which specified ends are achieved. The ends themselves are not questioned as rigorously as they ought to be. The truth of this is reflected in the high profile corporate scandals that have come to light over the last decade or so, not to mention the global financial crisis.  Today, more than ever, there is a need for a new kind of management practice, one in which managers critically reflect on the goals they  pursue and the means by which they aim to achieve them.  In their book entitled,  Making Sense of Management: A Critical Introduction,  management academics Mats Alvesson and Hugh Willmott describe what such an approach to management entails.  This post is a summary of the central ideas described in the book.  Critical theory and its relevance to management The body of work that Alvesson and Willmott draw from is Critical Theory, a discipline that is based on the belief that knowledge ought to be based on dialectical reasoning – i.e. reasoning through dialogue – rather than scientific rationality alone. The main reason for this being that science (as it is commonly practiced) is value free and is therefore incapable of addressing problems that have a social or ethical dimension.  This idea is not new,  even scientists such as Einstein have commented on the limits of scientific reasoning.  Although Critical Theory has its roots in the Renaissance and Enlightenment,  its modern avatar is largely due to a group of German social philosophers who were associated with the Frankfurt-based Institute of Social Research which was established in the 1920s.  Among other things, these philosophers argued that knowledge in the social sciences  (such as management) can never be truly value-free or objective.  Our knowledge of social matters is invariably coloured by our background, culture, education and sensibilities. This ought to be obvious, but it isn't:  economists continue to proclaim objective truths about the right way to deal with economic issues, and management gurus remain ready to show us the one true path to management excellence.  The present day standard bearer of the Frankfurt School is the German social philosopher, Juergen Habermas  who is best known for his theory of communicative rationality – the idea that open dialogue, free from any constraints is the most rational way to decide on matters of importance.  For a super-quick introduction to the basic ideas  of communicative rationality and its relevance in organisational settings, see my post entitled, More than just talk: rational dialogue in project environments. For a more detailed (and dare I say, entertaining) introduction to communicative rationality with examples drawn from The Borg and much more, have a look at Chapter 7 of my book, The Heretic's Guide to Best Practices, co-written with Paul Culmsee.  The demise of command and control? Many professional managers see their jobs in purely technical terms,  involving things such as administration, planning, monitoring etc.  They tend to overlook the fact that these technical functions are carried out within a  particular social and cultural context.  More to the point,  and this is crucial, managers work under constraints of power and domination: they are not free to do what they think is right but have to do whatever their bosses order them to,  and, so  in turn behave with their subordinates in exactly the same way.  As Alvesson and Willmott put it:  Managers are intermediaries between those who hire them and those whom they manage. Managers are employed to coordinate, motivate, appease and control the productive efforts of others. These ‘others' do not necessarily share managerial agendas…  Despite the talk of autonomy and empowerment, modern day management is still very much about  control.  However, modern day employees are unlikely to accept a command and control approach to being managed, so organisations have taken recourse to subtler means of achieving the same result. For example, organisational culture initiatives aimed at getting employees to "internalise"the values of the organisation are attempts to "control sans command."  The point is,  despite the softening of  the rhetoric  of management its principal focus remains much the same as it was in the days of Taylor and Ford.  A critical look at the status quo A good place to start with a critical view of management is in the area of decision-making. Certain decisions,  particularly those made at executive levels,  can have a long term impact on an organisation and its employees. Business schools and decision theory texts tells us that decision-making is  a rational process. Unfortunately, reality belies that claim: decisions in organisations are more  often made on the basis of politics  and ideology rather than objective criteria.  This being the case, it is important that decisions be subject to critical scrutiny.  Indeed it is possible that many of the crises of the last decade could have been avoided if the decisions that lead to them had been subjected a to critical review.  Many of the initiatives that are launched in organisation-land have their origins in  executive-level decisions that are made on flimsy grounds  such as "best practice"  recommendations  from Big 4 consulting companies. Mid-level managers  who are required to see these through to completion are then faced with the problem of justifying these initiatives to the rank and file. Change management in modern organisation-land is largely about justifying the unjustifiable or defending the indefensible.  The critique, however, goes beyond just the practice of management.  For example, Alvesson and Willmott also draw attention to things such as the objectives of the organisation. They point out that short-sighted objectives such as "maximising shareholder value" is what lead to the downfall of companies such as Enron.  Moreover, they also remind us of an issue that is becoming increasingly important in today's world: that natural resources are not unlimited and should be exploited in a judicious, sustainable manner.  As interesting and important as these "big picture" issues are, in the remainder of this post I'll focus attention on management practices that impact mid and lower level employees.  A critical look at management specialisations Alvesson and Willmott analyse organisational functions such as Human Resource Management (HRM), Marketing and Information Systems (IS) from a critical perspective. It would take far too many pages to do justice to their discussion so I'll just present a brief summary of two areas: HR and IS.  The rhetoric of HRM in organisations stands in stark contradiction to its actions. Despite platitudinous sloganeering about empowerment etc., the actions of most HR departments are aimed at getting people to act and behave in organisationally acceptable ways.  Seen in a critical light, seemingly benign HR initiatives such as organizational culture events or self-management initiatives are exposed as being but subtle means of managerial control over employees.  (see this paper for an example of the former and this one for an example of the latter).  Since the practice of IS focuses largely on technology,  much of the IS  research and practice tends to focus on technology trends and "best practices." As might be expected, the focus is on "fad of the month" and thus turns stale rather quickly. As examples: the 1990s saw an explosion of papers and projects in business process re-engineering; the flavour of the decade in the 2000s was service-oriented architecture; more recently, we've seen a great deal of hot air about the cloud.  Underlying a lot of technology related decision-making is the tacit assumption that choices pertaining to technology are  value-free and can be decided on the basis of  technical and financial criteria alone.  The profession as a whole tends to take an overly scientific/rational approach to design and  implementation, often ignoring issues such as power and politics.  It can be argued that many failures of large-scale IS projects are due to the hyper-rational approach taken by many practitioners.  In a similar vein, most management specialisations can benefit from the insights that come from taking a critical perspective.  Alvesson and Willmott discuss marketing, accounting and other functions. However,  since my main interest is in solutions rather than listing the (rather well-known) problems,  I'll leave it here, directing the interested reader to the book for more.  Towards an enlightened practice of management In the modern workplace it is common for employees to feel disconnected from their work, at least from time to time if not always. In a prior post, I discussed how this sense of alienation is a consequence of our work and personal lives being played out in two distinct spheres – the system and the lifeworld. In brief, the system refers to the professional and administrative sphere in which we work and/or  interact with institutional authority and the lifeworld is is the everyday world that we share with others. Actions in the lifeworld are based on a shared understanding of the issue at hand whereas those in the system are not.  From the critical analysis of management specialisations presented in the book, it is evident that the profession, being mired in a paradigm consisting of prescriptive, top-down practices, serves to perpetuate the system by encroaching on the lifeworld values of employees. There are those who will say that this is exactly how it should be. However, as Alvesson and Wilmott have stated in their book, this kind of thinking is perverse because it is ultimately self-defeating:  The devaluation of lifeworld properties is perverse because …At the very least, the system depends upon human beings who are capable of communicating effectively and who are not manipulated and demoralized to the point of being incapable of cooperation and productivity.  Alvesson and Willmott use the term emancipation, to describe any process whereby employees are freed from shackles of system-oriented thinking even if only  partially  (Note: here I'm using the term system in the sense defined above – not to be confused with systems thinking, which is another beast altogether). Acknowledging that it is impossible to do this at the level of an entire organisation or even a department, they coin the term micro-emancipation to describe any process whereby sub-groups of organisations are empowered to think through issues  and devise appropriate  actions by themselves, free (to the extent possible) from management constraints or directives.  Although this might sound much too idealistic to some readers, be assured that it is eminently possible to implement micro-emancipatory  practices in real world organisations. See this paper  for one possible framework that can be used within a multi-organisation project along with a detailed case study that shows how the framework can be applied in a complex project environment.  Alvesson and Willmott warn that emancipatory practices are not without costs, both for employers and employees. For example, employees who have gained autonomy may end up being less productive which will in turn affect their job security.  In my opinion, view, this issue can be addressed through an incrementalist approach wherein both employers and employees work together to come up with micro-emancipatory projects at the grassroots level, as in the case study described in the paper mentioned in the previous paragraph.  …and so to conclude Despite the rhetoric of autonomy and empowerment, much of present-day management is stuck in a Taylorist/Fordist paradigm. In modern day organisations command and control may not be obvious, but they often sneak in through the backdoor in not-so-obvious ways. For example, employees almost always know that certain things are simply "out of bounds" for discussion and of the consequences of breaching those unstated boundaries can be severe.  In its purest avatar, a critical approach to management seeks to remove those boundaries altogether.  This is unrealistic because nothing will ever get done in an organisation in which everything is open for discussion; as is the case in all social systems, compromise is necessary. The concept of micro-emancipation offers just this. To be sure, one has to go beyond the rhetoric of empowerment to actually creating an environment that enables people to speak their minds and debate issues openly.  Though it is impossible to do this at the level of an entire organisation, it is definitely possible to achieve it (albeit approximately)  in small workgroups.  To conclude: the book is worth a read, not just by management researchers but also by practicing managers.  Unfortunately  the overly-academic style  may be a turn off for practitioners, the very people who need to read it the most.  The platitude "our people are  our most important asset"  reflects a belief that the survival and evolution of organisations depends  on the intellectual and cognitive capacities of the individuals who comprise them.  However,  in view of the many well documented examples of actions that demonstrate a lack of  foresight and/or general callousness about the fate of organisations or those who work in them,  one has to wonder if such a belief is justified, or even if it is really  believed by those who spout such platitudes.  Indeed,  cases such as Enron or Worldcom  (to mention just two) seem to suggest that stupidity may be fairly prevalent in present day organisations. This point is the subject of a brilliant paper by Andre Spicer and Mats Alvesson entitled, A stupidity based theory of organisations.  This post is an extensive summary and review of the paper.  Background The notion that the success of an organization depends on the intellectual and rational capabilities of its people seems almost obvious. Moreover, there is a good deal of empirical research that seems to support this. In the opening section of their paper, Alvesson and Spicer cite many studies which appear to establish that developing the knowledge (of employees) or hiring smart people  is the key to success in an ever-changing, competitive environment.  These claims are mirrored in theoretical work on organizations. For example Nonaka and Takeuchi's model of knowledge conversion acknowledges the importance of tacit knowledge held by employees. Although there is still much debate about tacit/explicit knowledge divide, models such as these serve to perpetuate the belief that knowledge (in one form or another) is central to organisational success.  There is also a broad consensus that decision making in organizations, though subject to bounded rationality and related cognitive biases,  is by and large a rational process. Even if a decision is not wholly rational, there is usually an attempt to depict it as being so. Such behaviour attests to the importance attached to rational thinking in organization-land.  At the other end of the spectrum there are decisions that can only be described as being, well… stupid. As Rick Chapman discusses in his entertaining book, In Search of Stupidity, organizations occasionally make decisions that are  plain dumb However, such behaviour seldom remains hidden because of its rather obvious negative consequences for the organisation.  Such stories thus end up being  immortalized in business school curricula as canonical examples of what not to do.  Functional stupidity Notwithstanding the above remarks on  obvious stupidity, there is another category of foolishness that is perhaps more pervasive but remains unnoticed and unremarked. Alvesson and Spicer use the term functional stupidity to refer to such  "organizationally supported lack of reflexivity, substantive reasoning, and justitication."  In their words, functional stupidity amounts to the "…refusal to use intellectual resources outside a narrow and ‘safe' terrain."  It is reflected in a blinkered approach to organisational problems, wherein people display  an unwillingness  to consider or think about solutions that lie outside an arbitrary boundary.  A common example of this is when certain topics are explicitly or tacitly deemed as being "out of bounds" for discussion. Many "business as usual" scenarios are riddled with functional stupidity, which is precisely why it's often so hard to detect.  As per the definition offered above, there are three cognitive elements to functional stupidity:  Lack of reflexivity: this refers to the inability or unwillingness to question claims and commonly accepted wisdom. Lack of substantive reasoning: This refers to  reasoning that is based on a small set of concerns that do not span the whole issue. A common example of this sort of myopia is when organisations focus their efforts on achieving certain objectives with little or no questioning of the objectives themselves. Lack of justification: This happens when  employees do not question managers or, on the other hand, do not provide explanations regarding their  own actions. Often this is a consequence of power relationships in organisations. This may, for example, dissuade employees from "sticking their necks out" by asking questions that managers might deem out of bounds. It should be noted that functional stupidity has little to do with limitations of human cognitive capacities. Nor does it have anything to do with ignorance, carelessness or lack of thought. The former can be  rectified through education and/or the hiring of consultants with the requisite knowledge,  and the latter via the use of standardised procedures and checklists.  It is also important to  note that  functional stupidity is not necessarily a bad thing. For example, by placing certain topics out of bounds, organisations can avoid discussions about potentially controversial topics and can thus keep conflict and uncertainty at bay.  This maintains  harmony, no doubt, but it also strengthens the existing organisational order which  in turn serves to reinforce functional stupidity.  Of course, functional stupidity also has negative consequences, the chief one being that it prevents organisations from finding solutions to issues that involve topics that have been arbitrarily deemed as being out of bounds.  Examples of functional stupidity There are many examples of functional stupidity in recent history, a couple being the irrational exuberance in the wake of the internet boom of the 1990s, and the lack of  critical examination of the complex mathematical models that lead to the financial crisis of last decade.  However, one does not have to look much beyond one's own work environment to find examples of functional stupidity.  Many of these come under the category of  "business as usual"  or "that's just the way things are done around here" – phrases that are used to label practices that are ritually applied without much thought or reflection.  Such practices often remain unremarked because it is not so easy to link them to negative outcomes.  Indeed, the authors point out that "most managerial practices are adopted on the basis of faulty reasoning, accepted wisdom and complete lack of evidence."  The authors cite the example of companies adopting HR practices that are actually detrimental to employee and organisational wellbeing.  Another common example  is when organisations place a high value on gathering information which is then not used in a meaningful way.   I have discussed this "information perversity" at length in my post on entitled, The unspoken life of information in organisations, so I won't  rehash it here.  Alvesson and Spicer point out that information perversity is a consequence of the high cultural value placed on information: it is seen as a prerequisite to "proper" decision making. However,  in reality it is often used to justify questionable decisions or simply "hide behind the facts."  These examples suggest that functional stupidity may be the norm rather than the exception. This is a scary thought…but I suspect it may not be surprising to many readers.  The dynamics of stupidity Alvesson and Spicer claim that functional stupidity is a common feature in organisations. To understand why it is so pervasive, one has to look into the dynamics of stupidity – how it is established and the factors that influence it.  They suggest that the root cause lies in the fact that organisations attempt to short-circuit critical thinking through what they call economies of persuasion, which are activities such as corporate culture initiatives, leadership training or team / identity building, relabelling positions with pretentious titles – and many other such activities that are aimed at influencing employees  through the use of symbols and images rather than substance. Such symbolic manipulation, as the authors calls it, is aimed at increasing employees' sense of commitment to the organisation.  As they put it:  Organizational contexts dominated by widespread attempts at symbolic manipulation typically involve managers seeking to shape and mould the ‘mind-sets' of employees . A core aspect of this involves seeking to create some degree of good faith and conformity and to limit critical thinking  Although such efforts are not always successful, many employees do buy in to them and thereby identify with the organisation. This makes employees uncritical of the organisation's  goals and the means by which these will be achieved. In other words, it sets the scene for functional stupidity to take root and flourish.  Stupidity management and stupidity self-management The authors use the term stupidity management to describe managerial actions that prevent or discourage organisational actors (employees and other stakeholders) from thinking for themselves.  Some of the ways in which this is done include the reinforcement of positive images of the organisation, getting employees to identify with the organisation's vision and myriad other organisational culture initiatives aimed at burnishing the image of the corporation. These initiatives are often backed by organisational structures (such as hierarchies and reward systems) that discourage employees from raising and exploring potentially disruptive issues.  The monitoring and sanctioning of activities that might disrupt the positive image of the organisation can be overt (in the form of warnings, say). More often, though, it is subtle. For example, in many meetings, participants participants know that certain issues cannot be raised. At other times, discussion and debate may be short circuited by exhortations to "stop thinking and start doing."  Such occurrences serve to create an environment in which stupidity flourishes.  The net effect of  managerial actions that encourage stupidity is that employees start to cast aside their own doubts and questions and behave in corporately acceptable ways – in other words, they start to perform their jobs in an unreflective and unquestioning way. Some people may actually internalise the values espoused by management; others may psychologically  distance themselves from the values but still act in ways that they are required to. The net effect of such stupidity self-management (as the authors call it) is that employees stop questioning what they are asked to do and just do it. After a while, doubts fade and this becomes the accepted way of working. The end result is the familiar situation that many of us know as "business as usual" or  "that's just the way things are done around here."  The paradoxes and consequences of stupidity Functional stupidity can cause both feelings of certainty and dissonance in members of an organisation. Suppressing  critical thinking  can result in an easy acceptance of  the way things are.  The feelings of certainty that come from suppressing difficult questions can be comforting. Moreover, those who toe the organisational line are more likely to be offered material rewards and promotions than those who don't. This can act to reinforce functional stupidity because others who see stupidity rewarded may also be tempted to behave in a similar fashion.  That said,  certain functionally stupid actions, such as ignoring obvious ethical lapses, can result in serious negative outcomes for an organisation. This has been amply illustrated in the recent past. Such events can prompt formal inquiries  at the level of the organisation, no doubt accompanied by  informal soul-searching at the individual level. However, as has also been amply illustrated, there is no guarantee that inquiries or self-reflection lead to any major changes in behaviour. Once the crisis passes, people seem all too happy to revert to business as usual.  In the end , though, when stark differences between the rhetoric and reality of the organisation emerge  – as they eventually will– employees will  see the contradictions between the real organisation and the one they have been asked to believe in. This can result in alienation from and cynicism about the organisation and its objectives. So, although stupidity management may have beneficial outcomes in the short run, there is a price to be paid  in the longer term.  Nothing comes for free, not even stupidity…  Conclusion The authors main message is that despite the general belief that organisations enlist the cognitive and intellectual capacities of their members in positive ways, the truth is that organisational behaviour often exhibits a wilful ignorance of facts and/or a lack of logic. The authors term this behaviour functional stupidity.  Functional stupidiy has the advantage of maintaining harmony at least in the short term, but its longer term consequences can be negative.  Members of an organisation "learn" such behaviour  by becoming aware that certain topics are out of bounds and that they broach these at their own risk. Conformance is rewarded by advancement or material gain whereas dissent is met with overt or less obvious disciplinary action. Functional stupidity thus acts as a barrier that can stop members of an organisation from developing potentially interesting perspectives on the problems the organisations face.  The paper makes an interesting and very valid point about the pervasiveness of wilfully irrational behaviour in organisations. That said, I  can't help but think that the authors  have written it with tongue firmly planted in cheek.Change, as the cliché goes, is the only constant.  At any given time, most organisations are either planning or implementing changes of some kind.  Perhaps because of its ubiquity, the rationale and results of change are not questioned as deeply as they ought to be.  In this post I describe some unintended effects of organisational change, drawing on Barbara Czarniawska's book, A Theory of Organizing and other sources. I also briefly discuss some ways in which these side effects can be avoided.  I'll begin with a few words about terminology.  In this article planned changes (also referred to as reforms) are changes instituted in order to achieve specific goals. The goals of reforms are referred to as planned effects – that is, planned effects are intended results of change. As I discuss below, although planned effects may eventually be achieved, change initiatives have a host of unforeseen but significant consequences. These are referred to as unplanned, unintended or side effects.  This article is organised as follows: I'll begin by describing some of the positive and negative side effects of change, following which I'll discuss why side effects come about and how they can be managed.  Advantageous side effects of change Although, the term side effect has a negative connotation, some side effects of change can actually be advantageous.  These include:  Questioning of the status quo: In most organisations, processes and structures are taken for granted, rarely is the status quo questioned. Organisational change presents an opportunity to pose those "How can we do this better?" type questions that challenge the way things are done. Such questioning is unplanned in that it generally occurs spontaneously. Opportunities for reflection: This is a consequence of the previous point: questioning the status quo can cause people to reflect on how things can be done better. Again, this is an unintended consequence of a reform, not part of its planned goals. Also, it should be noted that although opportunities for reflection arise often, they are generally ignored because of time pressures. Spontaneous inventions: Finally, questioning of and reflecting on the status quo can trigger ideas for improvement. Most people would agree that the above points are indeed Good Things that ought to be encouraged.  However, the important point is that people who are in the throes of a planned change seldom have the time or motivation to pursue these opportunities.  Harmful side effects of change The negative side effects of planned changes are insidious because they tend to occur as a result of inaction – i.e. by not taking corrective actions to counter the detrimental effects of change. The following side effects serve to illustrate this point:  The aims of reform become cast in stone: The objectives of a change initiative are formulated based on an understanding of a situation as it exists at a particular point in time. Problem is, as time evolves the original objectives maybecome irrelevant or obsolete. Yet, in many (most?) change initiatives, objectives are rarely reviewed and adjusted. The means get confused with the ends: Following from the previous point, a change initiative becomes pointless when its objectives are no longer relevant.  However, a common reaction in such situations is to continue the initiative, justifying it as a worthwhile end in itself. For example, if the benefits of, say, a restructuring initiative become moot,  the  restructuring itself  becomes the objective rather than the benefits that were supposed to flow from it.  This helps save face as the project can be declared a success once the restructuring is completed, regardless of whether or not the promised benefits are realised. Improvisations and spontaneous inventions are suppressed: As I have discussed at length in this post, planning and improvisation are complementary but contradictory aspects of organizational work. A negative aspect of planned change initiatives is that they are inimical to improvisations: those responsible for overseeing the change tend to ignore, even suppress any improvisations that arise because they are seen as getting in the way of achieving the objectives of the primary change. Planned change initiatives are generally implemented through programs or projects.  In fact, most major projects in organisations – restructurings, enterprise system implementations etc – are aimed at implementing reforms of some kind. However, although the raison d'etre of such projects is to achieve the planned objectives, many suffer from the negative side effects mentioned above.  In her book Czarniawska states, "Planned change rarely, if ever, leads to planned effects."  Although this claim may be a tad exaggerated, the significant proportion of large projects that fail suggests there is at least a whiff of truth about it.  In the next two sections I take a brief look at why planned changes fail and what can be done about it.  The origin of the side effects of change Most structures and processes within organisations have a complex, path-dependent history. Among other things, they develop in ways that are unique to an organisation and are often deeply intertwined with each other.  As a result, it is impossible to be certain about the consequences of changing processes or structures – there are just too many variables and dependencies involved.  There are two related points that flow from this:  Firstly, those who plan changes need to have a good understanding of legacy: the history of the issues that the change aims to fix and those that it may create in the future. The problem is most of the people involved in planning, initiating and executing reforms have little appreciation of such issues.  Secondly, most major changes are conceived by a small number of people who hold positions of authority within organisations. These folks have a tendency to gloss over complexities, and often fail to involve those who have a detailed knowledge of the affected processes and structures. Consequently, their plans overlook dependencies and possible knock-on effects that can arise from them. This results in the negative side effects discussed in the previous section.  ..and what can be done about them Czarniawska recommends the following informal rules for successful change:  Be willing to modify the objectives of the change and your path to get there as your understanding of it evolves. Implement lightweight processes, avoid bureaucratic procedures. Be open to improvisations. This is good advice as it goes, but how exactly does one use it?  In our recently published book, The Heretic's Guide to Best Practices, Paul Culmsee and I discuss how issues of legacy and lack of inclusiveness can be addressed.  Firstly, we suggest that apart from time, cost and scope (the classic iron triangle), project decision-makers would be well served by considering legacy as a separate variable in projects (also see this post on Paul's blog for more on this point). More importantly, we describe techniques that can be used to surface hidden assumptions and aspects of history that could have a bearing on the project and those that might cause problems in the future.  Secondly, we discuss how one can work towards creating an environment in which a diverse group of stakeholders can air and reconcile their viewpoints. Such a discussion is a prerequisite to creating a plan that: a) considers as many viewpoints (variables) as possible and b) has the support of all stakeholders.  Without this, any implementation is bound to have side-effects because of overlooked variables and/or the actions (or non-actions) of stakeholders who do not support the plan.  Of course, inclusiveness sounds great but it can be difficult in practice, especially in large organisations. What can decision-makers do in such cases?  The answer comes from a slightly different, if rather obvious direction.  In his very illuminating book on decision-making, James March notes that organisations face messy and inconsistent environments. Given this, decisions made and implemented at lower levels have a better chance of success than those made in rarefied air of board-rooms.  Paraphrasing a statement from his book:  Since knowledge of local conditions and specialized competencies are both essential and more readily found in decentralized units, control over the details of policy implementation and adaptation of general policies to local conditions are [best] delegated to local units. From the standpoint of general management, the strategy is usually seen as one of gaining the informational and motivational advantages of using people with local involvement, [but] at the cost of accentuating problems of central coordination and control.  Indeed, most of the nasty side effects of planned change arise from over-centralisation of coordination and control.  The solution is to devolve control and decision-making authority down to the level at which the changes are to be implemented.  Conclusion Planned change fails to achieve its goals because planners cannot foresee all the consequences of change or even know which factors may be important in determining these. Moreover, individuals will view changes through the lens of their background, biases and interests.  Since organisations consist of many individuals with different views, managing change is essentially a wicked problem.  To sum up, those who initiate large-scale changes should keep in mind the law of unintended consequences:  any planned action will have consequences that are not intended, or even foreseen.  These consequences can be managed by getting a better appreciation of the factors that affect the processes and the structures to be changed.  One can gain an understanding of these factors through a consideration of legacy and/or via dialogue involving all those who work with the processes and structures that are to be changed. The simplest way to achieve both is by delegating decision making and implementation authority down to where it belongs – with the people who work at the coalface of the organisation.It is a truism that two organisations using the same project management practices and structures will have different levels of success with them. Clearly, there's a lot more to project success than project management. Despite this, most studies of project success tend to focus on  project level, or operational, variables such as  level of user involvement, use (or not) of a formal methodology, reliability of estimates etc (Note: these variables have been taken from the oft quoted Standish Report). As important as these factors are, they fail to take into account that projects live and evolve in a wider environment which includes the sponsoring organisation.  A recent paper entitled, New Product Development Projects: The Effects of Organizational Culture published in the December 2007 issue of the Project Management Journal, studies the effect of organisational culture on project success with specific reference to new product development (NPD) projects.  I summarise and review the paper below.  The authors claim that despite the importance of NPD projects for the long term success of an organisation, the effect of strategic level variables (organisational culture, organizational strategy, management involvement etc.)  on project success has not been widely studied. They suggest this might be so because these variables are hard to define, quantify and measure.  Further, on reviewing the existing literature, they find that the few published, organisation-oriented studies tend to focus on the end result of the development process (i.e. the product) rather than on factors affecting the project. Hence the motivation for their study.  Incidentally, they note that there has been some work on the effect of national culture on NPD project performance,  but these studies find no correlation between the two.  To measure something as elusive as organisational culture, you first have to pin it down by defining it. The definition does not have to be all-encompassing, but it needs to be precise enough for people to have a common understanding of what you're talking about. To do this, the authors created a set of questions based on various definitions of organisational culture available in the literature. The resulting questionnaires were mailed out to various organisations engaged in NPD projects. The responses received (from over a hundred organisations) were analysed using  exploratory factor analysis,  enabling the authors to group the questions  into the following dimensions of organisational culture:  Positive work environment: this includes factors such as openness to new ideas, employees feeling valued as individuals, open discussion with superiors encouraged etc. Management leadership:  this includes factors such as clear goals set and responsibilities delegated, employees have input in decision making, incentives offered to work on new ideas, high-risk high-return projects encouraged etc.  Results orientation: this includes factors such as employees are pressured to finish work, correct procedures more important than correct results etc. These dimensions define organisational culture for the purposes of their study.  To measure project success, the authors use the following dimensions adapted from Griffin and Page:  Consumer-based:  the customers are satisfied with the product. This can also be classed as Customer Satisfaction. Commercial success: the product makes money Technical success: the product works as intended. Note that these variables are actually a subset of those suggested by Griffin and Page.  Project success was measured by getting upper management in the surveyed companies to rate product success along each of the above dimensions.  Finally, the authors correlate organisational culture to product success (for the surveyed companies) using correlation and regression analysis. The results (which are really no surprise) indicate that:  Positive work environments and management leadership are strongly correlated with each other and with the three measures of product success. That is: Strong management leadership and positive work environments go hand-in-hand. Companies with positive work environments (and, by implication, strong management leadership)  have better commercial success with new products, enjoy better customer satisfaction and have greater technical success than those with less positive work environments (and, by implication, weak leadership). Results orientation is not strongly correlated with any of the other variables. If this seems surprising at first sight, take another look at what goes into making up this variable and it will seem less so! Although the paper focuses on NPD projects, I think the conclusions – especially those pertaining to customer satisfaction and technical success –  apply to other  projects  as well.  Further, though the conclusions may be obvious to many, such research is important because it lends analytical backing to otherwise intuitive notions. It does this by:  Defining (albeit, in a limited way) what is meant by organisational culture and project success. Studying the relationship between the variables that make up the two. Defining variables and quantifying relationships can give us a sense for which organisational culture variables are the most significant determinants of project success. So, although the study is a preliminary one (as the authors themselves admit), the work is a useful step in understanding the relationship between projects and the larger environment in which projects live and breathe.Over the last two decades or so, it has been recognized that creativity and innovation tend to thrive in organisations  where employees have a say in decisions that affect their work.  This has lead to the notion of a post-bureaucratic organisation –  an organisation in which decisions are made collectively through dialogue and consensus, and where  the hierarchy is flat.  Although a number of workgroups within large organizations function this way (and with some success too),  a post-bureaucratic organisation is generally seen as a utopian and academic ideal; one that is  unlikely to work in the real world. Those who manage  organizations, departments or workgroups  are generally uncomfortable with employees working autonomously,  even though this might lead to the generation of novel ideas and products.  This is understandable: it is ultimately the responsibility of managers to ensure that  organisational or departmental goals are achieved. How else to do this but through the time-tested command and control approach to management?  In response to the question posed above, project management is often touted as a means to manage creative and innovative efforts in organisations (see some of the articles in this issue of PM Network, for example).  The claim seems a reasonable one: project management (by definition) provides a means to manage  collective, goal oriented endeavours. Further, many projects – especially those involving  new product or software development – have a creative/innovative component. In practice, though, project management tends to be a bureaucratic affair;  involving plans that must be followed, schedules that must be adhered to and regular progress reports that must be made. Even so (or perhaps, because this is so) many organizations see project management as a means to manage all creative work in a post-bureaucratic setting.  Implicit in this view is the assumption that the implementation of project management processes will  enable managers to control and direct creative work without any adverse side-effects.  An article by Damian Hodgson entitled Project Work: The Legacy of Bureaucratic Control in the Post-Bureaucratic Organisation, explores the tensions and contradictions presented by this notion. Although the article was written a while ago (in 2003), I believe the ideas explored in it are ever more relevant today, particularly in view of the increasing projectisation of organisations and the work carried out within them.  Hence my motivation to  summarise and review  the paper.  Background – setting the stage To be fair, many organizations recognise that a "light hand on the rudder" is needed in order to encourage creativity and innovation.  In these organisations, project management is often seen as a means to achieve this.  But  how well does it work in practice? Hodgson's paper aims to provide some insight into this question via a case study of an organization in which a project-based form of management was implemented as a means to balance the  requirements of creativity and control. In his words:  In response to the challenges of the post-bureaucratic form, project management has  been put forward by many as a ‘tried-and-tested' package of techniques able to cope with discontinuous work, expert labour and continuous and unpredictable change while delivering the levels of reliability and control of the traditional bureaucracy. In this article I explore some of the contradictions and tensions within a department where such a ‘hybrid' mode of control is implemented, embodying both bureaucratic and post-bureaucratic logics. In particular, I focus upon the discursive tactics employed to sell ‘rebureaucratization' as ‘debureaucratization', and the complex employee responses to this initiative. I argue that the tensions evident here cast significant doubt on the feasibility of a seamless integration of bureaucracy and the post-bureaucratic [organization].  The "discursive tactics" that Hodgson mentions to are (seemingly reasonable and rational) arguments that an organization might use to "sell" the idea that the methods and approaches of project management are consistent with the ideals of a post-bureaucratic organization. An example of such an argument goes as follows: project management is routinely used to manage new product development projects, so it is eminently suited to managing creative work (Incidentally,  this isn't quite right – see this paper review for more on why)  Post bureaucracy vs. bureaucracy Before moving on, it is worth comparing the characteristics of bureaucratic and post-bureaucratic organizations. Hodgson provides the following comparison, based on the work of Charles Hekscher:  Bureacracy	Post-bureaucracy Consensus through Acquiescence to Authority	Consensus through Institutionalized Dialogue Influence based on Formal Position	Influence through Persuasion/PersonalQualities Internal Trust Immaterial	High Need for Internal Trust Emphasis on Rules and Regulations	Emphasis on Organizational Mission Information Monopolised at Top of Hierarchy	Strategic Information shared in Organization Focus on Rules for Conduct	Focus on Principles Guiding Action Fixed (and Clear) Decision Making Processes	Fluid/Flexible Decision Making Processes Network of Specialized FunctionalRelationships	Communal Spirit/Friendship Groupings Hierarchical Appraisal	Open and Visible Peer Review Processes Definite and Impermeable Boundaries	Open and Permeable Boundaries Objective Rules to ensure Equity of Treatment	Broad Public Standards of Performance Expectation of Constancy	Expectation of Change The aim of the case study is to highlight some of the inconsistencies and contradictions that result from applying a bureaucratic mechanism (project management) to manage the work of a group that was very good at doing creative work, but was used to a more free-wheeling, hands-off management style (i.e. a group which approximates the idealised post-bureaucratic environment described above).  Why project management? Project management has its roots in classical management (ala Taylor and Fayol), so it is perhaps  surprising that it is considered as a means to manage work in a post-bureaucratic setting. In Hodgson's words:  I would argue that what distinguishes project management as of particular relevance to 21stcentury organizations is its rediscovery of a very 19th-century preoccupation with comprehensive planning, linked to a belief in the necessity of tight managerial discipline.  Project management tools and techniques support managerial discipline by providing means to decompose the project into manageable bits – by using a WBS say – and then assigning these bits to individuals (or small teams). The work assigned can then be tracked and controlled tightly – which is a good thing.  If the matter rested there it wouldn't be too bad, but very often management also prescibes how the work should be done. This level of control results in a loss of autonomy (and motivation)  for team members whose job it is to do the work. The loss of motivation can have negative effects, especially in projects with a large creative component.  To counter this criticism, in recent years project management has started to focus on the creative aspects of projects – what's needed to motivate teams, how to foster creativity (in new product development work, say) etc. As Hodgson puts it,  The linking of project management and change management has increased project management's influence across industries, such that now the largest professional organization in project management includes special interest groups in areas as diverse as healthcare, retail, media, marketing, and hospitality. As a consequence the last decade has been a time of particularly rapid expansion for project management, as issues of change, knowledge management, and constant innovation emerged as central themes in popular management discourse.  So, project management offers two (seemingly contradictory) benefits: the ability to maintain tight control of work and the ability to foster innovation and creativity:  …project management can be seen as an essentially bureaucratic system of control, based on the principles of visibility, predictability and accountability, and operationalized through the adherence to formalized procedure and constant written reporting mechanisms. At the same time, however, project management draws upon the rhetoric of empowerment, autonomy and self-reliance central… In principle, then, project management offers a system which attempts to integrate bureaucratic control and a form of responsible autonomy more in keeping with the interdisciplinary, knowledge-intensive nature of much project work in teams.  Seen in this light, it is perhaps not so surprising that project management is viewed as a means to manage creative work.  The case study With the above background done, I now move on to a discussion of the case study.  In Hodgson's words, the study:  …focused upon  a telephone bank in northern England which I have referred to under the pseudonym Buzzbank. In the late 1980s, Buzzbank had been set up by a major UK bank, which I will call TN Banking, and represented one of several success stories in the retail banking sector over this period. Through reduced overheads and the extensive use of new technology in the form of sophisticated marketing techniques and call-centre technology, Buzzbank had expanded rapidly in terms of market share and turnover, developing into a key component of TN Banking's global operations. My interest in particular centred on Buzzbank senior management's identification of project management as the prime ‘critical success factor' for the organization; the development of project management expertise throughout the organization was seen as a key priority to maintain performance into the next decade. To an extent, the project teams researched could scarcely be more ‘cutting edge', representing highly-trained ‘knowledge workers' developing innovative high technology applications and solutions in a new sector of an enormously profitable industry  Hodgson conducted interviews and observed operations within the IT department of Buzzbank over a period of two years. During this period, the organization was implementing a "strategic plan" aimed at formalizing innovation and creative work using project management processes. The idea, in the words of a couple of senior IT managers was to "bring a level of discipline" and "bring an idea of professional structuring" to the work of the highly successful unit. The structuring and discipline was to be achieved by implementing project management processes.  The main rationale used to sell project management to the Buzzbank IT team is a familiar one: the need to ensure predictability and repeatability of work done whilst ensuring that innovation and creativity would not be impeded. Another justification offered by management was that the size of the organization (which had grown considerably in the years prior to the implementation of the strategic plan) meant that  the existing "ad-hoc" work culture would no longer be successful. That is,  the size of the organization necessitated a degree of formalization,  ergo bureaucratic procedures had to be put in place. This was rationalised  (by senior management) as a natural and inevitable consequence of growth:  …The organization was therefore portrayed by senior management in IT as approaching its ‘next stage of evolution'. The immediate benefit of such a metaphor for those members of senior management charged with rebureaucratizing the organization is that it carries a very strong sense of inevitability. As such, it casts opposition to such changes as irrational and futile, standing in the way of natural ‘evolution'.  Further, managers in the organization dubbed any employee resistance as "natural growing pains" – like those of an adolescent, say. Cast in this light, dissenting viewpoints were portrayed as natural and unavoidable – and possibly even necessary – but ultimately without any validity.  Another interesting aspect that Hodgson highlights is the way in which old practices (the successful but "bad" ones) were subsumed in the new (formal) framework. For example, in the old world, employees were given the freedom to experiment, and many considered this as a strength not a weakness. In the new world, however, such a practice was seen as a threat; it was considered more important to capture how to do things correctly so that things became repeatable (ala best practice) and experimentation would not be necessary. As one manager put it:  If we capture how we do things right, at least it makes things repeatable, and we can record the improvement required when things don't go right, which doesn't happen in a rapidly-expanding, gung-ho environment.  Hodgson notes that the terms rapidly-expanding and gung-ho, which are used in a negative sense, could just as well be cast in positive terms such as  flexible, proactive etc. The point being that management framed the existing situation in terms that made the implementation of the new procedures seem like a logical and reasonable next step. The processes were touted as a means to achieve change (i.e. be flexible), but in a controlled way. So, management went to great lengths to avoid use of terms that would be perceived as being negative – for example, the term "structure" was used instead of "bureaucracy" or "formalization." In this way, management attempted to assimilate the existing values of Buzzbank into the strategic plan.  So, how well did it work? Here's what Hodgson says about the end result:  The impression given [by senior management] was that of an organizational change which was inevitable, which gave rise to some understandable but irrational resistance, and which had now been effectively completed, for the good of the organization as a whole.  On the other hand, the impression Hodgson got from speaking to lower level employees was very different:  However, in the time spent by myself in the organization, the tone and target of much of the humour, as well as much stronger reactions, appeared to throw doubt on the extent to which this discourse had permeated among the general employees, particularly within the IT department. Humour was commonplace in the everyday banter both within teams and between teams in the IT division at Buzzbank, and the increasing levels of bureaucratization was the butt of most of the humour, particularly at the lower levels of the hierarchy. The main experience of project management as reported by many  Buzzbank employees was one of intensified bureaucratic surveillance…  A key example is the reaction of employees to managerial jargon that was used in company circulars and literature intended to promote the strategic plan.  Typically, comments were provoked by the circulation of literature on the strategic plan, and again, excerpts of the document were read out by members of staff, adding ironic comments to underline the gap between the document and their experience of life and work in the department.  Hodgson notes that employees often appeared to comply with the new regulations, but not in the way intended by management:  At other times, the employees appeared to comply with the formal requirements of the new system, in terms of filling in the necessary forms, reporting in at given times, completing the necessary work-logs and so on. Even here, despite the relative sophistication of senior management's re-articulation of key discourses, compliance on the part of Buzzbank employees in many cases bore all the hallmarks of instrumental behaviour, accompanied by insubordinate statements and humour ranging from the cynical to the confrontational. At other times, assurances were given to senior management and immediately contravened, fictionalized accounts of project activities were submitted late, or else procedures were observed meticulously to the detriment of deadlines and other constraints. The emergent organizational order was a precarious negotiation between alienated compliance and an autonomous disregard for bureaucratic demands…  In short: there was a clear gap between the perceptions of management and employees as to the success of the newly implemented processes.  It is clear that Buzzbank managers saw  project management as  a means to control and direct creative / innovative work in a way that would have no negative effect on employee morale and motivation.  The challenge, of course, lay in achieving employee buy-in.  Management used many creative (!) tactics to "sell" project management to staff. These included:  References to a "natural process of evolution" and the consequent "growing pains" that the organization would experience. This made the pain of the change natural and inevitable, but necessary in the interest of future gain. Manipulation of terminology to make the changes seem more palatable – e.g. using the word "structure" instead of "formalization" or "bureaucracy". Co-opting terminology of post-bureaucratic organizations into literature designed to promote the new structure. For example, claiming that project management processes would enable the organization to be even more responsive to change (i.e. flexible), through change management processes. From employees' perspective, such techniques were plainly seen for what they were: methods to "sell the unsellable". The negative reactions of employees were manifested through sarcastic humour and (often minor) acts of insubordination. The case study thus highlights the difficulties in using project management as a means to control work in a post-bureaucratic work environment.  Wrapping-up: reflections and summary Hodgson sees the case study as exemplifying the problem of control vs. autonomy in emerging post-bureaucratic organizations:  managers view project management as a means to address the risks inherent in post-bureaucratic work, whereas employees view it as a unnecessary and unjustified imposition. Management was looking for the "best of both worlds", a hybrid model that incorporated the best elements of a post bureaucratic model and a traditional command and control approach. The case study casts doubt on whether such a hybrid is possible solely through the implementation standard project management techniques and processes. It does so by exposing some of the tensions and differences in perceptions that can occur when such a model is implemented.  So where does this leave managers? Is there a way to manage creative work without destroying employee morale and motivation?  Looking over the complaints of the Buzzbank employees, it is clear that most of the problems arose from the loss of autonomy that they had enjoyed prior to the implementation of the new processes. This being the case, any measure to increase autonomy should improve the situation. A couple of possibilities come to mind – both of which I have discussed in earlier pieces. These are:  Empower employees to make decisions that affect their work. This means allowing them the freedom to decide the best approach to solving problems (within limits specified by organisational and resource constraints). In situations where (1) isn't possible, one could use collaborative techniques such as dialogue mapping to achieve employee buy-in. Of course, management has to be prepared to engage in true dialogue, and be willing to act upon (reasonable) suggestions made by employees. The key message is simple and obvious: the more of say employees have in making work-related decisions, the more engaged and motivated they'll be. This is not just a warm and fuzzy notion, but one that is backed up by research on motivation (see this paper review, for example). Yes, this does mean letting go of the "reins of control" to an extent but it is clear, as highlighted by Hodgson's work, that holding the reins tightly might cause more problems that it solves. What's called for, above all, is a degree of flexibility:  use project management processes by all means, but be open to employee input as to what's working well and what's not.  To sum up:  Hodgson's case study suggests that inflexible project management based approaches to managing creative work  may not work as well as advertised by purveyors of frameworks and methodologies. As an alternative,  it might be worth taking a step towards the utopian ideal of a post-bureaucratic organisation  by using techniques that encourage employee input in organisational decision making.Conventional approaches to knowledge management on projects focus on  the cognitive (or thought-related) and mechanical  aspects of knowledge creation and capture. There is alternate view, one which considers knowledge as being created through interactions between people who –  through their interactions –  develop mutually acceptable interpretations of theories and facts in ways that suit their particular needs. That is, project knowledge is socially constructed. If this is true, then project managers need to pay attention to the environmental and social factors that influence knowledge construction.  This is the position taken by Paul Jackson and Jane Klobas in their paper entitled, Building knowledge in projects: A practical application of social constructivism to information systems development, which presents a  knowledge creation / sharing process model based social constructivist theory. This article is a summary and review of the paper.  A social constructivist view of knowledge Jackson and Klobas begin with the observation that engineering disciplines are founded on the belief that knowledge can be expressed in propositions that correspond to a reality which  is independent of human perception.  However, there is an alternate view that knowledge is not absolute, but relative – i.e.  it depends on the mental models and beliefs used to interpret facts, objects and events. A  relevant example is how a software product is viewed by business users and software developers. The former group may see an application in terms of its utility whereas the latter may see it as an instance of a particular technology. Such perception gaps can also occur within seemingly homogenous groups – such as teams comprised of software developers, for example. This can happen for a variety of reasons such as the differences in the experience and cultural backgrounds of those who make up the group. Social constructivism looks at how such gaps can be bridged.  The authors' discussion relies on the work of Berger and Luckmann, who described how the gap between perceptions of different individuals can be overcome to create a socially constructed, shared reality. The phrase "socially constructed" implies that reality (as it pertains to a project, for example) is created via a common understanding of issues, followed by mutual agreement between all the players as to what comprises that reality. For me this view strikes a particular chord because of it is akin to the stated aims of dialogue mapping, a technique that I have described in several earlier posts (see this article for an example relevant to projects).  Knowledge in information systems development as a social construct First up, the authors make the point that information systems development (ISD) projects are:  …intensive exercises in constructing social reality through process and data modeling. These models are informed with the particular world-view of systems designers and their use of particular formal representations. In ISD projects, this operational reality is new and explicitly constructed and becomes understood and accepted through negotiated agreement between participants from the two cultures of business and IT  Essentially, knowledge emerges  through interaction and discussion  as the project proceeds.  However, the methodologies used in design are typically founded on an engineering approach, which takes a positivist view rather than a social one. As the authors suggest,  Perhaps the social constructivist paradigm offers an insight into continuing failure, namely that what is happening in an ISD project is far more complex than the simple translation of a description of an external reality into instructions for a computer. It is the emergence and articulation of multiple, indeterminate, sometimes unconscious, sometimes ineffable realities and the negotiated achievement of a consensus of a new, agreed reality in an explicit form, such as a business or data model, which is amenable to computerization.  With this in mind, the authors aim to develop a model that addresses the shortcomings of the traditional, positivist view of knowledge in ISD projects. They do this by representing Berger and Luckmann's theory of social constructivism in terms of a knowledge process model. They then identify management principles that map on to these processes. These principles form the basis of a survey which is used as an operational version of the process model. The operational model is then assessed by experts and tested by a project manager in a real-life project.  The knowledge creation/sharing process model The process model that Jackson and Klobas describe is based on Berger and Luckmann's work.  Figure 1: Knowledge creation/sharing model Figure 1: Knowledge creation/sharing model  The model  describes how personal knowledge is created – personal knowledge being what an individual knows. Personal knowledge is built up using mental models of the world – these models are frameworks that individuals use to make sense of the world.  According to the Jackson-Klobas process model, personal knowledge is built up through a number of process including:  Internalisation: The absorption of knowledge by an individual  Knowledge creation: The construction of new knowledge through repetitive performance of tasks (learning skills) or becoming aware of new ideas, ways of thinking or frameworks. The latter corresponds to learning concepts and theories, or even new ways of perceiving the world. These correspond to a change in subjective reality for the individual.  Externalisation: The representation and description of knowledge using speech or symbols so that it can be perceived and internalized by others. Think of this as explaining ideas or procedures to other individuals.  Objectivation: The creation of a shared constructs that represent a group's understanding of the world. At this point, knowledge is objectified – and is perceived as having an existence independent of individuals.  Legitimation: The authorization of objectified knowledge as being "correct" or "standard."  Reification: The process by which objective knowledge assumes a status that makes it difficult to change or challenge. A familiar example of reified knowledge is any procedure or process that is "hardened" into a system – "That's just the way things are done around here," is a common response when such processes are challenged.  The links depicted in the figure show the relationships between these processes.  Jackson and Klobas suggest that knowledge creation in ISD projects is a social process, which occurs through continual communication between the business and IT. Sure, there are other elements of knowledge creation – design, prototyping, development, learning new skills etc. – but these amount to nought unless they are discussed, argued, agreed on and communicated through social interactions. These interactions occur in the wider context of the organization, so it is reasonable to claim that the resulting knowledge takes on a form that mirrors the social environment of the organization.  Clearly, this model of knowledge creation is very different from the usual interpretation of knowledge having an independent reality, regardless of whether it is known to the group or not.  An operational model The above is good theory, which makes for interesting, but academic, discussions. What about practice? Can the model be operationalised?  Jackson and Klobas describe an approach to creating to testing the utility (rather than the validity) of the model.  I discuss this in the following sections.  Knowledge sharing heuristics  To begin with, they surveyed the literature on knowledge management to identify knowledge sharing heuristics (i.e. experience-based techniques to enable knowledge sharing).  As an example, some of the heuristics associated with the externalization process were:  We have standard documentation and modelling tools which make business requirements easy to understand Stakeholders and IS staff communicate regularly through direct face-to-face contact We use prototypes The authors identified more than 130 heuristics. Each of these was matched with a process in the model. According to the authors, this matching process was simple: in most cases there was no doubt as to which process a heuristic should be attached to. This suggests that the model provides a natural way to organize the voluminous and complex body of research in knowledge creation and sharing. Why is this important? Well, because it suggests that the conceptual model (as illustrated in Fig. 1) can form the basis for a simple means to assess knowledge creation / sharing capabilities in their work environments, with the assurance that they have all relevant variables covered.  Validating the mapping  The validity of the matching was checked using twenty historical case studies of ISD projects. This worked as follows: explanations for what worked well and what didn't were mapped against the model process areas (using the heuristics identified in the prior step). The aim was to answer the question:  "is there a relationship between project failure and problems in the respective knowledge processes or, conversely, between project success and the presence of positive indicators?"  One of the case studies the authors use is the well-known (and possibly over-analysed) failure of the automated dispatch system for the London Ambulance Service.  The paper has a succinct summary of the case study, which I reproduce below:  The London Ambulance Service (LAS) is the largest ambulance service in the world and provides accident and emergency and patient transport services to a resident population of nearly seven million people. Their ISD project was intended to produce an automated system for the dispatch of ambulances to emergencies. The existing manual system was poor, cumbersome, inefficient and relatively unreliable. The goal of the new system was to provide an efficient command and control process to overcome these deficiencies. Furthermore, the system was seen by management as an opportunity to resolve perceived issues in poor industrial relations, outmoded work practices and low resource utilization. A tender was let for development of system components including computer aided dispatch, automatic vehicle location, radio interfacing and mobile data terminals to update the status of any call-out. The tender was let to a company inexperienced in large systems delivery. Whilst the project had profound implications for work practices, personnel were hardly involved in the design of the system. Upon implementation, there were many errors in the software and infrastructure, which led to critical operational shortcomings such as the failure of calls to reach ambulances. The system lasted only a week before it was necessary to revert to the manual system.  Jackson and Klobas show how their conceptual model maps to knowledge-related factors that may have played a role in the failure project. For example, under the heading of personal knowledge, one can identify at least two potential factors: lack of involvement of end-users in design and selection of an inexperienced vendor. Further, the disconnect between management and employees suggests a couple of factors relating to reification: mutual negative perceptions and outmoded (but unchallenged) work practices.  From their validation, the authors suggest that the model provides a comprehensive framework that explains why these projects failed. That may be overstating the case – what's cause and what's effect is hard to tell, especially after the fact. Nonetheless, the model does seem to be able to capture many, if not all, knowledge-related gaps that could have played a role in these failures. Further, by looking at the heuristics mapped to each process, one might be able to suggest ways in which these deficiencies could have been addressed. For example, if externalization is a problem area one might suggest the use of prototypes or encourage face to face communication between IS and business personnel.  Survey-based tool  Encouraged by the above, the authors created a survey tool which was intended to evaluate knowledge creation/sharing effectiveness in project environments. In the tool, academic terms used in the model were translated into everyday language (for example, the term externalization was translated to knowledge sharing – see Fig 1 for translated terms). The tool asked project managers to evaluate their project environments against each knowledge creation process (or capability) on a scale of 1 to 10.  Based on inputs, it could recommend specific improvement strategies for capabilities that were scored low. The tool was evaluated by four project managers, who used it in their work environment over a period of 4-6 weeks. At the end of the period, they were interviewed and their responses were analysed using content analysis to match their experiences and requirements against the designed intent of the tool.  Unfortunately, the paper does not provide any details about the tool, so it's difficult to say much more than paraphrase the authors comments.  Based on their evaluation, the authors conclude that the tool provides:  A common framework for project managers to discuss issues pertaining to knowledge creation and sharing. A means to identify potential problems and what might be done to address them. Field testing  One of the evaluators of the model tested the tool in the field. The tester was a project manager who wanted to identify knowledge creation/sharing deficiencies in his work environment, and ways in which these could be addressed.  He answered questions based on his own evaluation of knowledge sharing capabilities in his environment and then developed an improvement plan based on strategies suggested by the tool along with some of his own ideas.  The completed survey and plan were returned to the researchers.  Use of the tool revealed the following knowledge creation/sharing deficiencies in the project manager's environment:  Inadequate personal knowledge. Ineffective externalization Inadequate standardization (objectivation) Strategies suggested by the tool include:  An internet portal to promote knowledge capture and sharing. This included discussion forums, areas to capture and discuss best practices etc. Role playing workshops to reveal how processes worked in practice (i.e. surface tacit knowledge). Based on the above, the authors suggest that:  Technology can be used to promote support knowledge sharing and standardization, not just storage. Interventions that make tacit knowledge explicit can be helpful. As a side benefit, they note that the survey has raised consciousness about knowledge creation/sharing within the team. Reflections and Conclusions In my opinion, the value of the paper lies not in the model or the survey tool, but the conceptual framework that underpins them – namely, the idea knowledge depends on, and is shaped by, the social environment in which it evolves. Perhaps an example might help clarify what this means. Consider an organisation that decides to implement project management "best practices" as described by <fill in any of the popular methodologies here>. The wrong way to do this would be to implement practices wholesale, without regard to organizational culture, norms and pre-existing practices. Such an approach is unlikely to lead to the imposed practices taking root in the organisation. On the other hand, an approach that picks the practices that are useful and tailors these to organizational needs, constraints and culture is likely to meet with more success. The second approach works because it attempts to bridge gap between the "ideal best practice" and social reality in the organisation. It encourages employees to adapt practices in ways that make sense in the context of the organization. This invariably involves modifying practices, sometimes substantially, creating new (socially constructed!) knowledge in the bargain.  Another interesting point the authors make is that several knowledge sharing heuristics (130, I think the number was) could be classified unambiguously under one of the processes in the model. This suggests that the model is a reasonable view of the knowledge creation/sharing process. If one accepts this conclusion, then the model does indeed provide a common framework for discussing issues relating knowledge creation in project environments. Further, the associated heuristics can help identify processes that don't work well.  I'm unable to judge the usefulness of the survey-based tool developed by the authors because they do not provide much detail about it in the paper. However, that isn't really an issue;  the field of project management has too many "tools and techniques" anyway.  The key message of the paper, in my opinion, is the that every project has a unique context, and that the techniques used by others have to be interpreted and applied in ways that are meaningful in the context of the particular project. The paper is an excellent counterpoint to the methodology-oriented practice of knowledge management in projects; it should be required reading for methodologists and  project managers who believe that things need to be done by The Book, regardless of social or organizational context.System design is a creative activity, but one that is subject to a variety of constraints. Many of these constraints are obvious: for example, when tasked with designing a new software product, a team might be asked to work within a budget or use a particular technology. These constraints place boundaries on the design activity; they force designers to work within parameters specified by the constraints. But there are other less obvious  constraints too. In a paper entitled, How Do Committees Invent, published in 1968, Melvin Conway described a notion that is now called Conway's Law: An organisation which designs a system will inevitably produce a design that mirrors the organisation's communication structure. This post is a summary of the key points of the paper.  Conway begins the paper with the observation that the system design is an activity that involves specifying how a system will be built using a number of diverse parts.  Many elements of the act of design are similar, regardless of the nature of the system –be it software or a shopping mall. The objective of a design team or organisation is to produce a specification or blueprint based on which the system can be built.  Much of design work is about making choices. Conway points out that these choices may be more than design decisions:  Most design activity requires continually making choices. Many of these choices may be more than design decisions; they may also be personal decisions the designer makes about his own future. As we shall see later, the incentives which exist in a conventional management environment can motivate choices which subvert the intent of the sponsor.  The paper is essentially an elaboration and justification of this claim.  Pre-design work The preliminary stages of design work are more about organizing than design itself.  First, the boundaries have to be understood so that the solution space can be defined. Second, the high-level structure of the system has to be explored so that work can be subdivided in a sensible way within the organisation that's doing the design. This latter point is the crux of Conway's argument:  …the very act of organizing a design team means that certain design decisions have already been made, explicitly or otherwise. Given any design team organization, there is a class of design alternatives which cannot be effectively pursued by such an organization because the necessary communication paths do not exist. Therefore, there is no such thing as a design group which is both organized and unbiased.  There are a couple of important points here:  The act of delegating design tasks narrows the scope of design options that can be pursued. Once tasks are delegated to groups, coordination (via communication) between these groups is the only way that the work can be integrated. Further, once established, it is very hard to change design idea (or a project team, for that matter).  The system mirrors the organisation Most systems of any significance are composed of several subsystems that communicate with each other via interfaces. According to Conway, these elements (italicized in the previous sentence) have a correspondence with the organisation that designs the system.  How so? Well, every subsystem is designed by a group within the organisation (call it a design group).  If two subsystems are to communicate interact with each other, the two groups responsible for their design must communicate with each other (to negotiate the interface design).  If subsystems don't interact, no communication is necessary.  What we see from this argument is that the communication between subsystems roughly mirrors the communication paths within the organisation.  As any system designer knows: given a set of requirements, there are a number of designs that can satisfy them.  If the argument of the previous paragraph is true then the structure of the design organisation (or project team)  influences the choice that is made.  Managing systems design Conway points out that large system design efforts spin out of control more often than those for small systems. He surmises that this happens when the design becomes too complex for one person (or a small, tightly-knit group of people). A standard management reaction to such a situation is to delegate the design of component to sub-teams. Why? Well here's what Conway says:  A manager knows that he will be vulnerable to the charge of mismanagement if he misses his schedule without having applied all his resources. This knowledge creates a strong pressure on the initial designer who might prefer to wrestle with the design rather than fragment it by delegation, but he is made to feel that the cost of risk is too high to take the chance. Therefore, he is forced to delegate in order to bring more resources to bear.  A major fallacy in this line of thinking is that more resources means that work gets done faster. It is well known that this isn't so – at least as far as software systems development is concerned. Conway points out that politics also contributes to this effect. In most organisations, managerial status is tied to team size and project budgets. This provides an incentive to managers to expand their organisations (i.e. project teams), making design delegation almost inevitable.  Large teams have a large number of communication paths between their members. Specifically, in a team consisting of N people, there are N(N-1)/2 possible communication paths – each person can communicate with N-1 people making N(N-1), but this has to be halved because paths between every two individuals are counted twice. Organisations deal with this by restricting communication paths to hierarchical management structures.  Because communication paths mirror organizational structures, it is almost inevitable that system designs will mirror them.  Conclusion The main implication of Conway's thesis is that a project team (or any organisation) charged with designing a system should be structured in a way that suits the communication needs of the system. For example, sub-teams involved in designing related subsystems should have many more communication channels than those that design independent components.  Further, system design is inherently complex, and the first design is almost never the final one.  A consequence that flows from this is that design organisations should be flexible because they'll almost always need to be reorganized.  In the end it is less about the number of people on a team than the communication between them. As Conway mentions in the last two lines of his paper:  There is need for a philosophy of system design management which is not based on the assumption that adding manpower simply adds to productivity. The development of such a philosophy promises to unearth basic questions about value of resources and techniques of communication which will need to be answered before our system-building technology can proceed with confidence.  This is as true now as it was forty-odd years ago.Most of the research and practice literature on project management tends to view projects as being isolated from their environment.  It is obvious to anyone who has worked on a project that this isn't so. In view of this,  it is useful to look at the relationship between projects and the organisations that host them.  This post looks at this issue, drawing on a paper by Gernot Grabher entitled, Cool Projects, Boring Institutions: Temporary Collaboration in Social Context.  The emergence of projects Grabher begins his discussion with a sketch of the how projects emerged as a distinct work form. Projects  – i.e. time bound, goal focused activities – have always been around. The modern notion of a project, however,  arose from a development philosophy that came out of the US Department of Defense in the 1950s.  He states,  …Instead of fragmenting and pre-specifying the development of military technologies along functional disciplines, these technologies were described in relation to their objectives, i.e. the military parameters of these weapons. The pacing of these concentrated efforts was crucial: parameters had to be met, goals had to be accomplished according to a grand scheme (program?) to win the armament race. Development processes that earlier were seen as separate activities were now conceptualized as an integrated entity called a program, system or project. The overwhelming scale of these projects in terms of financial and scientific resources as well as their ambitious timing created formidable problems of coordination and control. Experiments with various forms of organizational control ultimately lead to the professionalization of the role of the project manager…  From thereon the concepts of projects and project management were taken up (with much enthusiasm and optimism) by business and industry. The formalization of various project management methodologies, standards , qualifications and trade journals can be seen a culmination of this process.  Given the military-industrial origins of the profession, it is easy to see why a "command and control" philosophy dominates much of project management thought and practice. Many of the early projects that are paraded as textbook examples of successful projects operated outside normal organizational oversight. They were, to a large extent, deliberately shielded from external influences.  I believe this is why isolation from the environment is seen as a Good Thing by project managers –  problems of coordination and control become so  much simpler when one does not have to manage relationships and politics that are (perceived as being) external to a project. This practice may be necessary and workable for classified projects that run on billion dollar budgets, but it doesn't work so well in  environments that most project managers work in. Projects don't take place in a  vacuum; they are born, live and die in real-world organizations. To forget that is to see the "tree of the project" and miss the "forest of the organization." This is particularly so because,  unlike those near-mythical mega-projects of the 1950s,  the efforts that you and I work on are deeply entwined with their hosting organizations.  Organisation-related characteristics of projects Grabher then notes some characteristics of projects. I summarize these in the next few paragraphs.  First, it is interesting that the original meaning of the word "project" referred to a "proposal" or "idea", rather than a "directed, time-bound effort." Grabher points out that this  shift in meaning was  accompanied by a shift in focus: from project as idea (or goal) to project as process (or means to achieving a goal).  Projects are thus seen as vehicles for achieving organisational goals.  Second, Grabher notes that projects are often hard to decompose into constituent tasks, and that such a (commonly agreed) decomposition is only possible when stakeholders interrelate with each other continually.  This underscores the importance of communication in projects.  Third, Grabher highlights the importance of the project manager (he uses the term contractor) as the "lynchpin on whom trust is focused." The role of the manager is particularly important in projects on which team members do not have the time to get to know each other well.  Fourth, the project manager / contractor is also the wielder of organizational authority as far as the project is concerned. He or she is, in this sense, a representative of the organization – a person whose presence underlines the fact that the project exists to achieve specified organizational goals.  Finally, deadlines are a defining aspect of projects. They serve several functions. For example, they ensure that a sense of urgency for action and progress remains through the duration of the project. They also might serve to legitimize execution of project work without external interference (this argument was frequently used in the military-industrial projects of the 1950s). But above all, the final deadline,  which culminates in the termination of the project, also serves as a connector to the rest of the organization. It is a time in which handoffs, documentation, team disbanding etc. occurs, thus enabling the results and  experiences from the project disperse into the wider organization.  Projects in organisations The characteristics noted above highlight the dual nature of projects: on the one hand, as noted earlier, projects are seen as semi-autonomous temporary organisations, but on the other they are also firmly embedded within the hosting organisation. An effect of the latter is particularly evident  in consulting and software services firms (or even corporate IT shops), which tend to do similar projects over and over.  As Grabher notes,  [projects] apparently operate in a milieu of recurrent collaboration that, after several project cycles, fills a pool of resources and gels into latent networks. Project organising is mostly directed towards the actual realization of a potential that is generated and reproduced by the practice of drawing on core members of (successful) prior projects to serve on derivative successor projects. Such chains of repeated co-operation are held together (or cut off ) by the reputation members gain (or lose) in previous collaborations…  Another aspect of embedded-ness is the co-location of team members within a larger organizational milieu. The standard benefits of co-location are well known. These are:  Savings of transactional costs such as those incurred in communication, supervision of staff at remote locations etc. See my post on a transaction cost view of outsourcing for more on this. Co-location improves the efficacy of communication by encouraging face-to-face interactions. It enables "near real-time" monitoring of the health of the project and its environment. There's more though. Grabher notes that  in addition to the above "intentional" or "strategic" benefits, co-location also ensures that  team members are exposed to the same organizational noise – which consists of  a "concoction of rumours, impressions, recommendations, trade folklore and strategic misinformation (falsehoods!)."  Co-location enables project teams to make collective sense of organisational noise – this shared understanding of the environment can contribute significantly to the creation of a team spirit.  A related notionis that of enculturation: that is, the process of becoming an accepted member of the group, or an insider. This has less to do with expertise and knowledge than learning the unspoken rules and norms of a community.  Although becoming a member of  a community has much to do with social interactions within the workplace, there is more: a lot of essential know-how and know-what is transferred through informal interactions between senior members of the team (who are often senior members of the organisation) and others.  Projects generally need to draw upon a range of organizational resources: people and physical infrastructure being the most obvious ones.  Grabher notes that the increasing projectisation of organizations can be attributed to a perception that project-based management is an efficient way to allocate productive resources in a flexible manner  (…whether this perception is correct, is another matter altogether). However, there are other less obvious influences that organisations exert too.  For example,  Grabher points out that organizational norms and rules provide the basis for the emergence of swift trust, which is trust based on roles and professional ability rather than individuals and personalities.  Further, at a higher level,  organizational culture plays a role in determining how a project is governed, managed and run. These explicit and implicit norms have a stabilising influence on projects.  In addition to the stabilizing influence of the hosting organisation, projects also offer opportunities to build and enhance links between organisations – for instance, strategic partnerships.  This is, in effect, institution building aimed at leveraging the strengths of the participating organisations for a greater joint benefit. In such situations the participating organisations take on the role of "lynchpins" on whom trust is focused.  Grabher makes the point that firms (and institutions comprised of firms) not only provide resources that make projects possible, but also host a range of processes that are needed to organize and run projects. For one, projects are usually preceded by several organisational processes involving deliberation, selection and preparation. These activities have to occur for a project to happen, but they normally fall outside the purview of the project.  A somewhat paradoxical aspect of projects is although they offer the opportunity for enhancing organizational knowledge, this rarely happens in practice. The high pressure environment in projects leaves little time for formal training or informal learning, or even to capture knowledge in documents. To  a large extent the hosting organisations are to blame: Grabher suggests that this paradox is a consequence of the lack of organizational redundancy in project-based organizing.  I'll end this section with the observation that the social dimension of projects is often neglected.  Projects are often hindered by organizational politics and inertia.  Further, a large number of  projects fail because of varying perceptions of project goals and the rationale behind them. Although it seems obvious that a project should not proceed unless all stakeholders have a shared understanding of objectives and the reasons for them, it is surprising how many projects drift along without it.  Many project planners neglect this issue, and it invariably comes back to bite them.  Conclusions In the conclusion to the paper, Grabher states:  The formation and operation of projects essentially relies on a societal infrastructure which is built on and around networks, localities, institutions and firms. Relations between temporary and permanent systems are not a matter of straightforward substitution but have to be regarded in terms of interdependence. ‘Cool' projects, indeed, rely on ‘boring' institutions…  This is unarguable, but it should also be kept in mind that projects are often subject to negative organisational influences which can slow them down, or even kill them altogether (which is perhaps why those early defence projects were set up as near-autonomous initiatives). So  although  it is true that projects  are made possible and sustained by the organisations they're embedded in,  they are sometimes  hindered by those very organisations .  To sum up in a line:   Projects depend on organisations not only for material and human resources, but also draw sustenance from (and are affected by)  the  social environment and culture that exists within those organisations.The fundamental premise behind best practices  is that it is possible to reproduce the successes of those who excel by imitating them. At first sight this assumption seems obvious and uncontroversial. However, most people who have lived through an implementation of a best practice know that following such prescriptions does not guarantee success. Actually, anecdotal evidence suggests the contrary:  that most attempts at implementing best practices fail. This paradox remains unnoticed by managers and executives who continue to commit their organisations to implementing best practices that are, at best, of dubious value.  Why do best practices fail?  There has been a fair bit of research on the shortcomings of best practices, and the one thing it tells us is that there is no simple answer to this question.  In this post I'll discuss this issue, drawing upon an old (but still very relevant) paper by Jonathan Wareham and Han Cerrits entitled, De-Contextualising Competence: Can Best Practice be Bundled and Sold.  Note that I will not cover the paper in its entirety;  my discussion will focus only on those aspects that relate to the question raised above.  I may as well say it here:  I have a secondary aim  (or more accurately,  a vested interest)  in discussing this paper.  Over the last few months Paul Culmsee and I have been working on a book that discusses reasons why best practices fail and proposes some practical techniques to address their shortcomings.  I'll end this post with a brief discussion of the background and content of the book (see this post for Paul's take on the book). But let's look at the paper first…  Background On the first page of the paper the authors state:  Although the concept of ‘imitating excellent performers' may seem quite banal at first glance, the issue, as we will argue, is not altogether that simple after deeper consideration. Accordingly, the purpose of the paper is to explore many of the fundamental, often unquestioned, assumptions which underlie the philosophy and application of Business Best Practice transfer. In illuminating the central empirical and theoretical problems of this emerging discipline, we hope to refine our expectations of what the technique can yield, as well as contribute to theory and the improvement of practice.  One of the most valuable aspects of the paper is that it lists some of the implicit assumptions that are often glossed over by consultants and  others who sell  and implement best practice methodologies.  It turns out that these assumptions are not valid in most practical situations, which renders the practices themselves worthless.  The implicit assumptions According to Wareham and Cerrits, the unstated premises behind best practices include:  Homogeneity of organisations: Most textbooks and courses on best practices present the practices as though they have an existence that is independent of organizational context.  Put another way: they assume that all organisations are essentially the same. Clearly, this isn't the case – organisations are defined by their differences. Universal yardstick: Best practices assume that there is a universal definition of what's best, that what's best for one is best for all others. This assumption is clearly false as organisations have different (dare I say, unique) environments, objectives and strategies. How can a universal definition of "best" fit all? Transferability:  Another tacit assumption in the best practice business is that practices can be transplanted on to receiving organisations wholesale. Sure, in recent years it has been recognized that such transplants are successful only if a) the recipient organisation undertakes the changes necessary for the transplant to work and b) the practice itself is adapted to the recipient organisation. The point is in most successful cases, the change or adaptation is so great that it no longer resembles that original  best practice. This is an important point – to have a hope in hell of working, best practices have to be adapted extensively. It is also worth mentioning that  such adaptations will succeed only if they are  made in consultation with those who will be affected by the practices. I'll say more about this later in this post Alienability and stickiness: These are concepts that relate to the possibility of extracting relevant knowledge pertaining to a best practice from a source and transferring it without change to a recipient. Alienability refers to the possibility of extracting relevant knowledge from the source. Alienability is difficult because best practice knowledge is often tacit, and is therefore difficult to codify. Stickiness refers to the willingness of the recipient to learn this knowledge, and his or her ability to absorb it. Stickiness highlights the importance of obtaining employee buy-in before implementing best practices. Unfortunately most best practice implementations gloss over the issues of alienability and stickiness. Validation: Wareham and Cerrits contend that best practices are rarely validated. More often than not,  recipient organisations simply believe that they will work, based on their consultants' marketing spiel. See this short piece by Paul Strassman for more on the dangers of doing so. What does  "best" mean anyway? After listing the implicit assumptions, Wareham and Cerrits argue that the conceptual basis for defining a particular practice as being "best" is weak.  Their argument hinges on the observation that it is impossible to attribute the superior performance of a firm to specific managerial practices. Why? Well, because one cannot perform a control experiment to see what would happen if those practices weren't used.  Related to the above is the somewhat subtle point that it is impossible to say, with certainty, whether practices, as they exist within model organisations, are consequences of well-thought out managerial action or whether they are merely adaptations to changing environments. If the latter were true, then there is no best practice, because the practices as they exist in model organisations are essentially random responses to organizational stimuli.  Wareham and Cerrits also present an economic perspective on best practice acquisition and transfer, but I'll omit this as it isn't of direct relevance to the question of why best practices fail.  Implications The authors draw the following conclusions from their analysis:  The very definition of best practices is fraught with pitfalls. Environmental factors have a significant effect on the evolution and transfer(ability) of "best" practices. Consequently, what works in one organisation may not work in another. So, can anything be salvaged?  Wareham and Cerrits think so. They suggest an expanded view of best practices which includes things such as:  Using best practices as guides for learning new technologies or new ways of working. Using best practices to generate creative insight into how business processes work in practice. Using best practices as a guide for change – that is, following the high-level steps, but not necessarily the detailed prescriptions. These are indeed sensible and reasonable statements. However, they  are much weaker than the usual hyperbole-laden claims that accompany best practices.  Discussion Cerrits and Johnson focus on the practices themselves, not the problems they are used to solve. In my opinion, another key reason why best practices fail is that they are applied without a comprehensive understanding of the problem that they are intended to address.  I'll clarify this using an example:  in a quest to improve efficiency an organisation might go through a major restructure. All too often, such organisations will not think through all the consequences of the restructuring (what are the long-term consequences of outsourcing certain functions, for instance). The important point to realize is that a comprehensive understanding of the consequences is possible only if all stakeholders – management and employees – are involved in planning the restructure.  Unfortunately, such a bottom-up approach is rarely taken because of the effort involved, and the  wrong-headed perception that chaos may ensue from management actually talking to people on the metaphorical shop floor.  So  most organizations take a top-down approach, dictating what will be done, with little or no employee involvement.  Organisations focus on how to achieve a particular end. The end itself, the reasons for wanting to achieve it and the consequences of doing so remain unexplored; it is assumed that these are obvious to all stakeholders. To put it in aphoristically: organizations focus on the "how" not the on the "what" or why."  The heart of the matter The key to understanding why best practices do not work is to realise that many organizational problems are wicked problems: i.e., problems that are hard to define, let alone solve's  (see this paper for a comprehensive discussion of wicked problems).  Let's look at organizational efficiency, for example.  What does it really mean to improve organizational efficiency?  More to the point, how can one arrive at a generally agreed way to improve organizational efficiency?  By generally agreed, I mean a measure that all stakeholders understand and agree on. Note that "efficiency "is just an example here – the same holds for most other matters of strategic importance to organizations: organisational strategy is a wicked problem.  Since wicked problems are hard to pin down (because they mean different things to different people), the first step to solving them is to ensure that all stakeholders have a common (or shared) understanding of what the problem is. The next step  is to achieve a shared commitment to solving that problem. Any technique that could help achieve a shared understanding of wicked problems and commitment to solving them  would  truly deserve to be called the one best practice to rule them all.  The genesis of a collaboration About a year ago, in a series of landmark posts entitled The One Best Practice to Rule Them All, Paul Culmsee wrote about his search for a practical method to manage wicked problems.  In the articles he made a convincing case that dialogue mapping can help a diverse group of stakeholders achieve a shared understanding of such problems.  Paul's writings inspired me to learn dialogue mapping and use it at work. I was impressed – here, finally, was a technique that didn't claim to be a best practice, but had the potential to address some of the really complex problems that organisations face.  Since then, Paul and I have had several conversations about the failure of best practices in to tackling issues ranging from organizational change to project management. Paul is one of those rare practitioners with an excellent grounding in theory and practice.  I learnt a lot from him in those conversations. Among other things, he told me about his experiences in using dialogue mapping to tackle apparently intractable problems (see this case study from Paul's company, for example).  Late last year, we thought of writing up some of the things we'd been talking about in a series of joint blog posts. Soon we realised that we had much more to say than would fit into a series of posts – we probably had  enough for a book.  We're a few months into writing that book, and are quite pleased with the way it's turning out.  Here's a very brief summary of the book. The first part analyses why best practices fail.  Our analysis  touches  upon diverse areas like organizational rhetoric, cognitive bias, memetics and scientific management (topics that both Paul and I have written about on our blogs).  The second part of the book presents a series of case studies that illustrate some techniques that address complex problems that organizations face. The case studies are based on our experiences in using dialogue mapping and other techniques to tackle wicked problems relating to organizational strategy and project management.  The techniques we discuss go beyond the rhetoric of best practices – they work because they use a bottom-up approach that takes into account the context and environment in which the problems live.  Now, Paul writes way better than I do. For one, his writing is laugh-out-loud funny, mine  isn't. Those who have read his work and mine may be wondering how our very different styles will combine.  I'm delighted to report that the book is way more conversational and entertaining than my blog posts.  However, I should also emphasise that we are trying to be as rigorous as we can by backing up our claims by references to research papers and/or case studies.  We're learning a lot in the process of writing, and are enthused and excited about the book . Please stay tuned – we'll post occasional updates on how it is progressing.The meeting had been going on for a while but it was going nowhere.  Finally John came out and said it,  "There is no way I can do this in 2 days," he said. "It will take me at least a week."  There it was, the point of contention between the developer and his manager. Now that it was out in the open, the real work of meeting could begin; the two could start talking about a realistic delivery date.  The manager, let's call him Jack, was not pleased, "Don't tell me a simple two-page web app – which you have done several times before I should add – will take  you a week to do. "  "OK, let me walk you through the details," said John.  ….and so it went for another half hour or so, Jack and John arguing about what would be a reasonable timeframe for completing the requested work.  Dialogue, rationality and action Most developers, designers  and indeed most "doers" on project teams– would have had several conversations similar to the one above. These folks spend a fair bit of time  discussing matters relating to the projects they work on. In such discussions, the aim is to come to a shared understanding of the issue under discussion and  a shared commitment on future action.  In the remainder of this post I'll take a look at project discussions from a somewhat  philosophical perspective, with a view to understanding some of the obstacles to open dialogue and how they can be addressed.  When we participate in discussions we want our views to be taken seriously. Consequently, we present our views through statements that we hope others will see as being rational – i.e. based on sound premises and logical thought.  One presumes that John – when he made his claim about the delivery date being unrealistic – was willing to present arguments that would convince Jack that this was indeed so. The point is that John is judged (by Jack and others in the meeting) based on the validity of the statements he (John) makes. When Jack's  validity claims are contested, debate ensues with the aim of  getting to some kind of agreement.  The philosophy underlying such a process of discourse (which is simply another word for "debate" or "dialogue")  is described in the theory of communicative rationality proposed by the German philosopher Jurgen Habermas.  The  basic premise of communicative rationality is that rationality (or reason) is tied to social interactions and dialogue. In other words, the exercise of reason can  occur only through dialogue.  Such communication, or mutual deliberation,  ought to result in a general agreement about the issues under discussion.  Only once such agreement is achieved can there be a consensus on actions that need to be taken.  Habermas refers to the  latter  as communicative action,  i.e.  action resulting from collective deliberation.  [Note: Just to be clear:  I have not read Habermas' books, so my discussion is based entirely on secondary sources: papers by authors who have studied Habermas in detail. Incidentally, the Wikipedia article on the topic is  quite good, and well worth a read.]  Validity Claims Since the objective in project discussions is to achieve shared understanding of issues and shared commitment on future action, one could say that such discussions are aimed at achieving communicative action.  The medium through which mutual understanding is achieved is speech – i.e.  through  statements that a speaker makes based on his or her perceptions of reality.  Others involved in the dialogue do the same,  conveying their perceptions (which may or may not match the speaker's).  Now, statements made in discussions  have implicit or explicit validity claims – i.e. they express a speaker's belief that something is true or valid, at least in the context of the dialogue.  Participants who disagree with a speaker are essentially contesting claims.  According to the theory of communicative action, every utterance makes the following validity claims:  It makes a claim about objective (or external) reality. John's statement about the deadline being impossible refers to the timing of an objective event – the delivery of working software. Habermas refers to this as the truth claim. It says something about social reality – that is, it expresses something about the relationship between the speaker and listener(s). The relationship is typically defined by social or workplace norms,  for example – the relationship between a manager and employee as in the case of John and Jack. John's statement is an expression of disagreement with his manager.  Of course, he believes his position is justified – that it ought to take about a week to deliver the software. Habermas refers to this as the rightness claim. It expresses something about subjective reality – that is, the speaker's personal viewpoint. John believes, based on his experience, intutition etc., that the deadline is impossible. For communication to happen, Jack must work on the assumption that John is being honest – i.e. that John truly believes the deadline is impossible, even though Jack may not agree. Habermas refers to this as the truthfulness claim. The validity claims and their relation to rationality are nicely summed up in the Wikipedia article on communicative rationality, and I quote:  By earnestly offering a speech act to another in communication, a speaker claims not only that what they say is true but also that it is normatively right and honest . Moreover, the speaker implicitly offers to justify these claims if challenged and justify them with reasons. Thus, if a speaker, when challenged, can offer no acceptable reasons for the normative framework they implied through the offering of a given speech act, that speech act would be unacceptable because it is irrational.  When John says that the task is going to take him a week, he implies that he can justify the statement (if required) in three ways:  it will take him a week (objective), that it ought to take him a week (normative – based on rightness)  and that he truly believes it will take him a week (subjective).  In all dialogues validity claims are implied, but rarely tested;  we usually take what people say at face value, we don't ask them to justify their claims. Nevertheless, it is assumed that they can offer justifications should we ask them to. Naturally, we will do so only when we have reason to doubt the validity of what they say.  It is at that point that discourse begins. As Wener Ulrich puts it in this paper:  In everyday communication, the validity basis of speech is often treated as unproblematic. The purpose consists in exchanging information rather than in examining validity claims. None of the three validity claims is then made an explicit subject of discussion. It is sufficient for the partners to assume (or anticipate, as Habermas  likes to say) that speakers are prepared to substantiate their claims if asked to do so, and that it is at all times possible for the participants to switch to a different mode of communication in which one or several validity claims are actually tested. Only when validity claims do indeed become problematic, as one of the participants feels compelled to dispute either the speaker's sincerity or the empirical and/or normative content of his statements, ordinary communication breaks down and discourse begins.  Progress in project discussions actually depends on such breakdown in "ordinary communication" – good project decisions emerge from open deliberation about the pros and cons of competing approaches. Only once this is done can one move to action.  Conditions for ideal discourse All this sounds somewhat idealistic, and it is. Habermas noted five prerequisites for open debate. They are:  Inclusion: all affected parties should be included in the dialogue. Autonomy: all participants should be able to present and criticise validity claims independently. Empathy: participants must be willing to listen to and understand claims made by others. Power neutrality: power differences (levels of authority) between participants should not affect the discussion. Transparency: participants must not indulge in strategic actions (i.e. lying!). In this paper Bent Flyvbjerg adds a sixth point:  that the group should be able to take as long as it needs to achieve consensus – Flyvbjerg calls this the requirement of unlimited time.  From this list it is clear that open discourse (or communicative rationality)  is an ideal that is difficult to achieve in practice.  Nevertheless, because  it is always possible to improve the quality of dialogue on projects, it behooves us as project professionals to strive towards the ideal. In the next section I'll look at one practical way to do this.  Boundary judgements Most times in discussions we jump straight to the point, without bothering to explain the assumptions that underpin our statements.  By glossing over assumptions, however, we leave ourselves open to being  misunderstood because  others have no means to assess the validity of our statements. Consequently it becomes difficult for them to empathise with us. For example, when John says that it is impossible to finish the work in less than a week, he ought to support his claim by stating the assumptions he makes and how these bear on his argument.  He may be assuming that he has to do the work in addition to all the other stuff he has on his plate.  On the other hand, he may be assuming too much because his manager may be willing to reassign the other stuff to someone else.  Unless this assumption is  brought out in the open, the two will continue to argue without reaching agreement.  Werner Ulrich pointed out that the  issue of tacit assumptions and unstated frameworks is essentially one of defining the boundaries within which one's claims hold. He coined the term boundary judgement to describe  facts and norms that a speaker deems relevant to his or her statements. A boundary judgement determines the context within which a  statement holds and  also determines the range of validity of the statement.  For example, John is talking about the deadline being impossible in the context of his current work situation;  if the situation changed, so might his estimate.  Ulrich invented the notion of boundary critique to address this point. In essence, boundary critique is a way to uncover boundary judgements by asking the right questions. According to Ulrich, such boundary questions probe the assumptions made by various stakeholders. He classifies boundary questions into four categories. These are:  Motivation: this includes questions such as: Why are we doing this project? Who are we doing it for? How will we measure the benefits of the project once it is done? Power: this includes questions such as: Who is the key decision-maker regarding scope? What resources are controlled by the decision-maker? What are the resources that cannot be controlled by the decision-maker (i.e. what are the relevant environmental factors)? Knowledge:  This includes: What knowledge is needed to do this work? Who (i.e which professionals) have this knowledge? What are the key success factors – e.g. stakeholder consensus, management support, technical soundness etc? Legitimation: This includes: Who are the stakeholders (including those that are indirectly affected by the project)? How do we ensure that the interests of all stakeholders are taken into account? How can  conflicting views of project objectives be reconciled? The questions above are drawn from a paper by Ulrich.  I have paraphrased them in a way that makes sense in project environments.  Many of these questions are difficult to address openly, especially those relating to power and  legitimation. Answers to these often bump up against organisational politics or power. The point, however, is that once these questions are asked, such constraints become evident to all. Only after this happens can discourse proceed in the full knowledge of what is possible and what isn't.  Before closing this section I'll note that there are  other techniques that do essentially the same thing1, but I won't discuss them here as I've already exceeded a reasonable word count.  Conclusion Someone recently mentioned to me that the problem in project meetings (and indeed any conversation) is that participants  see their own positions  as being rational, even when they are not.  Consequently, they stick to their views, even when faced with evidence to the contrary. According to the theory of communicative rationality, however, such folks aren't being rational because they do not subject their positions and views to "trial by  argumentation".  Rationality lies in  dialogue, not in individual statements or positions. A productive discussion is one in which validity claims are continually challenged until they converge on an optimal decision.  The best (or most rational) position  is  one that  emerges from such collective deliberation.  In closing, a caveat is in order – a complete discussion of dialogue in projects (or organisations) would take an entire book and more. My discussion here has merely highlighted a few issues (and a technique)  that I daresay are rarely touched upon in management texts or courses. There are many more tools and techniques that can help improve the quality of discourse within organisations.  Paul Culmsee and I discuss some of these in our book, The Heretic's Guide to Best Practices.  